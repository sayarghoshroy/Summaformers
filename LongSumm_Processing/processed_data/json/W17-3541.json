15:18:42.995 [main] DEBUG com.amazonaws.AmazonWebServiceClient - Internal logging successfully configured to commons logger: true
15:18:43.049 [main] DEBUG com.amazonaws.metrics.AwsSdkMetrics - Admin mbean registered under com.amazonaws.management:type=AwsSdkMetrics
15:18:43.108 [main] DEBUG c.a.internal.config.InternalConfig - Configuration override awssdk_config_override.json not found.
15:18:43.494 [ModelLoaderThread] INFO  org.allenai.scienceparse.Parser - Loading model from /home/risubaba/.ai2/datastore/public/org.allenai.scienceparse/productionModel-v9.dat
15:18:43.494 [scala-execution-context-global-12] INFO  org.allenai.scienceparse.Parser - Loading gazetteer from /home/risubaba/.ai2/datastore/public/org.allenai.scienceparse/gazetteer-v5.json
15:18:43.496 [scala-execution-context-global-12] INFO  org.allenai.scienceparse.Parser - Loading bib model from /home/risubaba/.ai2/datastore/public/org.allenai.scienceparse/productionBibModel-v7.dat
15:18:43.500 [scala-execution-context-global-12] INFO  org.allenai.scienceparse.Parser - Creating gazetteer cache at /tmp/gazetteer-v5.json-fa485aef.gazetteerCache.bin
15:18:54.263 [scala-execution-context-global-12] INFO  o.a.scienceparse.ParserGroundTruth - Read 1609659 papers.
15:19:19.239 [ModelLoaderThread] INFO  org.allenai.scienceparse.Parser - Loaded model from /home/risubaba/.ai2/datastore/public/org.allenai.scienceparse/productionModel-v9.dat
15:19:29.669 [scala-execution-context-global-12] INFO  o.a.scienceparse.ExtractReferences - could not load kermit gazetter
15:19:29.727 [scala-execution-context-global-12] INFO  org.allenai.scienceparse.Parser - Loaded gazetteer from /home/risubaba/.ai2/datastore/public/org.allenai.scienceparse/gazetteer-v5.json
15:19:29.727 [scala-execution-context-global-12] INFO  org.allenai.scienceparse.Parser - Loaded bib model from /home/risubaba/.ai2/datastore/public/org.allenai.scienceparse/productionBibModel-v7.dat
15:19:29.732 [scala-execution-context-global-12] INFO  org.allenai.scienceparse.RunSP$ - Starting /home/risubaba/LongSumm/pdf/W17-3541.pdf
{
  "name" : "/home/risubaba/LongSumm/pdf/W17-3541.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Neural Response Generation for Customer Service based on Personality Traits",
    "authors" : [ "Jonathan Herzig", "Michal Shmueli-Scheuer", "Tommy Sandbank" ],
    "emails" : [ "hjon@il.ibm.com", "shmueli@il.ibm.com", "tommy@il.ibm.com", "davidko@il.ibm.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of The 10th International Natural Language Generation conference, pages 252–256, Santiago de Compostela, Spain, September 4-7 2017. c©2017 Association for Computational Linguistics"
    }, {
      "heading" : "1 Introduction",
      "text" : "Automated conversational agents are becoming popular for various tasks, such as personal assistants, shopping assistants, or as customer service agents. Automated agents benefit from adapting their personality according to the task at hand (Reeves and Nass, 1996; Tapus and Mataric, 2008) or to the customer (Herzig et al., 2016). Thus, it is desirable for automated agents to be capable of generating responses that express a target personality.\nPersonality is defined as a set of traits which represent durable characteristics of a person. Many models of personality exist while the most common one is the Big Five model (Digman, 1990) , including: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. These traits were correlated with linguistic choices including lexicon and syntax (Mairesse and Walker, 2007).\nIn this paper we study how to encode personality traits as part of neural response generation for conversational agents. Our approach builds upon a sequence-to-sequence (SEQ2SEQ) architecture (Sutskever et al., 2014) by adding an additional\nlayer that represents the target set of personality traits, and a hidden layer that learns high-level personality based features. The response is then generated conditioned on these features.\nSpecifically, we focus on conversational agents for customer service; in this context, many studies examined the effect of specific personality traits of human agents on service performance. Results indicate that conscientiousness (a person’s tendency to act in an organized or thoughtful way) and agreeableness (a person’s tendency to be compassionate and cooperative toward others) correlate with service quality (Blignaut et al., 2014; Sackett, 2014).\nFigure 1 shows examples of customer utterances, followed by two automatically generated responses. The first response (in each example), is generated by a standard SEQ2SEQ response generation system that ignores personality modeling and in effect generates the consensus response of the humans represented in the training data. The second response is generated by our system, and is aimed to generate\n252\ndata for an agent that expresses a high level of a specific trait. In example 1, the agreeableness-agent is more compassionate (expresses empathy) and is more cooperative (asks questions). In example 2, the conscientiousness-agent is more thoughtful (will ”check the issue”).\nWe experimented with a dataset of 87.5K real customer-agent utterance pairs from social media. We find that leveraging personality encoding improves relative performance up to 46% in BLEU score, compared to a baseline SEQ2SEQ model. To our knowledge, this work is the first to train a neural response generation model that encodes target personality traits."
    }, {
      "heading" : "2 Related Work",
      "text" : "Generating responses that express a target personality was previously discussed in different settings. Early work on the PERSONAGE system (Mairesse and Walker, 2007; Mairesse and Walker, 2008; Mairesse and Walker, 2010; Mairesse and Walker, 2011) presented a framework projecting different traits throughout the different modules of an NLG system. The authors explicitly defined 40 linguistic features as generation parameters, and then learned how to weigh them to generate a desired set of traits. While we aim at the same objective, our methodology is different and does not require feature engineering. Our approach utilizes a neural network that automatically learns to represent high level personality based features.\nNeural response generation models (Vinyals and Le, 2015; Shang et al., 2015) are based on a SEQ2SEQ architecture (Sutskever et al., 2014) and employ an encoder to represent the user utterance and an attention-based decoder that generates the agent response one token at a time. Models that aim to generate a coherent persona also exist. Li et al. (2016) modified a SEQ2SEQ model to encode a persona (the character of an artificial agent). The main difference with our work is that we focus on modeling the expression of specific personality traits and not an abstract character. Moreover, their personabased model can only generate responses for the agents that appear in the training data, while our model has no such restriction. Finally, Xu et al. (2017) generated responses for customer service re-\nquests on social media using standard SEQ2SEQ, while we modify it to generate a target personality."
    }, {
      "heading" : "3 Sequence-to-Sequence Setup",
      "text" : "We review the SEQ2SEQ attention based model on which our model is based.\nNeural response generation can be viewed as a sequence-to-sequence problem (Sutskever et al., 2014), where a sequence of input language tokens x = x1, . . . , xm , describing the user utterance, is mapped to a sequence of output language tokens y1, . . . , yn , describing the agent response.\nThe encoder is an LSTM (Hochreiter and Schmidhuber, 1997) unit that converts x1, . . . , xm into a sequence of context sensitive embeddings b1, . . . , bm. An attention-based decoder (Bahdanau et al., 2015; Luong et al., 2015) generates output tokens one at a time. At each time step j, it generates yj based on the current hidden state sj , then updates the hidden state sj+1 based on sj and yj . Formally, the decoder is defined by the following equations:\ns1 = tanh(W (s)bm), (1)\np(yj = w | x, y1:j−1) ∝ exp(U [sj , cj ]), (2) sj+1 = LSTM([φ(out)(yj), cj ], sj), (3)\nwhere i ∈ {1, . . . ,m}, j ∈ {1, . . . , n} and the context vector, cj , is the result of global attention (see (Luong et al., 2015)). The matricesW (s),W (a), U , and the embedding function φ(out) are decoder parameters. The entire model is trained end-to-end by maximizing p(y | x) = ∏nj=1 p(yj | x, y1:j−1)."
    }, {
      "heading" : "4 Personality Generation Model",
      "text" : "The model described in section 3 generates responses with maximum likelihood which reflect the consensus of the agents that appear in the training data. This kind of response does not characterize a specific personality and thus can result in inconsistent or unwanted personality cues. In this section we present our PERSONALITY-BASED model (Figure 2) which generates responses conditioned on a target set of personality traits values which the responses should express. The target set of personality traits is represented as a vector p, where pi represents the desired value for the ith trait. This value encodes\nhow strongly should this trait be expressed in the response. Consequently, the size of p depends on the selected personality model (e.g., five traits for the Big Five model).\nAs in (Mairesse and Walker, 2011), we argue that personality traits are exhibited as different types of stylistic linguistic variation. Thus, our model’s response is conditioned on generation parameters which are based on personality traits. In comparison to (Mairesse and Walker, 2011) where generation parameters were defined manually, we learn these high-level features automatically during training. We introduce a personality based features hidden layer hp = σ(W (p)p + b), where W (p) and b are parameters learned by the model during training. Each personality feature hi is a weighted sum of the targeted traits values (following a sigmoid activation). Now, at each token generation, the decoder updates the hidden state conditioned on the personality traits features hp, as well as on the previous hidden state, the output token and the context. Formally, Equation 3 is changed to:\nsj+1 = LSTM([φ(out)(yj), cj , hp], sj), (4)\nConditioning on hp captures the relation of text generation to the underlining personality traits."
    }, {
      "heading" : "5 Experiments",
      "text" : "Data. Our model is designed to generate text conditioned on a target set of personality traits. Specifically, we verified its performance in a scenario of customer service. For our experiments we utilized the dataset presented in (Xu et al., 2017), which exhibits a large variety of customer service properties. This dataset is a collection of 1M conversations over customer service Twitter channels of 62 different\nbrands which cover a large variety of product categories. Several preprocessing steps were performed for our purposes:\nWe first split the data to pairs consisting of a single customer utterance and its corresponding agent response. We removed pairs containing non-English sentences. We further removed pairs for agents that participated in less than 30 conversation pairs, so we would have sufficient data for each agent to extract their personality traits (see below). This resulted in 87.5K conversation pairs in total including 633 different agents (138±160 pairs per agent on average).\nFollowing (Sordoni et al., 2015; Li et al., 2016) we used BLEU (Papineni et al., 2002) for evaluation. Besides BLEU scores, we also report perplexity as an indicator of model capability. For implementation details, refer to Appendix A.\nResults. We experimented with two different settings to measure our model’s performance.\nWarm Start: In the first experiment, data for each agent in the dataset was split between training, validation and test data sets with a fraction of 80%/10%/10%, respectively. We then extracted the agents’ personality traits using an external service (described in Appendix B), from the training data for each agent. These personality traits values are then used during the model training as the values for the personality vector p. In this setting, since all the agents that appear in the test data appear also in the training data, we can also test the performance of (Li et al., 2016), which learns a persona vector for each agent in the training data.\nThe results in table 1 show that the standard SEQ2SEQ model achieved the lowest performance in terms of both perplexity and BLEU score while the competing models which learn a representation\nfor the agents achieved higher performance. The PERSONA-BASED model achieved similar perplexity but higher BLEU score than our model. This is reasonable since PERSONA-BASED is not restricted to personality based features. However, this model can not generate content for agents which do not appear in the training data, and thus, it is limited.\nCold Start: In our second experiment, we split the dataset such that 10% of the agents only formed the validation and test sets (half of each agent’s examples for each set). Data for the other 90% of the agents formed the training set.\nIn this setting, data for agents in the test set does not appear in the training set. These agents represent new personality distributions we would like to generate responses for. Note that, we extracted target personality traits for agents in the training set using their training data, or, for agents in the test set, using validation data. In this setting, it is not possible to test the PERSONA-BASED model since no representation is learned during training for agents in the test set. Thus, we only compare our model to the baseline SEQ2SEQ model. Table 2 shows that, in this setting, we get better performance by utilizing personality based representation: our model achieves a relative 6.7% decrease in perplexity, and a 46% relative improvement in BLEU score. Results from both experiments demonstrate that we can better model the linguistic variation in agent responses by conditioning on target personality traits.\nHuman Evaluation. We conducted a human evaluation of our PERSONALITY-BASED model using a crowd-sourcing service. This evaluation measures whether the responses generated by our model are correlated with the target personality traits. We focused on two personality traits from the Big Five model that are important to customer service: agreeableness and conscientiousness (Blignaut et al., 2014; Sackett, 2014). We extracted 60 customer utterances from the validation set of the cold start setting described above. We selected customer utterances that convey a negative sentiment, since re-\nsponses to this kind of utterances vary much. After sentences were selected, we generated corresponding agent responses in the following way. We generated a high-trait target personality distribution (trait was either agreeableness or conscientiousness), where trait was set to a value of 0.9, and all other traits to 0.5. Similarly, we created a low-trait version where trait was set to 0.1. For each trait and customer utterance we generated a response for the high-trait and low-trait versions.\nEach triplet (a customer utterance followed by high-trait and low-trait generated responses) was evaluated by five master level judges. To get the judges familiar with personality traits, we first presented clear definitions of the two traits, followed by several examples (from the task’s domain), and explanation. Following Li et al. (2016) methodology, the two responses were presented in a random order, and judged on a 5-point zero-sum scale. A score of 2 (−2) was assigned if one response was judged to express the trait more (less) than the other response, and 1 (−1) if one response expressed the trait “somewhat” more (less) than the other. Ties were assigned a score of zero.\nThe judges rated each pair, and their scores were averaged and mapped into 5 equal-width bins. After discarding ties, we found that the high-trait responses generated by our PERSONALITY-BASED model were judged either more expressive or somewhat more expressive than the low-trait corresponding responses in 61% of cases. If we ignore the somewhat more expressive judgments, the high-trait responses win in 17% of cases."
    }, {
      "heading" : "6 Conclusions and Future Work",
      "text" : "We have presented a personality-based response generation model and tested it in customer care tasks, outperforming baseline SEQ2SEQ model. In future work, we would like to generate responses adapted to the personality traits of the customer as well, and to apply our model to other tasks such as education systems."
    }, {
      "heading" : "B Personality Traits Detection",
      "text" : "To extract personality traits for agents in our experiments we utilized the IBM Personality Insights service, which is publicly available. This service infers three models of personality traits, namely, Big Five, Needs and Values from social media text. It extracts percentile scores for 52 traits1.\n1www.ibm.com/watson/developercloud/doc/ personality-insights/models.html"
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Personality as predictor of customer service centre agent performance in the banking industry: An exploratory study",
      "author" : [ "Linda Blignaut", "Leona Ungerer", "Helene Muller." ],
      "venue" : "SA Journal of Human Resource Management, 12(1).",
      "citeRegEx" : "Blignaut et al\\.,? 2014",
      "shortCiteRegEx" : "Blignaut et al\\.",
      "year" : 2014
    }, {
      "title" : "Personality structure: Emergence of the five-factor model",
      "author" : [ "John M Digman." ],
      "venue" : "Annual review of psychology, 41(1):417–440.",
      "citeRegEx" : "Digman.,? 1990",
      "shortCiteRegEx" : "Digman.",
      "year" : 1990
    }, {
      "title" : "Predicting customer satisfaction in customer support conversations in social media using affective features",
      "author" : [ "Jonathan Herzig", "Guy Feigenblat", "Michal ShmueliScheuer", "David Konopnicki", "Anat Rafaeli." ],
      "venue" : "UMAP 2016, Halifax, NS, Canada, July 13 - 17,",
      "citeRegEx" : "Herzig et al\\.,? 2016",
      "shortCiteRegEx" : "Herzig et al\\.",
      "year" : 2016
    }, {
      "title" : "Long shortterm memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber." ],
      "venue" : "Neural Computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "A persona-based neural conversation model",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Georgios P. Spithourakis", "Jianfeng Gao", "William B. Dolan." ],
      "venue" : "ACL.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "M. Luong", "H. Pham", "C.D. Manning." ],
      "venue" : "EMNLP, pages 1412–1421.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Personage: Personality generation for dialogue",
      "author" : [ "Franois Mairesse", "Marilyn Walker." ],
      "venue" : "ACL, pages 496–503.",
      "citeRegEx" : "Mairesse and Walker.,? 2007",
      "shortCiteRegEx" : "Mairesse and Walker.",
      "year" : 2007
    }, {
      "title" : "Trainable generation of big-five personality styles through data-driven parameter estimation",
      "author" : [ "François Mairesse", "Marilyn A Walker." ],
      "venue" : "ACL, pages 165– 173.",
      "citeRegEx" : "Mairesse and Walker.,? 2008",
      "shortCiteRegEx" : "Mairesse and Walker.",
      "year" : 2008
    }, {
      "title" : "Towards personality-based user adaptation: psychologically informed stylistic language generation",
      "author" : [ "François Mairesse", "Marilyn A. Walker." ],
      "venue" : "User Model. User-Adapt. Interact., 20(3):227–278.",
      "citeRegEx" : "Mairesse and Walker.,? 2010",
      "shortCiteRegEx" : "Mairesse and Walker.",
      "year" : 2010
    }, {
      "title" : "Controlling user perceptions of linguistic style: Trainable generation of personality traits",
      "author" : [ "François Mairesse", "Marilyn A. Walker." ],
      "venue" : "Computational Linguistics, 37(3):455–488.",
      "citeRegEx" : "Mairesse and Walker.,? 2011",
      "shortCiteRegEx" : "Mairesse and Walker.",
      "year" : 2011
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "ACL, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "How people treat computers, television, and new media like real people and places",
      "author" : [ "Byron Reeves", "Clifford Nass." ],
      "venue" : "CSLI Publications and Cambridge.",
      "citeRegEx" : "Reeves and Nass.,? 1996",
      "shortCiteRegEx" : "Reeves and Nass.",
      "year" : 1996
    }, {
      "title" : "Which personality attributes are most important in the workplace",
      "author" : [ "P.R. Walmsley P.T. Sackett" ],
      "venue" : "Perspectives on Psychological Science,",
      "citeRegEx" : "Sackett,? \\Q2014\\E",
      "shortCiteRegEx" : "Sackett",
      "year" : 2014
    }, {
      "title" : "A neural conversa",
      "author" : [ "Oriol Vinyals", "Quoc Le" ],
      "venue" : null,
      "citeRegEx" : "Vinyals and Le.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "sonality according to the task at hand (Reeves and Nass, 1996; Tapus and Mataric, 2008) or to the customer (Herzig et al.",
      "startOffset" : 39,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "sonality according to the task at hand (Reeves and Nass, 1996; Tapus and Mataric, 2008) or to the customer (Herzig et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "Many models of personality exist while the most common one is the Big Five model (Digman, 1990) , including: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism.",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 7,
      "context" : "These traits were correlated with linguistic choices including lexicon and syntax (Mairesse and Walker, 2007).",
      "startOffset" : 82,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "Results indicate that conscientiousness (a person’s tendency to act in an organized or thoughtful way) and agreeableness (a person’s tendency to be compassionate and cooperative toward others) correlate with service quality (Blignaut et al., 2014; Sackett, 2014).",
      "startOffset" : 224,
      "endOffset" : 262
    }, {
      "referenceID" : 13,
      "context" : "Results indicate that conscientiousness (a person’s tendency to act in an organized or thoughtful way) and agreeableness (a person’s tendency to be compassionate and cooperative toward others) correlate with service quality (Blignaut et al., 2014; Sackett, 2014).",
      "startOffset" : 224,
      "endOffset" : 262
    }, {
      "referenceID" : 14,
      "context" : "Neural response generation models (Vinyals and Le, 2015; Shang et al., 2015) are based on a SEQ2SEQ architecture (Sutskever et al.",
      "startOffset" : 34,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "The encoder is an LSTM (Hochreiter and Schmidhuber, 1997) unit that converts x1, .",
      "startOffset" : 23,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "An attention-based decoder (Bahdanau et al., 2015; Luong et al., 2015) generates output tokens one at a time.",
      "startOffset" : 27,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "An attention-based decoder (Bahdanau et al., 2015; Luong et al., 2015) generates output tokens one at a time.",
      "startOffset" : 27,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : ", n} and the context vector, cj , is the result of global attention (see (Luong et al., 2015)).",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "As in (Mairesse and Walker, 2011), we argue that personality traits are exhibited as different types of stylistic linguistic variation.",
      "startOffset" : 6,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "In comparison to (Mairesse and Walker, 2011) where generation parameters were defined manually, we learn these high-level features automatically during train-",
      "startOffset" : 17,
      "endOffset" : 44
    }, {
      "referenceID" : 5,
      "context" : "Following (Sordoni et al., 2015; Li et al., 2016) we used BLEU (Papineni et al.",
      "startOffset" : 10,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : ", 2016) we used BLEU (Papineni et al., 2002) for evaluation.",
      "startOffset" : 21,
      "endOffset" : 44
    }, {
      "referenceID" : 5,
      "context" : "In this setting, since all the agents that appear in the test data appear also in the training data, we can also test the performance of (Li et al., 2016), which learns a persona vector for each agent in the training data.",
      "startOffset" : 137,
      "endOffset" : 154
    }, {
      "referenceID" : 1,
      "context" : "We focused on two personality traits from the Big Five model that are important to customer service: agreeableness and conscientiousness (Blignaut et al., 2014; Sackett, 2014).",
      "startOffset" : 137,
      "endOffset" : 175
    }, {
      "referenceID" : 13,
      "context" : "We focused on two personality traits from the Big Five model that are important to customer service: agreeableness and conscientiousness (Blignaut et al., 2014; Sackett, 2014).",
      "startOffset" : 137,
      "endOffset" : 175
    } ],
    "year" : 2017,
    "abstractText" : "We present a neural response generation model that generates responses conditioned on a target personality. The model learns high level features based on the target personality, and uses them to update its hidden state. Our model achieves performance improvements in both perplexity and BLEU scores over a baseline sequence-to-sequence model, and is validated by human judges.",
    "creator" : "LaTeX with hyperref package"
  }
}