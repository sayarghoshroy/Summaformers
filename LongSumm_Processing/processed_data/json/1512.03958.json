15:15:45.751 [main] DEBUG com.amazonaws.AmazonWebServiceClient - Internal logging successfully configured to commons logger: true
15:15:45.815 [main] DEBUG com.amazonaws.metrics.AwsSdkMetrics - Admin mbean registered under com.amazonaws.management:type=AwsSdkMetrics
15:15:45.876 [main] DEBUG c.a.internal.config.InternalConfig - Configuration override awssdk_config_override.json not found.
15:15:46.258 [scala-execution-context-global-12] INFO  org.allenai.scienceparse.Parser - Loading gazetteer from /home/risubaba/.ai2/datastore/public/org.allenai.scienceparse/gazetteer-v5.json
15:15:46.258 [ModelLoaderThread] INFO  org.allenai.scienceparse.Parser - Loading model from /home/risubaba/.ai2/datastore/public/org.allenai.scienceparse/productionModel-v9.dat
15:15:46.259 [scala-execution-context-global-12] INFO  org.allenai.scienceparse.Parser - Loading bib model from /home/risubaba/.ai2/datastore/public/org.allenai.scienceparse/productionBibModel-v7.dat
15:15:46.264 [scala-execution-context-global-12] INFO  org.allenai.scienceparse.Parser - Creating gazetteer cache at /tmp/gazetteer-v5.json-fa485aef.gazetteerCache.bin
15:15:58.705 [scala-execution-context-global-12] INFO  o.a.scienceparse.ParserGroundTruth - Read 1609659 papers.
15:16:13.422 [ModelLoaderThread] INFO  org.allenai.scienceparse.Parser - Loaded model from /home/risubaba/.ai2/datastore/public/org.allenai.scienceparse/productionModel-v9.dat
15:16:26.128 [scala-execution-context-global-12] INFO  o.a.scienceparse.ExtractReferences - could not load kermit gazetter
15:16:26.195 [scala-execution-context-global-12] INFO  org.allenai.scienceparse.Parser - Loaded gazetteer from /home/risubaba/.ai2/datastore/public/org.allenai.scienceparse/gazetteer-v5.json
15:16:26.195 [scala-execution-context-global-12] INFO  org.allenai.scienceparse.Parser - Loaded bib model from /home/risubaba/.ai2/datastore/public/org.allenai.scienceparse/productionBibModel-v7.dat
15:16:26.200 [scala-execution-context-global-12] INFO  org.allenai.scienceparse.RunSP$ - Starting /home/risubaba/LongSumm/pdf/1512.03958.pdf
{
  "name" : "/home/risubaba/LongSumm/pdf/1512.03958.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Guy Lev", "Gil Sadeh", "Benjamin Klein", "Lior Wolf" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 2.\n03 95\n8v 1\n[c s.\nC V\n] 12\nD ec\nRecurrent Neural Networks (RNNs) have had considerable success in classifying and predicting sequences. We demonstrate that RNNs can be effectively used in order to encode sequences and provide effective representations. The methodology we use is based on Fisher Vectors, where the RNNs are the generative probabilistic models and the partial derivatives are computed using backpropagation. State of the art results are obtained in two central but distant tasks, which both rely on sequences: video action recognition and image annotation. We also show a surprising transfer learning result from the task of image annotation to the task of video action recognition."
    }, {
      "heading" : "1. Introduction",
      "text" : "Fisher Vectors have been shown to provide a significant performance gain on many different applications in the domain of computer vision [39, 33, 2, 35]. In the domain of video action recognition, Fisher Vectors and Stacked Fisher Vectors [33] have recently outperformed state-of-theart methods on multiple datasets [33, 53]. Fisher Vectors (FV) have also recently been applied to word embedding (e.g. word2vec [30]) and have been shown to provide state of the art results on a variety of NLP tasks [24], as well as on image annotation and image search tasks [18].\nIn all of these contributions, the FV of a set of local descriptors is obtained as a sum of gradients of the loglikelihood of the descriptors in the set with respect to the parameters of a probabilistic mixture model that was fitted on a training set in an unsupervised manner. In spite of being richer than the mean vector pooling method, Fisher Vectors based on a probabilistic mixture model are invariant to order. This makes them less appealing for annotating, for example, video, in which the sequence of events determines much of the meaning.\nThis work presents a novel approach for FV representation of sequences using a Recurrent Neural Network\n(RNN). The RNN is trained to predict the next element of a sequence given the previous elements. Conveniently, the gradients needed for the computation of the FV are extracted using the available backpropagation infrastructue.\nThe new representation is sensitive to ordering and therefore mitigates the disadvantage of using the standard Fisher Vector representation. It is applied to two different and challenging tasks: video action recognition and image annotation by sentences.\nSeveral recent works have proposed to use an RNN for sentence representation [44, 1, 31, 17]. The Recurrent Neural Network Fisher Vector (RNN-FV) method differs from these works in that a sequence is represented by using derived gradient from the RNN as features, instead of using a hidden or an output layer of the RNN.\nThe paper explores two different approaches for training the RNN for the image annotation and image search tasks. In the classification approach, the RNN is trained to predict the following word in the sentence. The regression approach tries to predict the embedding of the following word (i.e. treating it as a regression task). The large vocabulary size makes the regression approach more scalable and achieves better results than the classification approach. In the video action recognition task, the regression approach is the only variant being used, since the notion of a discrete word does not exist. The VGG [41] Convolutional Neural Network (CNN) is used to extract features from the frames of the video and the RNN is trained to predict the embedding of the next frame given the previous ones. Similarly, C3D [46] features of sequential video sub-volumes are used with the same training technique.\nAlthough the image annotation and video action recognition tasks are quite different, a surprising boost in performance in the video action recognition task was achieved by using a transfer learning approach from the image annotation task. Specifically, the VGG image embedding of a frame is projected using a linear transformation which was learned on matching images and sentences by the Canonical Correlation Analysis (CCA) algorithm [10].\n1\nThe proposed RNN-FV method achieves state-of-theart results in action recognition on the HMDB51 [20] and UCF101 [43] datasets. In image annotation and image search tasks, the RNN-FV method is used for the representation of sentences and achieves state-of-the-art resul s on the Flickr8K dataset [8] and competitive results on other benchmarks."
    }, {
      "heading" : "2. Previous Work",
      "text" : "Action Recognition As in other object recognition problems, the standard pipeline in action recognition is comprised of three main steps: feature extraction, pooling and classification. Many works [23, 49, 19] have focused on the first step of extracting local descriptors. Laptev et al. [22] extend the notion of spatial interest points into the spatiotemporal domain and show how the resulting features can be used for a compact representation of video data. Wang et al. [51, 50] used low-level hand-crafted features such as histogram of oriented gradients (HOG), histogram of optical flow (HOF) and motion boundary histogram (MBH).\nRecent works have attempted to replace these handcrafted features by deep-learned features for video action recognition due to its wide success in the image domain. Early attempts [45, 12, 15] achieved lower results in comparison to hand-crafted features, proving that it is challenging to apply deep-learning techniques on videos due to the relatively small number of available datasets and complex motion patterns. More recent attempts managed to overcome these challenges and achieve state of the art results with deep-learned features. Simonyan et al. [40] designed two-stream ConvNets for learning both the appearance of the video frame and the motion as reflected by the estimated optical flow. Du Tran et al. [46] designed an effective approach for spatiotemporal feature learning using 3-dimensional ConvNets.\nIn the second step of the pipeline, the pooling, Wang et al. [54] compared different pooling techniques for the application of action recognition and showed empirically that the Fisher Vector encoding has the best performance. Recently, more complex pooling methods were demonstrated by Peng et al. [33] who proposed Stacked Fisher Vectors (SFV), a multi-layer nested Fisher Vector encoding and Wang et al. [53] who proposed a trajectory-pooled deepconvolutional descriptor (TDD). TDD uses both a motion CNN, trained on UCF101, and an appearance CNN, originally trained on ImageNet [3], and fine-tuned on UCF101.\nImage Annotation and Image Search In the past few years, the state-of-the-art results in image annotation and image search have been provided by deep learning approaches [42, 29, 18, 14, 27, 16, 4, 13, 48, 26]. A typical system is composed of three important components: (i) Image Representation, (ii) Sentence Representation, and (iii)\nMatching Images and Sentences. The image is usually represented by applying a pre-trained CNN on the image and taking the activations from the last hidden layer.\nThere are several different approaches for the sentence representation; Socher et al. [42] used a dependency tree Recursive Neural Network. Yan et al. [29] used a TFIDF histogram over the vocabulary. Klein et al. [18] used word2vec [30] as the word embedding and then applied Fisher Vector based on a Hybrid Gaussian-Laplacian Mixture Model (HGLMM) in order to pool the word2vec embeddings of the words in a given sentence into a single representation. Ma et al. [26] proposed a matching CNN (mCNN) that composes words to different semantic fragments and learns the inter-modal relations between image and the composed fragments at different levels.\nSince a sentence can be seen as a sequence of words, many works have used a Recurrent Neural Network (RNN) in order to represent sentences [14, 48, 27, 16, 17]. To address the need for capturing long term semantics in the sentence, these works mainly use Long Short-Term Memory (LSTM) [7] or Gated Recurrent Unit (GRU) [5] cells. Generally, the RNN treats a sentence as an ordered sequence of words, and incrementally encodes a semantic vector of the sentence, word-by-word. At each time step, a new word is encoded into the semantic vector, until the end of the sentence is reached. All of the words and their dependencies will then have been embedded into the semantic vector, which can be used as a feature vector representation of the entire sentence. Our work also uses an RNN in order to represent sentences but takes the derived gradient from the RNN as features, instead of using a hidden or an output layer of the RNN.\nA number of techniques have been proposed for the task of matching images and sentences. Klein et al. [18] used CCA [10] and Yan et al. [29] introduced a Deep CCA in order to project the images and sentences into a common space and then performed a nearest neighbor search between the images and the sentences in the common space. Kiros et al. [16], Karpathy et al.[14], Socher et al. [42] and Ma et al. [26] used a contrastive loss function trained on matching and unmatching pairs of (image,sentence) in order to learn a score function for a given pair. Mao et al. [27] and Vinyals et al. [48] learned a probabilistic model for inferring a sentence given an image and, therefore, are able to compute the probability that a given sentence will be created by a given image and used it as the score."
    }, {
      "heading" : "3. Baseline pooling methods",
      "text" : "In this section we describe two baseline pooling methods that can represent a multiset of vectors as a single vector. The notation of a multiset is used to clarify that the order of the words in a sentence does not affect the representation, and that a vector can appear more than once. Both methods\ncan be applied to sequences, however, the resulting representation will be insensitive to ordering. To address this, we propose in Sec.4 a novel pooling method: RNN-FV."
    }, {
      "heading" : "3.1. Mean Vector",
      "text" : "This pooling technique takes a multiset of vectors, X = {x1, x2, . . . , xN} ∈ R\nD, and computes its mean: v = 1N ∑N i=1 xi. Clearly, the vectorv that results from the pooling is inRD. The disadvantage of this method is the blurring of the multiset’s content. Consider, for example, the text encoding task, where each word is represented by its word2vec embedding. By adding multiple vectors together, the location obtained – in the semantic embedding space – is somewhere in the convex hull of the words that belong to the multiset."
    }, {
      "heading" : "3.2. Fisher Vector of a GMM",
      "text" : "Given a multiset of vectors,X = {x1, x2, . . . , xN} ∈ RD, the standard FV [34] is defined as the gradient of the log-likelihood of X with respect to the parameters of a pre-trained Diagonal-Covariance Gaussian Mixture Model (GMM). It is a common practice to limit the FV representation to the partial derivatives with respect to the means,µ, and the standard deviations,σ, and ignore the partial derivatives with respect to the mixture weights.\nIt is worth noting the linear structure of the GMM FV pooling. Since the likelihood of the multiset is the multiplication of the likelihoods of the individual elements, the log-likelihood is additive. This convenient property would not be preserved in the RNN model, where the probability of an element in the sequence depends on all the previous elements.\nTo all types of FV, we apply the two improvements that were introduced by Perronnin et al. [35]. The first improvement is to apply an element-wise power normalization function, f(z) = sign(z)|z|α where0 ≤ α ≤ 1 is a parameter of the normalization. The second improvement is to apply an L2 normalization on the FV after applying the power normalization function."
    }, {
      "heading" : "4. RNN-Based Fisher Vector",
      "text" : "The pooling methods described above share a common disadvantage: insensitivity to the order of the elements in the sequence. A way to tackle this, while keeping the power of gradient-based representation, would be to replace the Gaussian model by a generative sequence model that takes into account the order of elements in the sequence. A desirable property of the sequence model would be the ability to calculate the gradient (with respect to the model’s parameters) of the likelihood estimate by this model to an input sequence.\nIn this section, we show that such a model can be obtained by training an RNN to predict the next element in a\nsequence, given the previous elements. Having this, we propose, for the first time, the RNN-FV: A Fisher Vector that is based on such an RNN sequence model.\nWe propose two types of RNN-FVs. One type is based on training a regression problem, and the other on training a classification problem. In practice, only the first type is directly useful for video analysis. For image annotation, the first type outperforms the second.\nGiven a sequence of vectorsS with N vector elementsx1, ..., xN , we convert it to the input sequenceX = (x0, x1, ..., xN−1), wherex0 = xstart. This special element is used to denote the beginning of the input sequence, and we usexstart = 0 throughout this paper. The RNN is trained to predict, at each time stepi, the next element xi+1 of the sequence, given the previous elementsx0, ..., xi. Therefore, given the input sequence, the target sequence would be:Y = (x1, x2, ...xN ).\nThe training data and the training process are application dependent, as is described in Sec.5 for action recognition and in Sec.6 for image annotation."
    }, {
      "heading" : "4.1. RNN Trained for Regression",
      "text" : "Given a sequence of input vectorsX , the regression RNN is trained to predict the next vector in the sequence S, i.e., the sequenceY . The output layer of the network is a fully-connected layer, the size of which would beD, i.e., the dimension of the input vector space.\nThere are several regression loss functions that can be used. Here, we consider the following loss function:\nLoss(y, v) = 1\n2 ‖y − v‖2 (1)\nwherey is the target vector andv is the predicted vector. After the RNN training is done, and given a new sequenceS, the derived sequenceX is fed to the RNN. Denote the output of the RNN at time stepi (i = 0, ..., N − 1) by RNN(x0, ..., xi) = vi ∈ RD. The target at time stepi is xi+1 (the next element in the sequence), and the loss is:\nLoss(xi+1, vi) = 1\n2 ‖xi+1 − vi‖\n2 (2)\nThe RNN can be seen as a generative model, and the likelihood of any vectorx being the next element of the sequence, givenx0, ..., xi, can be defined as:\np (x|x0, ..., xi) = (2π) −D/2 exp\n(\n− 1\n2 ‖x− vi‖\n2\n)\n(3)\nWe are generally interested in the likelihood of the correct prediction, i.e., in the likelihood of the vectorxi+1 givenx0, ..., xi: p (xi+1|x0, ..., xi).\nThe RNN-based likelihood of the entire sequence X is:\np(X) =\nN−1 ∏\ni=0\np (xi+1|x0, ..., xi) (4)\nThe negative log likelihood ofX is:\nL(X) = − log (p(X)) = −\nN−1 ∑\ni=0\nlog (p (xi+1|x0, ..., xi))\n= ND\n2 log(2π) +\n1\n2\nN−1 ∑\ni=0\n‖xi+1 − vi‖ 2\n(5)\nIn order to representX using the Fisher Vector scheme, we have to compute the gradient ofL(X) with respect to our model’s parameters. With RNN being our model, the parameters are the weightsW of the network. By (2) and (5), we get thatL(X) equals the loss that would be obtained whenX is fed as input to the RNN, up to an additive constant. Therefore, the desired gradient can be computed by backpropagation: we feedX to the network and perform forward and backward passes. The obtained gradient ∇WL(X) would be the (unnormalized) RNN-FV representation ofX . Notice that this gradient isnot used to update the network’s weights as done in training - here we perform backpropagationat inference time.\nOther loss functions may be used instead of the one presented in this analysis. Given a sequence, the gradient of the RNN loss may serve as the sequence representation, even if the loss is not interpretable as a likelihood."
    }, {
      "heading" : "4.2. RNN Trained for Classification",
      "text" : "The classification application is applicable for predicting a sequence of symbolsw1,w2,...,wN that have matching vector representationsR(w1) = x1, R(w2) = x2, ..., R(wN ) = xN . The RNN predicts the sequenceU = (w1, w2, . . . , wN ) from the sequenceX = (x0, x1, . . . , xN−1).\nDenote byM the size of our symbol alphabet, i.e., the number of unique symbols in the input sequences. The output layer of the network is a softmax layer withM units, where thej’th element in the output is the probability of the j’th symbol to be the next output element. The loss function for the training of the RNN is the cross-entropy loss.\nAfter the RNN is trained, it is ready to be used as a feature vector extractor for new sequences. Denote the new sequence byU and its vector representation byX as above. Consider feeding the sequenceX to the RNN. At time stepi (i = 0, ..., N − 1), the output of the RNN is RNN(x0, ..., xi) = (pi1, ..., p i M ), where ∑M j=1 p i j = 1. Here,pij is the probability which the RNN gives to thej’th symbol at time stepi.\nThe cross-entropy loss at time stepi is derived from the probability given to the correct next symbol:\nlossi = − log ( piwi+1 ) = − log (Pr (wi+1|w0, ..., wi))\n(6)\nThe RNN can be seen as a generative model which gives likelihood to the sequenceU :\nPr(U) =\nN−1 ∏\ni=0\nPr (wi+1|w0, ..., wi) =\nN−1 ∏\ni=0\npiwi+1 (7)\nThe negative log likelihood ofU is:\nL(U) = − log (Pr(U)) = −\nN−1 ∑\ni=0\nlog (\npiwi+1\n)\n(8)\nBy (6) and (8), we get thatL(U) equals the loss that would be obtained whenX is fed as input, andU as output to the RNN. Therefore, the desired gradient can be computed by backpropagation, i.e. feedingX to the network and performing forward and backward passes. The obtained gradient∇WL(U) would be the (unnormalized) RNN-FV representation ofU ."
    }, {
      "heading" : "4.3. Normalization of the RNN-FV",
      "text" : "It was suggested by [34] that normalizing the FVs by the Fisher Information Matrix is beneficial. We approximated the diagonal of the Fisher Information Matrix (FIM), which is usually used for FV normalization. Note, however, that we did not observe any empirical improvement due to this normalization, and our experiments are reported without it.\nLet ω ∈ W be a single weight of the RNN. The term in the diagonal of the FIM which corresponds to∂L(X|W )∂ω is: Fω = ∫\nX p (X |W )\n[\n∂L(X|W ) ∂ω\n]2\ndX .\nSince the probabilistic model which determines p (X |W ) is the RNN, it is impossible to derive a closedform expression for this term. Therefore, we approximated it directly from the gradients of the training sequences, by computing the mean of [\n∂L(X|W ) ∂ω\n]2\nfor eachω ∈ W .\nThe normalized partial derivatives of the FV are then: F −1/2 w ∂L(X|W ) ∂ω ."
    }, {
      "heading" : "5. Action recognition pipeline",
      "text" : "The action recognition pipeline contains the underlying appearance features used to encode the video, the sequence encoding using the RNN-FV, and an SVM classifier on top."
    }, {
      "heading" : "5.1. Visual features",
      "text" : "The RNN-FV is capable of encoding the sequence properties, and as underlying features, we rely on video encodings that are based on single frames or on fixed length blocks of frames. VGG Using the pre-trained VGG convolutional network [41], we extract a 4096-dimensional representation of each video frame. The VGG pipeline is used, namely, the original image is cropped in ten different ways into 224 by\n224 pixel images: the four corners, the center, and their xaxis mirror image. The mean intensity is then subtracted in each color channel and the resulting images are encoded by the network. The average of the 10 feature vectors obtained is then used as the single image representation. In order to speed up the method, the input video was sub-sampled, and one in every 10 frames was encoded. Empirically, we noticed that recognition performance was comparable to that of using all video frames. To further reduce run-time, the data dimensionality was reduced via PCA to 500D. In addition, L2 normalization was applied to each vector. All PCAs in this work were trained for each dataset and each training/test split separately, using only the training data. CCA Using the same VGG representation of video frames as mentioned above and the code of [18]1, we represented each frame by a vector as follows: we considered the common image-sentence vector space obtained by the CCA algorithm, using the best model (GMM+HGLMM) of [18] trained on the COCO dataset [25]. We mapped each frame to that vector space, getting a 4096-dimensional image representation. As the final frame representation, we used the first (i.e. the principal) 500 dimensions out of the 4096. For our application, the projected VGG representations were L2 normalized. The CCA was trained for an unrelated task of image to sentence matching, and its success, therefore, suggests a new application of transfer learning: from image annotation to action recognition. C3D While the representations above encode single frames, the C3D method [46] splits the video into sub-volumes that are encoded one by one. Following the recommended settings, we applied the Du Tran et al. pre-trained 3D convolutional neural network in order to extract 4096D representation to 16-frame blocks. The blocks are sampled with an 8 frame stride. Following feature extraction, PCA dimensionality reduction (500D) and L2 normalization were applied."
    }, {
      "heading" : "5.2. Network structure",
      "text" : "Our RNN model consists of three layers: a 200D fullyconnected layer units with Leaky-Relu activation (α = 0.1), a 200-units Long Short-Term Memory (LSTM) [7] layer, and a 500D linear fully-connected layer. Our network is trained for regression with the mean square error (MSE) loss function. Weight decay and dropouts were also applied. An improvement in recognition performance was noticed when the dropout rate was enlarged, up to a rate of 0.95, due to its ability to ensure the discriminative characterisics of each weight and hence also of each gradient."
    }, {
      "heading" : "5.3. Training and classification",
      "text" : "We train the RNN to predict the next element in our video representation sequence, given the previous elements, as described in Sec.4.1. In our experiments, we use only\n1Available atwww.cs.tau.ac.il/~wolf/code/hglmm\nthe part of gradient corresponding to the weights of the last fully-connected layer. Empirically, we saw no improvement when using the partial derivatives with respect to weights of other layers. In order to obtain a fixed size representation, we average the gradients over all time steps. The gradient representation dimension is 500x201=100500, which is the number of weights in the last fully-connected layer. We then apply PCA to reduce the representation size to 1000D, followed by power and L2 normalization.\nVideo classification is performed using a linear SVM with a parameterC = 1. Empirically, we noticed that the the best recognition performance is obtained very quickly and hence early stopping is necessary. In order to choose an early stopping point we use a validation set. Some of the videos in the dataset are actually segments of the same original video, and are included in the dataset as different samples. Care was taken to ensure that no such similar videos are both in the training and validation sets, in order to guarantee that high validation accuracy will ensure good generalization and not merely over-fitting.\nAfter each RNN epoch, we extract the RNN-FV representation as described above, train a linear SVM classifier on the training set and evaluate the performance on the validation set. The early stopping point is chosen at the epoch with highest recognition accuracy on the validation set. After choosing our model this way, we train an SVM classifier on all training samples (training + validation samples) and report our performance on the test set."
    }, {
      "heading" : "6. Image-sentence retrieval",
      "text" : "In the image-sentence retrieval tasks, vector representations are extracted separately for the sentences and the images. These representations are then mapped into a common vector space, where the two are being matched. [18] have presented a similar pipeline for GMM-FV. We replace this representation with RNN-FV.\nA sentence, being an ordered sequence of words, can be represented as a vector using the RNN-FV scheme. Given a sentence withN wordsw1, ..., wN , (wherewN is considered to be the period, namely awend special token), we treat the sentence as an ordered sequenceS = (w0, w1, ..., wN−1), wherew0 = wstart. An RNN is trained to predict, at each time stepi, the next wordwi+1 of the sentence, given the previous wordsw0, ..., wi. Therefore, given the input sequenceS, the target sequence would be: (w1, w2, ...wN ).\nThe training data may be any large set of sentences. These sentences may be extracted from the dataset of a specific benchmark, or, in order to obtain a generic representation, any external corpus, e.g., Wikipedia, may be used.\nThe two network alternatives are explored: classification and regression. As observed in the action recognition case, we did not benefit from extracting partial derivatives with\nrespect to the weights of the hidden layers, and hence we only use those of the output layer as our representation.\nWhen the RNN is trained for classification, each word in the dictionary is considered as a class. The input to the network is the word’s embedding, a 300D vector in our case. The hidden layer is LSTM with 512 units, which is followed by a softmax output layer. This design creates two challenges. The first is dimensionality: the size of the softmax layer is the size of the dictionary,M , which is typically large. As a result,∇WL(X) has a high dimensionality. The second issue is with generalization capability: since the softmax layer is fixed, a network cannot handle a sentence containing a word that does not appear in its training data.\nWhen training the RNN for regression, the same 300D input is used, followed by an LSTM layer of size 100. The output layer, in this case, is fully-connected, where the (300 dimensional) word embedding of next word is predicted. We use no activation function at the output layer. Notice that the two issues pointed out regarding the classification RNN are not present in the regression case. First, the size D of the output layer depends only on the dimension of the word embedding. Second, the network can naturally handle unseen words, since it predicts vectors in the word vector space rather than an index of a specific word.\nFor matching images and text, each image is represented as a 4096-dimensional vector extracted using the 19-layer VGG, as described in Sec.5 1. The regularized CCA algorithm [47], where the regularization parameter is selected based on the validation set, is used to match the the VGG representation with the sentence RNN-FV representation. In the shared CCA space, the cosine similarity is used.\nWe explored several configurations for training the RNN. RNN training data We employed either the training data of each split in the respective benchmark, or the 2010-EnglishWikipedia-1M dataset made available by the Leipzig Corpora Collection [38]. This dataset contains 1 million sentences randomly sampled from English Wikipedia.Word embeddingA word was represented either by word2vec, or by the GMM+HGLMM representation of [18], projected to a 300D sentence to VGG-encoded-image CCA space. We made sure to match the training split according to the benchmark tested. Sentence sequence directionWe explored both the conventional left-to-right sequence of words and the reverse direction."
    }, {
      "heading" : "7. Experiments",
      "text" : "We evaluated the effectiveness of the various pooling methods on two important yet distinct application domains: action recognition and image textual annotation and search.\nAs mentioned, applying the FIM normalization (Sec.4.3) did not seem to improve results. Another form of normalization we have tried, is to normalize each dimension of the gradient by subtracting its mean and\ndividing by its standard deviation. This also did not lead to an improved performance. Two normalizations that were found to be useful are the Power Normalization and the L2 Normalization, which were introduced in [36] (see Section2). Both are employed, using a constantα = 1/2."
    }, {
      "heading" : "7.1. Action recognition",
      "text" : "Our experiments were conducted on two large action recognition benchmarks. The UCF101 [43] dataset consists of 13,320 realistic action videos, collected from YouTube, and divided into 101 action categories. We use the three splits provided with this dataset in order to evaluate our results and report the mean average accuracy over these splits.\nThe HMDB51 dataset [20] consists of 6766 action videos, collected from various sources, and divided into 51 action categories. Three splits are provided as an official benchmark and are used here. The mean average accuracy over these splits is reported.\nTable1 compares our RNN-FV pooling method to Mean and GMM-FV pooling. Three sets of features, as described in Sec.5.1 are used: VGG coupled with PCA, VGG projected by the image to sentence matching CCA, and C3D.\nThe parameters were set on the validation split that we created for the provided training set. For GMM-FV, the only parameter isk, which is the number of components in the mixture. The validated values ofk were in the set {1, 2, 4, 8, 16, 32}. The parameter for RNN-FV was the stopping point of the RNN training, as described in Sec.5.3. Classification is conducted in all experiments using a multiclass (one-vs-all) linear SVM with C=1.\nAs can be seen in table1, the RNN-FV pooling outperformed the other pooling methods by a sizable margin. Another interesting observation is that with VGG frame representation, CCA outperformed PCA consistently in all pooling methods. Not shown is the performance obtained when using the activations of the RNN as a feature vector. These results are considerably worse than all pooling methods. Notice that the representation dimension of Mean pooling is 500 (like the features we used), the GMM-FV dimension is 2 × k × 500, where k is the number of clusters and the RNN-FV dimension is 1000.\nTable2 compares our proposed RNN-FV method, combining multiple features together, with recently published methods on both datasets. The combinations were performed using early fusion, i.e, we concatenated the normalized low-dimensional gradients of the models and train multi-class linear SVM on the combined representation. We also tested the combination of our two best models with idt [52] and got state of the art results on both benchmarks. Interestingly, when training the RNNs on UCF101 and applying to encode HMDB51 videos, a comparable results of 66.99 (54.47 without idt) is obtained, which is also above current state of the art.\nDataset HMDB51 UCF101 Method MP GMM-FV RNN-FV MP GMMFV RNN-FV\nVGG PCA 42.16 36.8 45.62 75.51 76.53 79.29 VGG CCA 43.05 39.61 46.14 77.49 76.84 79.49 C3D 51.2 45.82 52.88 81.05 80.04 82.33\nTable 1. Comparing pooling techniques (mean pooling, GMM-FV and RNN-FV) on HMDB51 and UCF101. Three types of features are used: VGG-PCA, VGGCCA, and C3D. The table reports recognition average accuracy (higher is better).\nMethod HMDB51 UCF101\nidt [52] 57.2 85.9 idt + high-D encodings [32] 61.1 87.9 Two-stream CNN (2 nets) [40] 59.4 88 Multi-skip Feature Stacking [21] 65.4 89.1 C3D (1 net) [46] – 82.3 C3D (3 nets) [46] – 85.2 C3D (3 nets) + idt [46] – 90.4 TDD (2 nets) [53] 63.2 90.3 TDD (2 nets) + idt [53] 65.9 91.5 stacked FV [33] 56.21 – stacked FV + idt [33] 66.78 – RNN-FV(C3D + VGG-CCA) 54.33 88.01 RNN-FV(C3D + VGG-CCA) + idt 67.71 94.08\nTable 2. comparison to the state of the art on UCF101 and HMDB51. In order to obtain the best performance, we combine, similar to all other contributions, multiple features. We also present a result where idt [52] is combined, similar to all other top results (Multi-skip extends idt). This adds motion based information to our method."
    }, {
      "heading" : "7.2. Image-sentence retrieval",
      "text" : "The effectiveness of RNN-FV as sentence representation is evaluated on the bidirectional image and sentence retrieval task. We perform our experiments on three benchmarks: Flickr8K [8], Flickr30K [9], and COCO [25]. The datasets contain8, 000, 30, 000, and123, 000 images respectively. Each image is accompanied with 5 sentences describing the image content, collected via crowdsourcing.\nThe Flickr8k dataset is provided with training, validation, and test splits. For Flickr30K and COCO, no training splits are given, and we use the same splits used by [18].\nThere are three tasks in this benchmark: image annotation, in which the goal is to retrieve, given a query image, the five ground truth sentences; image search, in which, given a query sentence, the goal is to retrieve the ground truth image; and sentence similarity, in which the goal is, given a sentence, to retrieve the other four sentences describing the same image. Evaluation is performed using Recall@K, namely the fraction of times the correct result was ranked within the top K items. The median and mean rank of the first ground truth result are also reported. For the sentence similarity task, only mean rank is reported.\nAs mentioned in Sec.6, we explored RNN-FV based\non several RNNs. The first RNN is a generic one: it was trained with the Wikipedia sentences as training data and word2vec as word embedding. In addition, for each of the three datasets, we trained three RNNs with the dataset’s training sentences as training data: one with word2vec as word embedding; one with the \"CCA word embedding\" derived from the semantic vector space of [18], as explained in Sec.6; and one with the CCA word embedding, and with feeding the sentences in reverse order. These RNNs were all trained for regression. For Flickr8K, we also trained an RNN for classification (with Flickr8K training sentences, and word2vec embedding). In this network, the softmax layer was of size 8,148, corresponding to the number of unique words in the Flickr8k dataset. Since the resulting number of weights of the output layer is around 4 million, we reduced the dimension of the gradient feature vector by random sampling of 72,000 coordinates. Training a classification model on the larger datasets is virtually impractical, since the number of unique words in these datasets is much higher, resulting in a very large softmax layer and a huge number of weights.\nIn the regression RNNs, we used an LSTM layer of size 100. We did not observe a benefit in using more LSTM units. We used the part of the gradient corresponding to all 30,300 weights of the output layer (including one bias per word2Vec dimension). In the case of the larger COCO dataset, due to the computational burden of the CCA calculation, we used PCA to reduce the gradient dimension from 30,300 to 20,000. PCA was calculated on a random subset of 300,000 sentences (around 50%) of the training set. We also tried PCA dimension reduction to a lower dimension of 4,096, for all three datasets. We observed no change in performance (Flickr8K) or slightly worse results (Flickr30K and COCO).\nThe number of RNN training epochs was 400, 100, 20, and 15, for the Flickr8k, Flickr30k, COCO and Wikipedia datasets respectively.\nTables3, 4 and5 show the results of the different RNNFV variants compared to the current state of the art methods. We also report results of combinations of models. Combining was done by averaging the image-sentence (or sentencesentence) cosine similarities obtained by each model.\nFirst, we see that regression-based RNN-FV should be preferred over the classification-based one. In addition to its lower dimension and natural handling of unseen words, the results obtained by regression RNN-FV are better. Sec-\nond, we notice the competitive performance of the model trained on Wikipedia sentences, which demonstrates the generalization power of the RNN-FV, being able to perform well on data different than the one which the RNN was trained on. Training using the dataset’s sentences only slightly improves result, and not always. Improved results are obtained when using the CCA word embedding instead of word2vec. It is interesting to see the result of the “reverse” model, which is on a par with the other models. It is somewhat complementary to the “left-to-right” model, as the combination of the two yields somewhat improved results. Finally, the combination of RNN-FV with the best model (GMM+HGLMM) of [18] outperforms the current state of the art on Flickr8k, and is competitive on the other datasets."
    }, {
      "heading" : "8. Conclusions",
      "text" : "This paper introduces a novel FV representation for sequences that is derived from RNNs. The proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive “bag” model typically used for conventional FVs.\nThe RNN-FV representation surpasses the state-of-theart results for video action recognition on two challenging datasets. When used for representing sentences, the RNNFV representation achieves state-of-the-art or competitiv results on image annotation and image search tasks. Since the length of the sentences in these tasks is usually short and, therefore, the ordering is less crucial, we believe that using the RNN-FV representation for tasks that use longer text will provide an even larger gap between the conventional FV and the RNN-FV.\nA transfer learning result from the image annotation task to the video action recognition task was shown. The con-\nceptual distance between these two tasks makes this result both interesting and surprising. It supports a human development-like way of training, in which visual labeling is learned through natural language, as opposed to, e.g., associating bounding boxes with nouns. While such training was used in computer vision to learn related image to text tasks, and while recently zero-shot action recognition was shown [11, 55], NLP to video action recognition transfer was never shown to be as general as presented here."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473 ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The devil is in the details: an evaluation of recent feature encoding methods",
      "author" : [ "K. Chatfield", "V. Lempitsky", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "InBritish Machine Vision Conference ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "and A",
      "author" : [ "K. Chatfield", "K. Simonyan", "A. Vedaldi" ],
      "venue" : "Zisserman. Return of the devil in the details: Delving deep into convolutional nets.arXiv preprint arXiv:1405.3531 ",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning a recurrent visual rep resentation for image caption generation",
      "author" : [ "X. Chen", "C.L. Zitnick" ],
      "venue" : "arXiv preprint arXiv:1411.5654",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and Y",
      "author" : [ "J. Chung", "C. Gulcehre", "K. Cho" ],
      "venue" : "Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling.arXiv preprint arXiv:1412.3555 ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and T",
      "author" : [ "J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko" ],
      "venue" : "Darrell. Long-term recurrent convolutional networks for visual recognition and description.arXiv preprint arXiv:1411.4389 ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Long short-term memor y",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation , 9(8):1735–1780",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Framing image description as a ranking task: Data",
      "author" : [ "M. Hodosh", "P. Young", "J. Hockenmaier" ],
      "venue" : "models and evaluation metrics.J. Artif. Intell. Res.(JAIR) , 47:853–899",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Relations between two sets of variates",
      "author" : [ "H. Hotelling" ],
      "venue" : "Biometrika, pages 321–377",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1936
    }, {
      "title" : "Objects2action: Classifying and localizing actions witho ut any video example",
      "author" : [ "M. Jain", "J.C. van Gemert", "T. Mensink", "C.G.M. Snoek" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "3d convolutional neural networks for human action recognition",
      "author" : [ "S. Ji", "W. Xu", "M. Yang", "K. Yu" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on , 35(1):221– 231",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep visual-semantic align ments for generating image descriptions",
      "author" : [ "A. Karpathy", "L. Fei-Fei" ],
      "venue" : "Technical report , Computer Science Department, Stanford University",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep fragment em beddings for bidirectional image sentence mapping",
      "author" : [ "A. Karpathy", "A. Joulin", "L. Fei-Fei" ],
      "venue" : "arXiv preprint arXiv:1406.5679 ",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "R",
      "author" : [ "A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung" ],
      "venue" : "Suktha nk r, and L. Fei-Fei. Large-scale video classification with convolutional neural networks. InComputer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on , pages 1725–1732. IEEE",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and R",
      "author" : [ "R. Kiros", "R. Salakhutdinov" ],
      "venue" : "S. Zemel. Unifying visual-semantic embeddings with multimodal neural language models.arXiv preprint arXiv:1411.2539 ",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "A. Torralba", "R. Urtasun", "S. Fidler" ],
      "venue" : "a Xiv preprint arXiv:1506.06726 ",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Associating neural word embeddings with deep image representations using fisher vectors",
      "author" : [ "B. Klein", "G. Lev", "G. Sadeh", "L. Wolf" ],
      "venue" : "InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4437– 4446",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "M otion interchange patterns for action recognition in unconstrained videos",
      "author" : [ "O. Kliper-Gross", "Y. Gurovich", "T. Hassner", "L. Wolf" ],
      "venue" : "InComputer Vision–ECCV 2012 , pages 256–269. Springer",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "and T",
      "author" : [ "H. Kuehne", "H. Jhuang", "E. Garrote", "T. Poggio" ],
      "venue" : "Serre . HMDB: a large video database for human motion recognition. In Proc. IEEE Int. Conf. Comput. Vision ",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "and B",
      "author" : [ "Z. Lan", "M. Lin", "X. Li", "A.G. Hauptmann" ],
      "venue" : "Raj. Beyond gaussian pyramid: Multi-skip feature stacking for action recognition.arXiv preprint arXiv:1411.6660 ",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On space-time interest points",
      "author" : [ "I. Laptev" ],
      "venue" : "Int. J. Comput. Vision, 64(2):107–123",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Learning realistic human actions from movies",
      "author" : [ "I. Laptev", "M. Marszalek", "C. Schmid", "B. Rozenfeld" ],
      "venue" : "Proc. IEEE Conf. Comput. Vision Pattern Recognition , pages 1–8",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "In defense of word embedding for generic text representation",
      "author" : [ "G. Lev", "B. Klein", "L. Wolf" ],
      "venue" : "Natural Language Processing and Information Systems , pages 35–50. Springer International Publishing",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "D",
      "author" : [ "T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona" ],
      "venue" : "R amanan, P. Dollár, and C. Zitnick. Microsoft coco: Common objects in context. In D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, editors, Computer Vision – ECCV 2014 , volume 8693 ofLecture Notes in Computer Science , pages 740–755. Springer International Publishing",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multimodal convolution al neural networks for matching image and sentence",
      "author" : [ "L. Ma", "Z. Lu", "L. Shang", "H. Li" ],
      "venue" : "arXiv preprint arXiv:1504.06063 ",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep captioning with multimodal recurrent neural networks (m-rnn)",
      "author" : [ "J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. Yuille" ],
      "venue" : "arXiv preprint arXiv:1412.6632 ",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Explain images with multimodal recurrent neural networks",
      "author" : [ "J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille" ],
      "venue" : "arXiv preprint arXiv:1410.1090 ",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep correlation for matching",
      "author" : [ "F.Y.K. Mikolajczyk" ],
      "venue" : "ima ges and text. 2015.2,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean" ],
      "venue" : "InAdvances in Neural Information Processing Systems , pages 3111–3119",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep sentence embedding using the long short term memory network: Analysis and application to information retrieval",
      "author" : [ "H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "R. Ward" ],
      "venue" : "arXiv preprint arXiv:1502.06922 ",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice",
      "author" : [ "X. Peng", "L. Wang", "X. Wang", "Y. Qiao" ],
      "venue" : "arXiv preprint arXiv:1405.4506",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Action recognition with stacked fisher vectors",
      "author" : [ "X. Peng", "C. Zou", "Y. Qiao", "Q. Peng" ],
      "venue" : "Computer Vision–ECCV 2014 , pages 581–595. Springer",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fisher kernels on visual voca bularies for image categorization",
      "author" : [ "F. Perronnin", "C. Dance" ],
      "venue" : "Computer Vision and Pattern Recognition, 2007. CVPR’07. IEEE Conference on , pages 1–8. IEEE",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Largescale image retrieval with compressed fisher vectors",
      "author" : [ "F. Perronnin", "Y. Liu", "J. Sánchez", "H. Poirier" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 3384–3391. IEEE",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Improving the fisher kernel for large-scale image classification",
      "author" : [ "F. Perronnin", "J. Sánchez", "T. Mensink" ],
      "venue" : "Computer Vision–ECCV 2010 , pages 143–156. Springer",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "and S",
      "author" : [ "B. Plummer", "L. Wang", "C. Cervantes", "J. Caicedo", "J. Hockenmaier" ],
      "venue" : "Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to sentence models.arXiv preprint arXiv:1505.04870 ",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Corpus portal for search in monolingual corpora",
      "author" : [ "U. Quasthoff", "M. Richter", "C. Biemann" ],
      "venue" : "Proceedings of the fifth international conference on language resources and evalua tion, volume 17991802",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Fisher vector faces in the wild",
      "author" : [ "K. Simonyan", "O.M. Parkhi", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "Proc. BMVC, volume 1, page 7",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Two-stream convolutiona l networks for action recognition in videos",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "Advances in Neural Information Processing Systems , pages 568–576",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "CoRR, abs/1409.1556",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Grounded compositional semantics for finding and describing images with sentences",
      "author" : [ "R. Socher", "Q. Le", "C. Manning", "A. Ng" ],
      "venue" : "InNIPS Deep Learning Workshop ",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "UCF101: A dataset of 101 human action classes from videos in the wild",
      "author" : [ "K. Soomro", "A.R. Zamir", "M. Shah" ],
      "venue" : "CRCV- TR-12-01,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2012
    }, {
      "title" : "Sequence to seque nce learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "Advances in neural information processing systems , pages 3104–3112",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Convolutional learning of spatio-temporal features",
      "author" : [ "G.W. Taylor", "R. Fergus", "Y. LeCun", "C. Bregler" ],
      "venue" : "Computer Vision–ECCV 2010 , pages 140–153. Springer",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "and M",
      "author" : [ "D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani" ],
      "venue" : "Palu ri. Learning spatiotemporal features with 3d convolutional ne tworks. arXiv preprint arXiv:1412.0767 ",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Canonical ridge and econometrics of joint pro duction",
      "author" : [ "H. Vinod" ],
      "venue" : "Journal of Econometrics , 4(2):147 – 166",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan" ],
      "venue" : "arXiv preprint arXiv:1411.4555",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Action recognition by dense trajectories",
      "author" : [ "H. Wang", "A. Klaser", "C. Schmid", "C. Liu" ],
      "venue" : "Proc. IEEE Conf. Comput. Vision Pattern Recognition , pages 3169–3176",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Dense trajectories and motion boundary descriptors for action recog nition",
      "author" : [ "H. Wang", "A. Kläser", "C. Schmid", "C.-L. Liu" ],
      "venue" : "Int. J. Comput. Vision , 103(1):60–79",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Action Recognition with Improved Trajectories",
      "author" : [ "H. Wang", "C. Schmid" ],
      "venue" : "InInternational Conference on Computer Vision,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2013
    }, {
      "title" : "Action recognition with improved trajectories",
      "author" : [ "H. Wang", "C. Schmid" ],
      "venue" : "InComputer Vision (ICCV), 2013 IEEE International Conference on , pages 3551–3558. IEEE",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Action recognition with trajectory-pooled deep-convolutional descriptors",
      "author" : [ "L. Wang", "Y. Qiao", "X. Tang" ],
      "venue" : "arXiv preprint arXiv:1505.04868 ",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A comparative study of encoding",
      "author" : [ "X. Wang", "L. Wang", "Y. Qiao" ],
      "venue" : "pooling and normalization methods for action recog nition. In Computer Vision–ACCV 2012 , pages 572–585. Springer",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Semantic embedding space for zero-shot action recognition",
      "author" : [ "X. Xu", "T.M. Hospedales", "S. Gong" ],
      "venue" : "CoRR, abs/1502.01540",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 44,
      "context" : "Similarly, C3D [46] features of sequential video sub-volumes are used with the same training technique.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 41,
      "context" : "The proposed RNN-FV method achieves state-of-theart results in action recognition on the HMDB51 [ 20] and UCF101 [43] datasets.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 49,
      "context" : "[51, 50] used low-level hand-crafted features such as histogram of oriented gradients (HOG), histogram of optical flow (HOF) and motion boundary histogram (MBH).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 48,
      "context" : "[51, 50] used low-level hand-crafted features such as histogram of oriented gradients (HOG), histogram of optical flow (HOF) and motion boundary histogram (MBH).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 52,
      "context" : "[54] compared different pooling techniques for the application of action recognition and showed empirically tha t the Fisher Vector encoding has the best performance.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[ 18] used word2vec [30] as the word embedding and then applied Fisher Vector based on a Hybrid Gaussian-Laplacian Mixture Model (HGLMM) in order to pool the word2vec embeddings of the words in a given sentence into a single representation.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "To address the need for capturing long term semantics in the sentence, these works mainly use Long Short-Term Memory (LSTM) [7] or Gated Recurrent Unit (GRU) [ 5] cells.",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 24,
      "context" : "[26] used a contrastive loss function trained on matching and unmatching pairs of (image,sentence) in order to learn a score function for a given pair.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "VGG Using the pre-trained VGG convolutional network [41], we extract a 4096-dimensional representation of each video frame.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 45,
      "context" : "The regularized CCA algorithm [47], where the regularization parameter is selected based on the validation set, is used to match the the VGG representation with the sentence RNN-FV representation.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 50,
      "context" : "We also tested the combination of our two best models with idt [52] and got state of the art results on both benchmarks.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "We perform our experiments on three benchmarks: Flickr8K [8], Flickr30K [9], and COCO [ 25].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : "Finally, the combination of RNN-FV with the best model (GMM+HGLMM) of [18] outperforms the current state of the art on Flickr8k, and is competitive on the other datasets.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 35,
      "context" : "0 NA NA RTP [37](manual annotations) 37.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 9,
      "context" : "While such training was used in computer vision to learn related image to text tasks, and while recently zero-shot action recognition was shown [11, 55], NLP to video action recognition transfer was never shown to be as general as presented here.",
      "startOffset" : 144,
      "endOffset" : 152
    }, {
      "referenceID" : 53,
      "context" : "While such training was used in computer vision to learn related image to text tasks, and while recently zero-shot action recognition was shown [11, 55], NLP to video action recognition transfer was never shown to be as general as presented here.",
      "startOffset" : 144,
      "endOffset" : 152
    } ],
    "year" : 2015,
    "abstractText" : "Recurrent Neural Networks (RNNs) have had considerable success in classifying and predicting sequences. We demonstrate that RNNs can be effectively used in order to encode sequences and provide effective representations . The methodology we use is based on Fisher Vectors, where the RNNs are the generative probabilistic models and the partial derivatives are computed using backpropagation. State of the art results are obtained in two central but dista nt tasks, which both rely on sequences: video action recognition and image annotation. We also show a surprising transfer learning result from the task of image annotation to the task of video action recognition.",
    "creator" : "LaTeX with hyperref package"
  }
}