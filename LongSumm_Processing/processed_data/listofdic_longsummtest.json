[
    {
        "title": "TALKSUMM: A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks",
        "sections": [
            {
                "heading": null,
                "text": "ar X\niv :1\n90 6.\n01 35\n1v 2\n[ cs\n.C L\n] 1\n3 Ju\nn 20\n19\nable for the task of scientific paper summarization. In this paper, we propose a novel method that automatically generates summaries for scientific papers, by utilizing videos of talks at scientific conferences. We hypothesize that such talks constitute a coherent and concise description of the papers\u2019 content, and can form the basis for good summaries. We collected 1716 papers and their corresponding videos, and created a dataset of paper summaries. A model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually. In addition, we validated the quality of our summaries by human experts."
            },
            {
                "heading": "1 Introduction",
                "text": "The rate of publications of scientific papers is\nincreasing and it is almost impossible for re-\nsearchers to keep up with relevant research. Au-\ntomatic text summarization could help mitigate\nthis problem. In general, there are two com-\nmon approaches to summarizing scientific papers:\ncitations-based, based on a set of citation sen-\ntences (Nakov et al., 2004; Abu-Jbara and Radev,\n2011; Yasunaga et al., 2019), and content-based,\nbased on the paper itself (Collins et al., 2017;\nNikola Nikolov and Hahnloser, 2018). Automatic\nsummarization is studied exhaustively for the\nnews domain (Cheng and Lapata, 2016; See et al.,\n2017), while summarization of scientific papers is\nless studied, mainly due to the lack of large-scale\ntraining data. The papers\u2019 length and complexity\nrequire substantial summarization effort from ex-\nperts. Several methods were suggested to reduce\nthese efforts (Yasunaga et al., 2019; Collins et al.,\n2017), still they are not scalable as they require\nhuman annotations. \u2217 The authors contributed equally.\nRecently, academic conferences started publishing videos of talks (e.g., ACL1, EMNLP1, ICML2,\nand more). In such talks, the presenter (usually\na co-author) must describe their paper coherently\nand concisely (since there is a time limit), provid-\ning a good basis for generating summaries. Based\non this idea, in this paper, we propose a new\nmethod, named TALKSUMM (acronym for Talk-\nbased Summarization), to automatically generate\nextractive content-based summaries for scientific\npapers based on video talks. Our approach uti-\nlizes the transcripts of video content of conference\ntalks, and treat them as spoken summaries of pa-\npers. Then, using unsupervised alignment algo-\nrithms, we map the transcripts to the correspond-\ning papers\u2019 text, and create extractive summaries.\nTable 1 gives an example of an alignment between\n1vimeo.com/aclweb 2 icml.cc/Conferences/2017/Videos\na paper and its talk transcript (see Table 3 in the\nappendix for a complete example).\nSummaries generated with our approach can\nthen be used to train more complex and data-\ndemanding summarization models. Although our\nsummaries may be noisy (as they are created auto-\nmatically from transcripts), our dataset can easily\ngrow in size as more conference videos are aggre-\ngated. Moreover, our approach can generate sum-\nmaries of various lengths.\nOur main contributions are as follows: (1) we\npropose a new approach to automatically gener-\nate summaries for scientific papers based on video\ntalks; (2) we create a new dataset, that contains\n1716 summaries for papers from several computer science conferences, that can be used as training\ndata; (3) we show both automatic and human eval-\nuations for our approach. We make our dataset and related code publicly available3. To our knowl-\nedge, this is the first approach to automatically cre-\nate extractive summaries for scientific papers by\nutilizing the videos of conference talks."
            },
            {
                "heading": "2 Related Work",
                "text": "Several works focused on generating train-\ning data for scientific paper summariza-\ntion (Yasunaga et al., 2019; Jaidka et al., 2018;\nCollins et al., 2017; Cohan and Goharian, 2018).\nMost prominently, the CL-SciSumm shared\ntasks (Jaidka et al., 2016, 2018) provide a total of\n40 human generated summaries; there, a citations-\nbased approach is used, where experts first read\ncitation sentences (citances) that reference the\npaper being summarized, and then read the whole\npaper. Then, they create a summary of 150 words\non average.\nRecently, to mitigate annotation cost,\nYasunaga et al. (2019) proposed a method, in\nwhich human annotators only read the abstract in\naddition to citances (not reading the full paper).\nUsing this approach, they generated 1000 summaries, costing 600+ person-hours. Conversely,\nwe generate summaries, given transcripts of\nconference talks, in a fully automatic manner,\nand, thus, our approach is much more scalable.\nCollins et al. (2017) also aimed at generating\nlabeled data for scientific paper summarization,\nbased on \u201chighlight statements\u201d that authors can\nprovide in some publication venues.\nUsing external data to create summaries was\n3 https://github.com/levguy/talksumm\nalso proposed in the news domain. Wei and Gao\n(2014, 2015) utilized tweets to decide which sen-\ntences to extract from news article.\nFinally, alignment between different modali-\nties (e.g., presentation, videos) and text was stud-\nied in different domains. Both Kan (2007) and\nBahrani and Kan (2013) studied the problem of\ndocument to presentation alignment for schol-\narly documents. Kan (2007) focused on the the\ndiscovery and crawling of document-presentation\npairs, and a model to align between documents to\ncorresponding presentations. In Bahrani and Kan\n(2013) they extended previous model to include\nalso visual components of the slides. Align-\ning video and text was studied mainly in the\nsetting of enriching videos with textual infor-\nmation (Bojanowski et al., 2015; Malmaud et al.,\n2015; Zhu et al., 2015). Malmaud et al. (2015)\nused HMM to align ASR transcripts of cook-\ning videos and recipes text for enriching videos\nwith instructions. Zhu et al. (2015) utilized books\nto enrich videos with descriptive explanations.\nBojanowski et al. (2015) proposed to align video\nand text by providing a time stamp for every sen-\ntence. The main difference between these works\nand ours is in the alignment being used to gener-\nate textual training data in our case, rather than to\nenrich videos."
            },
            {
                "heading": "3 The TALKSUMM Dataset",
                "text": ""
            },
            {
                "heading": "3.1 Data Collection",
                "text": "Recently, many computer science academic asso-\nciations including ACL, ACM, IMLS and more,\nhave started recording talks in different confer-\nences, e.g., ACL, NAACL, EMNLP, and other co-\nlocated workshops. A similar trend occurs in other domains such as Physics4, Biology5, etc.\nIn a conference, each speaker (usually a co-\nauthor) presents their paper given a timeframe of\n15-20 minutes. Thus, the talk must be coherent\nand concentrate on the most important aspects of a\npaper. Hence, the talk can be considered as a sum-\nmary of the paper, as viewed by its authors, and\nis much more comprehensive than the abstract,\nwhich is written by the authors as well.\nIn this work, we focused on NLP and ML\nconferences, and analyzed 1716 video talks from ACL, NAACL, EMNLP, SIGDIAL (2015-2018),\nand ICML (2017-2018). We downloaded the\n4www.cleoconference.org 5 igem.org/Videos/Lecture_Videos\nvideos and extracted the speech data. Then, via a publicly available ASR service6, we extracted\ntranscripts of the speech, and based on the video\nmetadata (e.g., title), we retrieved the correspond-\ning paper (in PDF format). We used ScienceParse7 to extract the text of the paper, and applied a\nsimple processing in order to filter-out some noise\n(e.g. lines starting with the word \u201cCopyright\u201d). At\nthe end of this process, the text of each paper is\nassociated with the transcript of the corresponding\ntalk."
            },
            {
                "heading": "3.2 Dataset Generation",
                "text": "The transcript itself cannot serve as a good sum-\nmary for the corresponding paper, as it constitutes\nonly one modality of the talk (which also consists\nof slides, for example), and hence cannot stand by\nitself and form a coherent written text. Thus, to\ncreate an extractive paper summary based on the\ntranscript, we model the alignment between spo-\nken words and sentences in the paper, assuming\nthe following generative process: During the talk,\nthe speaker generates words for describing ver-\nbally sentences from the paper, one word at each\ntime step. Thus, at each time step, the speaker\nhas a single sentence from the paper in mind, and\nproduces a word that constitutes a part of its ver-\nbal description. Then, at the next time-step, the\nspeaker either stays with the same sentence, or\nmoves on to describing another sentence, and so\non. Thus, given the transcript, we aim to retrieve\nthose \u201csource\u201d sentences and use them as the sum-\nmary. The number of words uttered to describe\neach sentence can serve as importance score, in-\ndicating the amount of time the speaker spent de-\nscribing the sentence. This enables to control the\nsummary length by considering only the most im-\nportant sentences up to some threshold.\nWe use an HMM to model the assumed genera-\ntive process. The sequence of spoken words is the\noutput sequence. Each hidden state of the HMM\ncorresponds to a single paper sentence. We heuris-\ntically define the HMM\u2019s probabilities as follows.\nDenote by Y (1 : T ) the spoken words, and by S(t) \u2208 {1, ...,K} the paper sentence index at time-step t \u2208 {1, ..., T}. Similarly to Malmaud et al. (2015), we define the emission\n6 www.ibm.com/watson/services/speech-to-text/ 7 github.com/allenai/science-parse\nprobabilities to be:\np(Y (t) = y|S(t) = k) \u221d max w\u2208words(k) sim(y,w)\nwhere words(k) is the set of words in the k\u2019th sentence, and sim is a semantic-\nsimilarity measure between words, based on\nword-vector distance. We use pre-trained GloVe\n(Pennington et al., 2014) as the semantic vector\nrepresentations for words.\nAs for the transition probabilities, we must\nmodel the speaker\u2019s behavior and the transitions\nbetween any two sentences in the paper. This\nis unlike the simpler setting in Malmaud et al.\n(2015), where transition is allowed between con-\nsecutive sentences only. To do so, denote the en-\ntries of the transition matrix by T (k, l) = p(S(t+ 1) = l|S(t) = k). We rely on the following assumptions: (1) T (k, k) (the probability of staying in the same sentence at the next time-step) is rel-\natively high. (2) There is an inverse relation be-\ntween T (k, l) and |l \u2212 k|, i.e., it is more probable to move to a nearby sentence than jumping to a\nfarther sentence. (3) S(t + 1) > S(t) is more probable than the opposite (i.e., transition to a later\nsentence is more probable than to an earlier one).\nAlthough these assumptions do not perfectly re-\nflect reality, they are a reasonable approximation\nin practice.\nFollowing these assumptions, we define the\nHMM\u2019s transition probability matrix. First, de-\nfine the stay-probability as \u03b1 = max(\u03b4(1 \u2212 K T ), \u01eb), where \u03b4, \u01eb \u2208 (0, 1). This choice of stayprobability is inspired by Malmaud et al. (2015),\nusing \u03b4 to fit it to our case where transitions be-\ntween any two sentences are allowed, and \u01eb to\nhandle rare cases where K is close to, or even\nlarger than T . Then, for each sentence index\nk \u2208 {1, ...,K}, we define:\nT (k, k) = \u03b1\nT (k, k + j) = \u03b2k \u00b7 \u03bb j\u22121, j \u2265 1\nT (k, k \u2212 j) = \u03b3 \u00b7 \u03b2k \u00b7 \u03bb j\u22121, j \u2265 1\nwhere \u03bb, \u03b3, \u03b2k \u2208 (0, 1), \u03bb and \u03b3 are factors reflecting assumptions (2) and (3) respectively, and for all k, \u03b2k is normalized s.t. \u2211K\nl=1 T (k, l) = 1. The values of \u03bb, \u03b3, \u03b4 and \u01eb were fixed through-\nout our experiments at \u03bb = 0.75, \u03b3 = 0.5, \u03b4 = 0.33 and \u01eb = 0.1. The average value of \u03b1, across all papers, was around 0.3. The values of\nthese parameters were determined based on eval-\nuation over manually-labeled alignments between\nthe transcripts and the sentences of a small set of\npapers.\nFinally, we define the start-probabilities assum-\ning that the first spoken word must be conditioned\non a sentence from the Introduction section, hence\np(S(1)) is defined as a uniform distribution over the Introduction section\u2019s sentences.\nNote that sentences which appear in the Ab-\nstract, Related Work, and Acknowledgments sec-\ntions of each paper are excluded from the HMM\u2019s\nhidden states, as we observed that presenters sel-\ndom refer to them.\nTo estimate the MAP sequence of sentences, we\napply the Viterbi algorithm. The sentences in the\nobtained sequence are the candidates for the pa-\nper\u2019s summary. For each sentence s appearing in this sequence, denote by count(s) the number of time-steps in which this sentence appears. Thus,\ncount(s) models the number of words generated by the speaker conditioned on s, and, hence, can\nbe used as an importance score. Given a desired\nsummary length, one can draw a subset of top-\nranked sentences up to this length."
            },
            {
                "heading": "4 Experiments",
                "text": ""
            },
            {
                "heading": "4.1 Experimental Setup",
                "text": "Data For Evaluation We evaluate the quality\nof our dataset generation method by training an\nextractive summarization model, and evaluating\nthis model on a human-generated dataset of sci-\nentific paper summaries. For this, we choose\nthe CL-SciSumm shared task (Jaidka et al., 2016,\n2018), as this is the most established bench-\nmark for scientific paper summarization. In this\ndataset, experts wrote summaries of 150 words\nlength on average, after reading the whole paper.\nThe evaluation is on the same test data used by\nYasunaga et al. (2019), namely 10 examples from\nCL-SciSumm 2016, and 20 examples from CL-\nSciSumm 2018 as validation data.\nTraining Data Using the HMM importance\nscores, we create four training sets, two with\nfixed-length summaries (150 and 250 words), and\ntwo with fixed ratio between summary and paper\nlengths (0.3 and 0.4). We train models on each\ntraining set, and select the model yielding the best\nperformance on the validation set (evaluation is\nalways done with generating a 150-words sum-\nmary).\nSummarization Model We train an extractive\nsummarization model on our TALKSUMM dataset,\nusing the extractive variant of Chen and Bansal\n(2018). We test two summary generation ap-\nproaches, similarly to Yasunaga et al. (2019).\nFirst, for TALKSUMM-ONLY, we generate a 150-\nwords summary out of the top-ranked sentences\nextracted by our trained model (sentences from the\nAcknowledgments section are omitted, in case the\nmodel extracts any). In the second approach, a\n150-words summary is created by augmenting the\nabstract with non-redundant sentences extracted\nby our model, similarly to the \u201cHybrid 2\u201d ap-\nproach of Yasunaga et al. (2019). We perform\nearly-stopping and hyper-parameters tuning using\nthe validation set.\nBaselines We compare our results to SCISUMM-\nNET (Yasunaga et al., 2019) trained on 1000 sci-\nentific papers summarized by human annotators.\nAs we use the same test set as in Yasunaga et al.\n(2019), we directly compare their reported model\nperformance to ours, including their ABSTRACT\nbaseline which takes the abstract to be the paper\u2019s\nsummary."
            },
            {
                "heading": "4.2 Results",
                "text": "Automatic Evaluation Table 2 summarizes the\nresults: both GCN CITED TEXT SPANS and\nTALKSUMM-ONLY models, are not able to obtain better performance than ABSTRACT8 . However,\nfor the Hybrid approach, where the abstract is aug-\nmented with sentences from the summaries emit-\nted by the models, our TALKSUMM-HYBRID out-\nperforms both GCN HYBRID 2 and ABSTRACT.\nImportantly, our model, trained on automatically-\ngenerated summaries, performs on par with mod-\nels trained over SCISUMMNET, in which training\ndata was created manually.\n8While the abstract was input to GCN CITED TEXT SPANS, it was excluded from TALKSUMM-ONLY.\nHuman Evaluation We conduct a human eval-\nuation of our approach with support from authors\nwho presented their papers in conferences. As our\ngoal is to test more comprehensive summaries, we\ngenerated summaries composed of 30 sentences (approximately 15% of a long paper). We randomly selected 15 presenters from our corpus and asked them to perform two tasks, given the gen-\nerated summary of their paper: (1) for each sen-\ntence in the summary, we asked them to indicate\nwhether they considered it when preparing the talk\n(yes/no question); (2) we asked them to globally\nevaluate the quality of the summary (1-5 scale,\nranging from very bad to excellent, 3 means good). For the sentence-level task (1), 73% of the sentences were considered while preparing the talk.\nAs for the global task (2), the quality of the sum-\nmaries was 3.73 on average, with standard deviation of 0.725. These results validate the quality of our generation method."
            },
            {
                "heading": "5 Conclusion",
                "text": "We propose a novel automatic method to gener-\nate training data for scientific papers summariza-\ntion, based on conference talks given by authors.\nWe show that the a model trained on our dataset\nachieves competitive results compared to models\ntrained on human generated summaries, and that\nthe dataset quality satisfies human experts. In the\nfuture, we plan to study the effect of other video\nmodalities on the alignment algorithm. We hope\nour method and dataset will unlock new opportu-\nnities for scientific paper summarization."
            },
            {
                "heading": "A A Detailed Example",
                "text": "This section elaborates on the example presented\nin Table 1. Table 3 extends Table 1 by showing\nthe manually-labeled alignment between the com-\nplete text of the paper\u2019s Introduction section, and\nthe corresponding transcript. Table 4 shows the\nalignment obtained using the HMM. Each row in\nthis table corresponds to an interval of consecutive\ntime-steps (i.e., a sub-sequence of the transcript)\nin which the same paper sentence was selected\nby the Viterbi algorithm. The first column (Pa-\nper Sentence) shows the selected sentences; The\nsecond column (ASR transcript) shows the tran-\nscript obtained by the ASR system; The third col-\numn (Human transcript) shows the manually cor-\nrected transcript, which is provided for readability\nTitle: Split and Rephrase: Better Evaluation and Stronger Baselines (Aharoni and Goldberg, 2018) Paper: Processing long, complex sentences is challenging. This is true either for humans in various circumstances or in NLP tasks like parsing and machine translation . An automatic system capable of breaking a complex sentence into several simple sentences that convey the same meaning is very appealing . A recent work by Narayan et al. (2017) introduced a dataset, evaluation method and baseline systems for the task, naming it Split-and Rephrase . The dataset includes 1,066,115 instances mapping a single complex sentence to a sequence of sentences that express the same meaning, together with RDF triples that describe their semantics. They considered two system setups: a text-to-text setup that does not use the accompanying RDF information, and a semantics-augmented setup that does. They report a BLEU score of 48.9 for their best text-to-text system, and of 78.7 for the best RDF-aware one. We focus on the text-to-text setup, which we find to be more challenging and more natural. We begin with vanilla SEQ2SEQ models with attention (Bahdanau et al., 2015) and reach an accuracy of 77.5 BLEU, substantially outperforming the text-to-text baseline of Narayan et al. (2017) and approaching their best RDF-aware method. However, manual inspection reveal many cases of unwanted behaviors in the resulting outputs: (1) many resulting sentences are unsupported by the input: they contain correct facts about relevant entities, but these facts were not mentioned in the input sentence; (2) some facts are repeated the same fact is mentioned in multiple output sentences; and (3) some facts are missing mentioned in the input but omitted in the output. The model learned to memorize entity-fact pairs instead of learning to split and rephrase. Indeed, feeding the model with examples containing entities alone without any facts about them causes it to output perfectly phrased but unsupported facts (Table 3). Digging further, we find that 99% of the simple sentences (more than 89% of the unique ones) in the validation and test sets also appear in the training set, which coupled with the good memorization capabilities of SEQ2SEQ models and the relatively small number of distinct simple sentences helps to explain the high BLEU score . To aid further research on the task, we propose a more challenging split of the data . We also establish a stronger baseline by extending the SEQ2SEQ approach with a copy mechanism, which was shown to be helpful in similar tasks (Gu et al., 2016; Merity et al., 2017; See et al., 2017). On the original split, our models outperform the best baseline of Narayan et al. (2017) by up to 8.68 BLEU, without using the RDF triples. On the new split, the vanilla SEQ2SEQ models break completely, while the copy-augmented models perform better. In parallel to our work, an updated version of the dataset was released (v1.0), which is larger and features a train/test split protocol which is similar to our proposal. We report results on this dataset as well. The code and data to reproduce our results are available on Github.1 We encourage future work on the split-and-rephrase task to use our new data split or the v1.0 split instead of the original one. Talk Transcript: Let\u2019s begin with the motivation so processing long complex sentences is a hard task this is true for arguments like children people with reading disabilities second language learners but this is also true for sentence level and NLP systems for example previous work show that dependency parsers degrade performance when they\u2019re introduced with longer and longer sentences in a similar result was shown for neural machine translation where neural machine translation systems introduced with longer sentences starting degrading performance the question rising here is can we automatically break a complex sentence into several simple ones while preserving the meaning or the semantics and this can be a useful component in NLP pipelines . For example the split and rephrase task was introduced in the last EMNLP by Narayan Gardent and Shimarina where they introduced a dataset an evaluation method and baseline models for this task. The task definition can be taking a complex sentence and breaking it into several simple ones with the same meaning . For example if you take the sentence Alan being joined NASA in nineteen sixty three where he became a member of the Apollo twelve mission along with Alfa Worden and his back a pilot and they\u2019ve just got its commander who would like to break the sentence into four sentences which can go as Alan bean serves as a crew member of Apolo twelve Alfa Worden was the back pilot will close it was commanded by David Scott now be was selected by NASA in nineteen sixty three we can see that the task requires first identifying independence semantics units in the source sentence and then rephrasing those units into a single sentences on the target site. In this work we first show the simple neural models seem to perform very well on the original benchmark but this is only due to memorization of the training set we propose a more challenging data split for the task to discourage this memorization and we perform automatic evaluation in error analysis on the new benchmark showing that the task is still very far from being solved.\nTable 3: Alignment example between a paper\u2019s Introduction section and first 2:40 minutes of the talk\u2019s transcript. The different colors show corresponding content between the transcript to the written paper. This is the full-text version of the example shown in Table 1.\n(our model predicted the alignment based on the\nraw ASR output); Finally, the forth column shows\nwhether our model has correctly aligned a paper\nsentence with a sub-sequence of the transcript.\nRows with no values in this column correspond\nto transcript sub-sequences which were not asso-\nciated with any paper sentence in the manually-\nlabeled alignment."
            }
        ],
        "references": [
            {
                "title": "Coherent citation-based summarization of scientific papers",
                "author": [
                    "Amjad Abu-Jbara",
                    "Dragomir Radev."
                ],
                "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT \u201911,",
                "citeRegEx": "Abu.Jbara and Radev.,? 2011",
                "shortCiteRegEx": "Abu.Jbara and Radev.",
                "year": 2011
            },
            {
                "title": "Split and rephrase: Better evaluation and stronger baselines",
                "author": [
                    "Roee Aharoni",
                    "Yoav Goldberg."
                ],
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 719\u2013724. Association for",
                "citeRegEx": "Aharoni and Goldberg.,? 2018",
                "shortCiteRegEx": "Aharoni and Goldberg.",
                "year": 2018
            },
            {
                "title": "Multimodal alignment of scholarly documents and their presentations",
                "author": [
                    "Bamdad Bahrani",
                    "Min-Yen Kan."
                ],
                "venue": "Proceedings of the 13th ACM/IEEE-CS Joint Conference on Digital Libraries, JCDL \u201913, pages 281\u2013284.",
                "citeRegEx": "Bahrani and Kan.,? 2013",
                "shortCiteRegEx": "Bahrani and Kan.",
                "year": 2013
            },
            {
                "title": "Weakly-supervised alignment of video with text",
                "author": [
                    "Piotr Bojanowski",
                    "Remi Lajugie",
                    "Edouard Grave",
                    "Francis Bach",
                    "Ivan Laptev",
                    "Jean Ponce",
                    "Cordelia Schmid."
                ],
                "venue": "The IEEE International Conference on Computer Vision (ICCV).",
                "citeRegEx": "Bojanowski et al\\.,? 2015",
                "shortCiteRegEx": "Bojanowski et al\\.",
                "year": 2015
            },
            {
                "title": "Fast abstractive summarization with reinforce-selected sentence rewriting",
                "author": [
                    "Yen-Chun Chen",
                    "Mohit Bansal."
                ],
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675\u2013686. Association for",
                "citeRegEx": "Chen and Bansal.,? 2018",
                "shortCiteRegEx": "Chen and Bansal.",
                "year": 2018
            },
            {
                "title": "Neural summarization by extracting sentences and words",
                "author": [
                    "Jianpeng Cheng",
                    "Mirella Lapata."
                ],
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 484\u2013494.",
                "citeRegEx": "Cheng and Lapata.,? 2016",
                "shortCiteRegEx": "Cheng and Lapata.",
                "year": 2016
            },
            {
                "title": "Scientific document summarization via citation contextualization and scientific discourse",
                "author": [
                    "Arman Cohan",
                    "Nazli Goharian."
                ],
                "venue": "International Journal on Digital Libraries, pages 287\u2013303.",
                "citeRegEx": "Cohan and Goharian.,? 2018",
                "shortCiteRegEx": "Cohan and Goharian.",
                "year": 2018
            },
            {
                "title": "A supervised approach to extractive summarisation of scientific papers",
                "author": [
                    "Ed Collins",
                    "Isabelle Augenstein",
                    "Sebastian Riedel."
                ],
                "venue": "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 195\u2013205.",
                "citeRegEx": "Collins et al\\.,? 2017",
                "shortCiteRegEx": "Collins et al\\.",
                "year": 2017
            },
            {
                "title": "Overview of the cl-scisumm 2016 shared task",
                "author": [
                    "Kokil Jaidka",
                    "Muthu Kumar Chandrasekaran",
                    "Sajal Rustagi",
                    "Min-Yen Kan."
                ],
                "venue": "In Proceedings of Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries",
                "citeRegEx": "Jaidka et al\\.,? 2016",
                "shortCiteRegEx": "Jaidka et al\\.",
                "year": 2016
            },
            {
                "title": "The cl-scisumm shared task 2018: Results and key insights",
                "author": [
                    "Kokil Jaidka",
                    "Michihiro Yasunaga",
                    "Muthu Kumar Chandrasekaran",
                    "Dragomir Radev",
                    "Min-Yen Kan."
                ],
                "venue": "Proceedings of the 3rd Joint Workshop on Bibliometric-enhanced Informa-",
                "citeRegEx": "Jaidka et al\\.,? 2018",
                "shortCiteRegEx": "Jaidka et al\\.",
                "year": 2018
            },
            {
                "title": "Slideseer: A digital library of aligned document and presentation pairs",
                "author": [
                    "Min-Yen Kan."
                ],
                "venue": "Proceedings of the 7th ACM/IEEE-CS Joint Conference on Digital Libraries, JCDL \u201907, pages 81\u201390.",
                "citeRegEx": "Kan.,? 2007",
                "shortCiteRegEx": "Kan.",
                "year": 2007
            },
            {
                "title": "What\u2019s cookin\u2019? interpreting cooking videos using text, speech and vision",
                "author": [
                    "Jonathan Malmaud",
                    "Jonathan Huang",
                    "Vivek Rathod",
                    "Nicholas Johnston",
                    "Andrew Rabinovich",
                    "Kevin Murphy."
                ],
                "venue": "Proceedings of the 2015 Conference of the North Amer-",
                "citeRegEx": "Malmaud et al\\.,? 2015",
                "shortCiteRegEx": "Malmaud et al\\.",
                "year": 2015
            },
            {
                "title": "Citances: Citation sentences for semantic analysis of bioscience text",
                "author": [
                    "Preslav I. Nakov",
                    "Ariel S. Schwartz",
                    "Marti A. Hearst."
                ],
                "venue": "In Proceedings of the SIGIR?04 workshop on Search and Discovery in Bioinformatics.",
                "citeRegEx": "Nakov et al\\.,? 2004",
                "shortCiteRegEx": "Nakov et al\\.",
                "year": 2004
            },
            {
                "title": "Data-driven summarization of scientific articles",
                "author": [
                    "Michael Pfeiffer Nikola Nikolov",
                    "Richard Hahnloser."
                ],
                "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).",
                "citeRegEx": "Nikolov and Hahnloser.,? 2018",
                "shortCiteRegEx": "Nikolov and Hahnloser.",
                "year": 2018
            },
            {
                "title": "Glove: Global vectors for word representation",
                "author": [
                    "Jeffrey Pennington",
                    "Richard Socher",
                    "Christopher D. Manning."
                ],
                "venue": "In EMNLP.",
                "citeRegEx": "Pennington et al\\.,? 2014",
                "shortCiteRegEx": "Pennington et al\\.",
                "year": 2014
            },
            {
                "title": "Get to the point: Summarization with pointergenerator networks",
                "author": [
                    "Abigail See",
                    "Peter J. Liu",
                    "Christopher D. Manning."
                ],
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
                "citeRegEx": "See et al\\.,? 2017",
                "shortCiteRegEx": "See et al\\.",
                "year": 2017
            },
            {
                "title": "Utilizing microblogs for automatic news highlights extraction",
                "author": [
                    "Zhongyu Wei",
                    "Wei Gao."
                ],
                "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 872\u2013883. Dublin City",
                "citeRegEx": "Wei and Gao.,? 2014",
                "shortCiteRegEx": "Wei and Gao.",
                "year": 2014
            },
            {
                "title": "Gibberish, assistant, or master?: Using tweets linking to news for extractive single-document summarization",
                "author": [
                    "Zhongyu Wei",
                    "Wei Gao."
                ],
                "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Informa-",
                "citeRegEx": "Wei and Gao.,? 2015",
                "shortCiteRegEx": "Wei and Gao.",
                "year": 2015
            },
            {
                "title": "Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks",
                "author": [
                    "Michihiro Yasunaga",
                    "Jungo Kasai",
                    "Rui Zhang",
                    "Alexander Fabbri",
                    "Irene Li",
                    "Dan Friedman",
                    "Dragomir Radev."
                ],
                "venue": "Proceed-",
                "citeRegEx": "Yasunaga et al\\.,? 2019",
                "shortCiteRegEx": "Yasunaga et al\\.",
                "year": 2019
            },
            {
                "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
                "author": [
                    "Yukun Zhu",
                    "Ryan Kiros",
                    "Rich Zemel",
                    "Ruslan Salakhutdinov",
                    "Raquel Urtasun",
                    "Antonio Torralba",
                    "Sanja Fidler."
                ],
                "venue": "The IEEE International Con-",
                "citeRegEx": "Zhu et al\\.,? 2015",
                "shortCiteRegEx": "Zhu et al\\.",
                "year": 2015
            }
        ],
        "abstractText": "Currently, no large-scale training data is available for the task of scientific paper summarization. In this paper, we propose a novel method that automatically generates summaries for scientific papers, by utilizing videos of talks at scientific conferences. We hypothesize that such talks constitute a coherent and concise description of the papers\u2019 content, and can form the basis for good summaries. We collected 1716 papers and their corresponding videos, and created a dataset of paper summaries. A model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually. In addition, we validated the quality of our summaries by human experts."
    },
    {
        "title": "Neural Response Generation for Customer Service based on Personality Traits",
        "sections": [
            {
                "heading": null,
                "text": "Proceedings of The 10th International Natural Language Generation conference, pages 252\u2013256, Santiago de Compostela, Spain, September 4-7 2017. c\u00a92017 Association for Computational Linguistics"
            },
            {
                "heading": "1 Introduction",
                "text": "Automated conversational agents are becoming popular for various tasks, such as personal assistants, shopping assistants, or as customer service agents. Automated agents benefit from adapting their personality according to the task at hand (Reeves and Nass, 1996; Tapus and Mataric, 2008) or to the customer (Herzig et al., 2016). Thus, it is desirable for automated agents to be capable of generating responses that express a target personality.\nPersonality is defined as a set of traits which represent durable characteristics of a person. Many models of personality exist while the most common one is the Big Five model (Digman, 1990) , including: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. These traits were correlated with linguistic choices including lexicon and syntax (Mairesse and Walker, 2007).\nIn this paper we study how to encode personality traits as part of neural response generation for conversational agents. Our approach builds upon a sequence-to-sequence (SEQ2SEQ) architecture (Sutskever et al., 2014) by adding an additional\nlayer that represents the target set of personality traits, and a hidden layer that learns high-level personality based features. The response is then generated conditioned on these features.\nSpecifically, we focus on conversational agents for customer service; in this context, many studies examined the effect of specific personality traits of human agents on service performance. Results indicate that conscientiousness (a person\u2019s tendency to act in an organized or thoughtful way) and agreeableness (a person\u2019s tendency to be compassionate and cooperative toward others) correlate with service quality (Blignaut et al., 2014; Sackett, 2014).\nFigure 1 shows examples of customer utterances, followed by two automatically generated responses. The first response (in each example), is generated by a standard SEQ2SEQ response generation system that ignores personality modeling and in effect generates the consensus response of the humans represented in the training data. The second response is generated by our system, and is aimed to generate\n252\ndata for an agent that expresses a high level of a specific trait. In example 1, the agreeableness-agent is more compassionate (expresses empathy) and is more cooperative (asks questions). In example 2, the conscientiousness-agent is more thoughtful (will \u201dcheck the issue\u201d).\nWe experimented with a dataset of 87.5K real customer-agent utterance pairs from social media. We find that leveraging personality encoding improves relative performance up to 46% in BLEU score, compared to a baseline SEQ2SEQ model. To our knowledge, this work is the first to train a neural response generation model that encodes target personality traits."
            },
            {
                "heading": "2 Related Work",
                "text": "Generating responses that express a target personality was previously discussed in different settings. Early work on the PERSONAGE system (Mairesse and Walker, 2007; Mairesse and Walker, 2008; Mairesse and Walker, 2010; Mairesse and Walker, 2011) presented a framework projecting different traits throughout the different modules of an NLG system. The authors explicitly defined 40 linguistic features as generation parameters, and then learned how to weigh them to generate a desired set of traits. While we aim at the same objective, our methodology is different and does not require feature engineering. Our approach utilizes a neural network that automatically learns to represent high level personality based features.\nNeural response generation models (Vinyals and Le, 2015; Shang et al., 2015) are based on a SEQ2SEQ architecture (Sutskever et al., 2014) and employ an encoder to represent the user utterance and an attention-based decoder that generates the agent response one token at a time. Models that aim to generate a coherent persona also exist. Li et al. (2016) modified a SEQ2SEQ model to encode a persona (the character of an artificial agent). The main difference with our work is that we focus on modeling the expression of specific personality traits and not an abstract character. Moreover, their personabased model can only generate responses for the agents that appear in the training data, while our model has no such restriction. Finally, Xu et al. (2017) generated responses for customer service re-\nquests on social media using standard SEQ2SEQ, while we modify it to generate a target personality."
            },
            {
                "heading": "3 Sequence-to-Sequence Setup",
                "text": "We review the SEQ2SEQ attention based model on which our model is based.\nNeural response generation can be viewed as a sequence-to-sequence problem (Sutskever et al., 2014), where a sequence of input language tokens x = x1, . . . , xm , describing the user utterance, is mapped to a sequence of output language tokens y1, . . . , yn , describing the agent response.\nThe encoder is an LSTM (Hochreiter and Schmidhuber, 1997) unit that converts x1, . . . , xm into a sequence of context sensitive embeddings b1, . . . , bm. An attention-based decoder (Bahdanau et al., 2015; Luong et al., 2015) generates output tokens one at a time. At each time step j, it generates yj based on the current hidden state sj , then updates the hidden state sj+1 based on sj and yj . Formally, the decoder is defined by the following equations:\ns1 = tanh(W (s)bm), (1)\np(yj = w | x, y1:j\u22121) \u221d exp(U [sj , cj ]), (2) sj+1 = LSTM([\u03c6(out)(yj), cj ], sj), (3)\nwhere i \u2208 {1, . . . ,m}, j \u2208 {1, . . . , n} and the context vector, cj , is the result of global attention (see (Luong et al., 2015)). The matricesW (s),W (a), U , and the embedding function \u03c6(out) are decoder parameters. The entire model is trained end-to-end by maximizing p(y | x) = \u220fnj=1 p(yj | x, y1:j\u22121)."
            },
            {
                "heading": "4 Personality Generation Model",
                "text": "The model described in section 3 generates responses with maximum likelihood which reflect the consensus of the agents that appear in the training data. This kind of response does not characterize a specific personality and thus can result in inconsistent or unwanted personality cues. In this section we present our PERSONALITY-BASED model (Figure 2) which generates responses conditioned on a target set of personality traits values which the responses should express. The target set of personality traits is represented as a vector p, where pi represents the desired value for the ith trait. This value encodes\nhow strongly should this trait be expressed in the response. Consequently, the size of p depends on the selected personality model (e.g., five traits for the Big Five model).\nAs in (Mairesse and Walker, 2011), we argue that personality traits are exhibited as different types of stylistic linguistic variation. Thus, our model\u2019s response is conditioned on generation parameters which are based on personality traits. In comparison to (Mairesse and Walker, 2011) where generation parameters were defined manually, we learn these high-level features automatically during training. We introduce a personality based features hidden layer hp = \u03c3(W (p)p + b), where W (p) and b are parameters learned by the model during training. Each personality feature hi is a weighted sum of the targeted traits values (following a sigmoid activation). Now, at each token generation, the decoder updates the hidden state conditioned on the personality traits features hp, as well as on the previous hidden state, the output token and the context. Formally, Equation 3 is changed to:\nsj+1 = LSTM([\u03c6(out)(yj), cj , hp], sj), (4)\nConditioning on hp captures the relation of text generation to the underlining personality traits."
            },
            {
                "heading": "5 Experiments",
                "text": "Data. Our model is designed to generate text conditioned on a target set of personality traits. Specifically, we verified its performance in a scenario of customer service. For our experiments we utilized the dataset presented in (Xu et al., 2017), which exhibits a large variety of customer service properties. This dataset is a collection of 1M conversations over customer service Twitter channels of 62 different\nbrands which cover a large variety of product categories. Several preprocessing steps were performed for our purposes:\nWe first split the data to pairs consisting of a single customer utterance and its corresponding agent response. We removed pairs containing non-English sentences. We further removed pairs for agents that participated in less than 30 conversation pairs, so we would have sufficient data for each agent to extract their personality traits (see below). This resulted in 87.5K conversation pairs in total including 633 different agents (138\u00b1160 pairs per agent on average).\nFollowing (Sordoni et al., 2015; Li et al., 2016) we used BLEU (Papineni et al., 2002) for evaluation. Besides BLEU scores, we also report perplexity as an indicator of model capability. For implementation details, refer to Appendix A.\nResults. We experimented with two different settings to measure our model\u2019s performance.\nWarm Start: In the first experiment, data for each agent in the dataset was split between training, validation and test data sets with a fraction of 80%/10%/10%, respectively. We then extracted the agents\u2019 personality traits using an external service (described in Appendix B), from the training data for each agent. These personality traits values are then used during the model training as the values for the personality vector p. In this setting, since all the agents that appear in the test data appear also in the training data, we can also test the performance of (Li et al., 2016), which learns a persona vector for each agent in the training data.\nThe results in table 1 show that the standard SEQ2SEQ model achieved the lowest performance in terms of both perplexity and BLEU score while the competing models which learn a representation\nfor the agents achieved higher performance. The PERSONA-BASED model achieved similar perplexity but higher BLEU score than our model. This is reasonable since PERSONA-BASED is not restricted to personality based features. However, this model can not generate content for agents which do not appear in the training data, and thus, it is limited.\nCold Start: In our second experiment, we split the dataset such that 10% of the agents only formed the validation and test sets (half of each agent\u2019s examples for each set). Data for the other 90% of the agents formed the training set.\nIn this setting, data for agents in the test set does not appear in the training set. These agents represent new personality distributions we would like to generate responses for. Note that, we extracted target personality traits for agents in the training set using their training data, or, for agents in the test set, using validation data. In this setting, it is not possible to test the PERSONA-BASED model since no representation is learned during training for agents in the test set. Thus, we only compare our model to the baseline SEQ2SEQ model. Table 2 shows that, in this setting, we get better performance by utilizing personality based representation: our model achieves a relative 6.7% decrease in perplexity, and a 46% relative improvement in BLEU score. Results from both experiments demonstrate that we can better model the linguistic variation in agent responses by conditioning on target personality traits.\nHuman Evaluation. We conducted a human evaluation of our PERSONALITY-BASED model using a crowd-sourcing service. This evaluation measures whether the responses generated by our model are correlated with the target personality traits. We focused on two personality traits from the Big Five model that are important to customer service: agreeableness and conscientiousness (Blignaut et al., 2014; Sackett, 2014). We extracted 60 customer utterances from the validation set of the cold start setting described above. We selected customer utterances that convey a negative sentiment, since re-\nsponses to this kind of utterances vary much. After sentences were selected, we generated corresponding agent responses in the following way. We generated a high-trait target personality distribution (trait was either agreeableness or conscientiousness), where trait was set to a value of 0.9, and all other traits to 0.5. Similarly, we created a low-trait version where trait was set to 0.1. For each trait and customer utterance we generated a response for the high-trait and low-trait versions.\nEach triplet (a customer utterance followed by high-trait and low-trait generated responses) was evaluated by five master level judges. To get the judges familiar with personality traits, we first presented clear definitions of the two traits, followed by several examples (from the task\u2019s domain), and explanation. Following Li et al. (2016) methodology, the two responses were presented in a random order, and judged on a 5-point zero-sum scale. A score of 2 (\u22122) was assigned if one response was judged to express the trait more (less) than the other response, and 1 (\u22121) if one response expressed the trait \u201csomewhat\u201d more (less) than the other. Ties were assigned a score of zero.\nThe judges rated each pair, and their scores were averaged and mapped into 5 equal-width bins. After discarding ties, we found that the high-trait responses generated by our PERSONALITY-BASED model were judged either more expressive or somewhat more expressive than the low-trait corresponding responses in 61% of cases. If we ignore the somewhat more expressive judgments, the high-trait responses win in 17% of cases."
            },
            {
                "heading": "6 Conclusions and Future Work",
                "text": "We have presented a personality-based response generation model and tested it in customer care tasks, outperforming baseline SEQ2SEQ model. In future work, we would like to generate responses adapted to the personality traits of the customer as well, and to apply our model to other tasks such as education systems."
            },
            {
                "heading": "B Personality Traits Detection",
                "text": "To extract personality traits for agents in our experiments we utilized the IBM Personality Insights service, which is publicly available. This service infers three models of personality traits, namely, Big Five, Needs and Values from social media text. It extracts percentile scores for 52 traits1.\n1www.ibm.com/watson/developercloud/doc/ personality-insights/models.html"
            }
        ],
        "references": [
            {
                "title": "Neural machine translation by jointly learning to align and translate",
                "author": [
                    "D. Bahdanau",
                    "K. Cho",
                    "Y. Bengio."
                ],
                "venue": "International Conference on Learning Representations (ICLR).",
                "citeRegEx": "Bahdanau et al\\.,? 2015",
                "shortCiteRegEx": "Bahdanau et al\\.",
                "year": 2015
            },
            {
                "title": "Personality as predictor of customer service centre agent performance in the banking industry: An exploratory study",
                "author": [
                    "Linda Blignaut",
                    "Leona Ungerer",
                    "Helene Muller."
                ],
                "venue": "SA Journal of Human Resource Management, 12(1).",
                "citeRegEx": "Blignaut et al\\.,? 2014",
                "shortCiteRegEx": "Blignaut et al\\.",
                "year": 2014
            },
            {
                "title": "Personality structure: Emergence of the five-factor model",
                "author": [
                    "John M Digman."
                ],
                "venue": "Annual review of psychology, 41(1):417\u2013440.",
                "citeRegEx": "Digman.,? 1990",
                "shortCiteRegEx": "Digman.",
                "year": 1990
            },
            {
                "title": "Predicting customer satisfaction in customer support conversations in social media using affective features",
                "author": [
                    "Jonathan Herzig",
                    "Guy Feigenblat",
                    "Michal ShmueliScheuer",
                    "David Konopnicki",
                    "Anat Rafaeli."
                ],
                "venue": "UMAP 2016, Halifax, NS, Canada, July 13 - 17,",
                "citeRegEx": "Herzig et al\\.,? 2016",
                "shortCiteRegEx": "Herzig et al\\.",
                "year": 2016
            },
            {
                "title": "Long shortterm memory",
                "author": [
                    "S. Hochreiter",
                    "J. Schmidhuber."
                ],
                "venue": "Neural Computation, 9(8):1735\u20131780.",
                "citeRegEx": "Hochreiter and Schmidhuber.,? 1997",
                "shortCiteRegEx": "Hochreiter and Schmidhuber.",
                "year": 1997
            },
            {
                "title": "A persona-based neural conversation model",
                "author": [
                    "Jiwei Li",
                    "Michel Galley",
                    "Chris Brockett",
                    "Georgios P. Spithourakis",
                    "Jianfeng Gao",
                    "William B. Dolan."
                ],
                "venue": "ACL.",
                "citeRegEx": "Li et al\\.,? 2016",
                "shortCiteRegEx": "Li et al\\.",
                "year": 2016
            },
            {
                "title": "Effective approaches to attention-based neural machine translation",
                "author": [
                    "M. Luong",
                    "H. Pham",
                    "C.D. Manning."
                ],
                "venue": "EMNLP, pages 1412\u20131421.",
                "citeRegEx": "Luong et al\\.,? 2015",
                "shortCiteRegEx": "Luong et al\\.",
                "year": 2015
            },
            {
                "title": "Personage: Personality generation for dialogue",
                "author": [
                    "Franois Mairesse",
                    "Marilyn Walker."
                ],
                "venue": "ACL, pages 496\u2013503.",
                "citeRegEx": "Mairesse and Walker.,? 2007",
                "shortCiteRegEx": "Mairesse and Walker.",
                "year": 2007
            },
            {
                "title": "Trainable generation of big-five personality styles through data-driven parameter estimation",
                "author": [
                    "Fran\u00e7ois Mairesse",
                    "Marilyn A Walker."
                ],
                "venue": "ACL, pages 165\u2013 173.",
                "citeRegEx": "Mairesse and Walker.,? 2008",
                "shortCiteRegEx": "Mairesse and Walker.",
                "year": 2008
            },
            {
                "title": "Towards personality-based user adaptation: psychologically informed stylistic language generation",
                "author": [
                    "Fran\u00e7ois Mairesse",
                    "Marilyn A. Walker."
                ],
                "venue": "User Model. User-Adapt. Interact., 20(3):227\u2013278.",
                "citeRegEx": "Mairesse and Walker.,? 2010",
                "shortCiteRegEx": "Mairesse and Walker.",
                "year": 2010
            },
            {
                "title": "Controlling user perceptions of linguistic style: Trainable generation of personality traits",
                "author": [
                    "Fran\u00e7ois Mairesse",
                    "Marilyn A. Walker."
                ],
                "venue": "Computational Linguistics, 37(3):455\u2013488.",
                "citeRegEx": "Mairesse and Walker.,? 2011",
                "shortCiteRegEx": "Mairesse and Walker.",
                "year": 2011
            },
            {
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "author": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "WeiJing Zhu."
                ],
                "venue": "ACL, pages 311\u2013318.",
                "citeRegEx": "Papineni et al\\.,? 2002",
                "shortCiteRegEx": "Papineni et al\\.",
                "year": 2002
            },
            {
                "title": "How people treat computers, television, and new media like real people and places",
                "author": [
                    "Byron Reeves",
                    "Clifford Nass."
                ],
                "venue": "CSLI Publications and Cambridge.",
                "citeRegEx": "Reeves and Nass.,? 1996",
                "shortCiteRegEx": "Reeves and Nass.",
                "year": 1996
            },
            {
                "title": "Which personality attributes are most important in the workplace",
                "author": [
                    "P.R. Walmsley P.T. Sackett"
                ],
                "venue": "Perspectives on Psychological Science,",
                "citeRegEx": "Sackett,? \\Q2014\\E",
                "shortCiteRegEx": "Sackett",
                "year": 2014
            },
            {
                "title": "A neural conversa",
                "author": [
                    "Oriol Vinyals",
                    "Quoc Le"
                ],
                "venue": null,
                "citeRegEx": "Vinyals and Le.,? \\Q2015\\E",
                "shortCiteRegEx": "Vinyals and Le.",
                "year": 2015
            }
        ],
        "abstractText": "We present a neural response generation model that generates responses conditioned on a target personality. The model learns high level features based on the target personality, and uses them to update its hidden state. Our model achieves performance improvements in both perplexity and BLEU scores over a baseline sequence-to-sequence model, and is validated by human judges."
    },
    {
        "title": "Sobolev Independence Criterion",
        "sections": [
            {
                "heading": "1 Introduction",
                "text": "Feature Selection is an important problem in statistics and machine learning for interpretable predictive modeling and scientific discoveries. Our goal in this paper is to design a dependency measure that is interpretable and can be reliably used to control the False Discovery Rate in feature selection. The mutual information between two random variables X and Y is the most commonly used dependency measure. The mutual information I(X;Y ) is defined as the Kullback-Leibler divergence between the joint distribution pxy of X,Y and the product of their marginals pxpy, I(X;Y ) = KL(pxy, pxpy). Mutual information is however challenging to estimate from samples, which motivated the introduction of dependency measures based on other f -divergences or Integral Probability Metrics [1] than the KL divergence. For instance, the Hilbert-Schmidt Independence Criterion (HSIC) [2] uses the Maximum Mean Discrepancy (MMD) [3] to assess the dependency between two variables, i.e. HSIC(X,Y ) = MMD(pxy, pxpy), which can be easily estimated from samples via Kernel mean embeddings in a Reproducing Kernel Hilbert Space (RKHS) [4]. In this paper we introduce the Sobolev Independence Criterion (SIC), a form of gradient regularized Integral Probability Metric (IPM) [5\u20137] between the joint distribution and the product of marginals. SIC relies on the statistics of the gradient of a witness function, or critic, for both (1) defining the IPM constraint and (2) finding the features that discriminate between the joint and the marginals. Intuitively, the magnitude of the average gradient with respect to a feature gives an importance score for each feature. Hence, promoting its sparsity is a natural constraint for feature selection.\nThe paper is organized as follows: we show in Section 2 how sparsity-inducing gradient penalties can be used to define an interpretable dependency measure that we name Sobolev Independence Criterion\n\u2217Tom Sercu is now with Facebook AI Research, and Cicero Dos Santos with Amazon AWS AI. The work was done when they were at IBM Research.\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nar X\niv :1\n91 0.\n14 21\n2v 1\n[ cs\n.L G\n] 3\n1 O\nct 2\n01 9\n(SIC). We devise an equivalent computational-friendly formulation of SIC in Section 3, that gives rise to additional auxiliary variables \u03b7j . These naturally define normalized feature importance scores that can be used for feature selection. In Section 4 we study the case where the SIC witness function f is restricted to an RKHS and show that it leads to an optimization problem that is jointly convex in f and the importance scores \u03b7. We show that in this case SIC decomposes into the sum of feature scores, which is ideal for feature selection. In Section 5 we introduce a Neural version of SIC, which we show preserves the advantages in terms of interpretability when the witness function is parameterized as a homogeneous neural network, and which we show can be optimized using stochastic Block Coordinate Descent. In Section 6 we show how SIC and conditional Generative models can be used to control the False Discovery Rate using the recently introduced Holdout Randomization Test [8] and Knockoffs [9]. We validate SIC and its FDR control on synthetic and real datasets in Section 8."
            },
            {
                "heading": "2 Sobolev Independence Criterion: Interpretable Dependency Measure",
                "text": "Motivation: Feature Selection. We start by motivating gradient-sparsity regularization in SIC as a mean of selecting the features that maintain maximum dependency between two randoms variable X (the input) and Y (the response) defined on two spaces X \u2282 Rdx and Y \u2282 Rdy (in the simplest case dy = 1). Let pxy be the joint distribution of (X,Y ) and px, py be the marginals of X and Y resp. Let D be an Integral Probability Metric associated with a function space F , i.e for two distributions p, q:\nD(p, q) = sup f\u2208F\nEx\u223cpf(x)\u2212 Ex\u223cqf(x).\nWith p = pxy and q = pxpy this becomes a generalized definition of Mutual Information. Instead of the usual KL divergence, the metric D with its witness function, or critic, f(x, y) measures the distance between the joint pxy and the product of marginals pxpy . With this generalized definition of mutual information, the feature selection problem can be formalized as finding a sparse selector or gate w \u2208 Rdx such thatD(pw x,y, pw xpy) is maximal [10\u201313] , i.e. supw,\u2016w\u2016`0\u2264sD(pw x,y, pw xpy), where is a pointwise multiplication and \u2016w\u2016`0 = #{j|wj 6= 0}. This problem can be written in the following penalized form:\n(P) : sup w sup f\u2208F Epxyf(w x, y)\u2212 Epxpyf(w x, y)\u2212 \u03bb||w||`0 .\nWe can relabel f\u0303(x, y) = f(w x, y) and write (P) as: supf\u0303\u2208F\u0303 Epxy f\u0303(x, y)\u2212Epxpy f\u0303(x, y), where F\u0303 = {f\u0303 |f\u0303(x, y) = f(w x, y)|f \u2208 F , \u2016w\u2016`0 \u2264 s}. Observe that we have: \u2202f\u0303 \u2202xj = wj \u2202f(w x,y) \u2202xj . Since wj is sparse the gradient of f\u0303 is sparse on the support of pxy and pxpy. Hence, we can reformulate the problem (P) as follows:\n(SIC): sup f\u2208F Epxyf(x, y)\u2212 Epxpyf(x, y)\u2212 \u03bbPS(f),\nwhere PS(f) is a penalty that controls the sparsity of the gradient of the witness function f on the support of the measures. Controlling the nonlinear sparsity of the witness function in (SIC) via its gradients is more general and powerful than the linear sparsity control suggested in the initial form (P), since it takes into account the nonlinear interactions with other variables. In the following Section we formalize this intuition by theoretically examining sparsity-inducing gradient penalties [14].\nSparsity Inducing Gradient Penalties. Gradient penalties have a long history in machine learning and signal processing. In image processing the total variation norm is used for instance as a regularizer to induce smoothness. Splines in Sobolev spaces [15], and manifold learning exploit gradient regularization to promote smoothness and regularity of the estimator. In the context of neural networks, gradient penalties were made possible through double back-propagation introduced in [16] and were shown to promote robustness and better generalization. Such smoothness penalties became popular in deep learning partly following the introduction of WGAN-GP [17], and were used as regularizer for distance measures between distributions in connection to optimal transport theory [5\u20137]. Let \u00b5 be a dominant measure of pxy and pxpy the most commonly used gradient penalties is\n\u2126L2(f) = E(x,y)\u223c\u00b5 \u2016\u2207xf(x, y)\u2016 2 .\nWhile this penalty promotes smoothness, it does not control the desired sparsity as discussed in the previous section. We therefore elect to instead use the nonlinear sparsity penalty introduced in [14] :\n\u2126`0(f) = #{j|E(x,y)\u223c\u00b5 \u2223\u2223\u2223\u2202f(x,y)\u2202xj \u2223\u2223\u22232 = 0}, and its relaxation :\n\u2126S(f) = dx\u2211 j=1\n\u221a E(x,y)\u223c\u00b5 \u2223\u2223\u2223\u2223\u2202f(x, y)\u2202xj \u2223\u2223\u2223\u22232.\nAs discussed in [14], E(x,y)\u223c\u00b5 \u2223\u2223\u2223\u2202f(x,y)\u2202xj \u2223\u2223\u22232 = 0 implies that f is constant with respect to variable xj , if the function f is continuously differentiable and the support of \u00b5 is connected. These considerations motivate the following definition of the Sobolev Independence Criterion (SIC):\nSIC(L1)2(pxy, pxpy) = sup f\u2208F\nEpxyf(x, y)\u2212 Epxpyf(x, y)\u2212 \u03bb\n2 (\u2126S(f)) 2 \u2212 \u03c1 2 E\u00b5f2(x, y).\nNote that we add a `1-like penalty (\u2126S(f) ) to ensure sparsity and an `2-like penalty (E\u00b5f2(x, y)) to ensure stability. This is similar to practices with linear models such as Elastic net.\nHere we will consider \u00b5 = pxpy (although we could also use \u00b5 = 12 (pxy + pxpy)). Then, given samples {(xi, yi), i = 1, . . . , N} from the joint probability distribution pxy and iid samples {(xi, y\u0303i), i = 1, . . . , N} from pxpy , SIC can be estimated as follows:\nS\u0302IC(L1)2(pxy, pxpy) = sup f\u2208F\n1\nN N\u2211 i=1 f(xi, yi)\u2212 1 N N\u2211 i=1 f(xi, y\u0303i)\u2212 \u03bb 2 ( \u2126\u0302S(f) )2 \u2212\u03c1 2 1 N N\u2211 i=1 f2(xi, y\u0303i),\nwhere \u2126\u0302S(f) = \u2211dx j=1 \u221a 1 N \u2211N i=1 \u2223\u2223\u2223\u2202f(xi,y\u0303i)\u2202xj \u2223\u2223\u22232. Remark 1. Throughout this paper we consider feature selection only on x since y is thought of as the response. Nevertheless, in many other problems one can perform feature selection on x and y jointly, which can be simply achieved by also controlling the sparsity of\u2207yf(x, y) in a similar way."
            },
            {
                "heading": "3 Equivalent Forms of SIC with \u03b7-trick",
                "text": "As it was just presented, the SIC objective is a difficult function to optimize in practice. First of all, the expectation appears after the square root in the gradient penalties, resulting in a non-smooth term (since the derivative of square root is not continuous at 0). Moreover, the fact that the expectation is inside the nonlinearity introduces a gradient estimation bias when the optimization of the SIC objective is performed using stochastic gradient descent (i.e. using mini-batches). We alleviate these problems (non-smoothness and biased expectation estimation) by making the expectation linear in the objective thanks to the introduction of auxiliary variables \u03b7j that will end up playing an important role in this work. This is achieved thanks to a variational form of the square root that is derived from the following Lemma (which was used for a similar purpose as ours when alleviating the non-smoothness of mixed norms encountered in multiple kernel learning and group sparsity norms):\nLemma 1 ([18],[19]). Let aj , j = 1 . . . d, aj > 0 we have: (\u2211d\nj=1\n\u221a aj )2 = inf{ \u2211d j=1 aj \u03b7j :\n\u03b7, \u03b7j > 0 \u2211d j=1 \u03b7j = 1}, optimum achieved at \u03b7j = \u221a aj/ \u2211 j \u221a aj .\nWe alleviate first the issue of non smoothness of the square root by adding an \u03b5 \u2208 (0, 1), and we define: \u2126S,\u03b5 = \u2211dx j=1 \u221a E(x,y)\u223c\u00b5 \u2223\u2223\u2223\u2202f(x,y)\u2202xj \u2223\u2223\u22232 + \u03b5. Using Lemma 1 the nonlinear sparsity inducing gradient penalty can be written as :\n(\u2126S,\u03b5(f)) 2 = inf{ dx\u2211 j=1 Epxpy \u2223\u2223\u2223\u2202f(x,y)\u2202xj \u2223\u2223\u22232 + \u03b5 \u03b7j : \u03b7, \u03b7j > 0, dx\u2211 j=1 \u03b7j = 1},\nwhere the optimum is achieved for : \u03b7\u2217j,\u03b5 = \u03b2j\u2211dx k=1 \u03b2k , where \u03b22j = Epxpy \u2223\u2223\u2223\u2202f(x,y)\u2202xj \u2223\u2223\u22232 + \u03b5. We refer to \u03b7\u2217j,\u03b5 as the normalized importance score of feature j. Note that \u03b7j is a distribution over the features and gives a natural ranking between the features. Hence, substituting \u2126(S)(f) with \u2126S,\u03b5(f) in its equivalent form we obtain the \u03b5 perturbed SIC:\nSIC(L1)2,\u03b5(pxy, pxpy) = \u2212 inf{L\u03b5(f, \u03b7) : f \u2208 F , \u03b7j , \u03b7j > 0, dx\u2211 j=1 \u03b7j = 1}\nwhere L\u03b5(f, \u03b7) = \u2212\u2206(f, pxy, pxpy) + \u03bb2 \u2211dx j=1 Epxpy \u2223\u2223\u2223 \u2202f(x,y)\u2202xj \u2223\u2223\u22232+\u03b5 \u03b7j + \u03c12Epxpyf 2(x, y), and \u2206(f, pxy, pxpy) = Epxyf(x, y)\u2212 Epxpyf(x, y). Finally, SIC can be empirically estimated as\nS\u0302IC(L1)2,\u03b5(pxy, pxpy) = \u2212 inf{L\u0302\u03b5(f, \u03b7) : f \u2208 F , \u03b7j , \u03b7j > 0, dx\u2211 j=1 \u03b7j = 1}\nwhere L\u0302\u03b5(f, \u03b7) = \u2212\u2206\u0302(f, pxy, pxpy) + \u03bb2 \u2211dx j=1 1 N \u2211N i=1 \u2223\u2223\u2223 \u2202f(xi,y\u0303i)\u2202xj \u2223\u2223\u22232+\u03b5 \u03b7j + \u03c12 1 N \u2211N i=1 f 2(xi, y\u0303i), and\nmain the objective \u2206\u0302(f, pxy, pxpy) = 1N \u2211N i=1 f(xi, yi)\u2212 1 N \u2211N i=1 f(xi, y\u0303i). Remark 2 (Group Sparsity). We can define similarly nonlinear group sparsity, if we would like our critic to depends on subsets of coordinates. Let Gk, k = 1, . . . ,K be an overlapping or non\noverlapping group : \u2126gS(f) = \u2211K k=1 \u221a\u2211 j\u2208Gk Epxpy \u2223\u2223\u2223\u2202f(x,y)\u2202xj \u2223\u2223\u22232. The \u03b7-trick applies naturally."
            },
            {
                "heading": "4 Convex Sobolev Independence Criterion in Fixed Feature Spaces",
                "text": "We will now specify the function space F in SIC and consider in this Section critics of the form:\nF = {f |f(x, y) = \u3008u,\u03a6\u03c9(x, y)\u3009 , \u2016u\u20162 \u2264 \u03b3}, where \u03a6\u03c9 : X \u00d7 Y \u2192 Rm is a fixed finite dimensional feature map. We define the mean embeddings of the joint distribution pxy and product of marginals pxpy as follow: \u00b5(pxy) = Epxy [\u03a6\u03c9(x, y)], \u00b5(pxpy) = Epxpy [\u03a6\u03c9(x, y)] \u2208 Rm. Define the covariance embedding of pxpy as C(pxpy) = Epxpy [\u03a6\u03c9(x, y) \u2297 \u03a6\u03c9(x, y)] \u2208 Rm\u00d7m and finally define the Gramian of derivatives embedding for coordinate j as Dj(pxpy) = Epxpy [ \u2202\u03a6\u03c9(x,y) \u2202xj \u2297 \u2202\u03a6\u03c9(x,y)\u2202xj ] \u2208 R m\u00d7m. We can write the constraint \u2016u\u20162 \u2264 \u03b3 as the penalty term \u2212\u03c4 \u2016u\u2016 2. Define L\u03b5(u, \u03b7) = \u3008u, \u00b5(pxpy)\u2212 \u00b5(pxy)\u3009+ 1 2 \u2329 u, ( \u03bb \u2211dx j=1 Dj(pxpy)+\u03b5 \u03b7j + \u03c1C(pxpy) + \u03c4Im ) u \u232a . Observe that :\nSIC(L1)2,\u03b5(pxy, pxpy) = \u2212 inf{L\u03b5(u, \u03b7) : u \u2208 Rm, \u03b7j , \u03b7j > 0, dx\u2211 j=1 \u03b7j = 1}.\nWe start by remarking that SIC is a form of gradient regularized maximum mean discrepancy [3]. Previous MMD work comparing joint and product of marginals did not use the concept of nonlinear sparsity. For example the Hilbert-Schmidt Independence Criterion (HSIC) [2] uses \u03a6\u03c9(x, y) = \u03c6(x)\u2297 \u03c8(y) with a constraint ||u||2 \u2264 1. CCA and related kernel measures of dependence [20, 21] use L22 constraints L 2 2(px) and L 2 2(py) on each function space separately.\nOptimization Properties of Convex SIC We analyze in this Section the Optimization properties of SIC. Theorem 1 shows that the SIC(L1)2,\u03b5 loss function is jointly strictly convex in (u, \u03b7) and hence admits a unique solution that solves a fixed point problem. Theorem 1 (Existence of a solution, Uniqueness, Convexity and Continuity). Note that L(u, \u03b7) = L\u03b5=0(u, \u03b7). The following properties hold for the SIC loss:\n1) L(u, \u03b7) is differentiable and jointly convex in (u, \u03b7). L(u, \u03b7) is not continuous for \u03b7, such that \u03b7j = 0 for some j.\n2) Smoothing, Perturbed SIC: For \u03b5 \u2208 (0, 1), L\u03b5(u, \u03b7) = L(u, \u03b7) + \u03bb2 \u2211dx j=1 \u03b5 \u03b7j\nis jointly strictly convex and has compact level sets on the probability simplex, and admits a unique minimizer (u\u2217\u03b5, \u03b7 \u2217 \u03b5 ).\n3) The unique minimizer of L\u03b5(u, \u03b7) is a solution of the following fixed point problem: u\u2217\u03b5 = ( \u03bb \u2211dx j=1 Dj(pxpy) \u03b7\u2217j + \u03c1C(pxpy) + \u03c4Im )\u22121 (\u00b5(pxy) \u2212 \u00b5(pxpy)), and \u03b7\u2217j,\u03b5 =\u221a \u3008u\u2217\u03b5 ,Dj(pxpy)u\u2217\u03b5\u3009+\u03b5\u2211dx k=1 \u221a \u3008u\u2217\u03b5 ,Dk(pxpy)u\u2217\u03b5\u3009+\u03b5 .\nThe following Theorem shows that a solution of the unperturbed SIC problem can be obtained from the smoothed SIC(L1)2,\u03b5 in the limit \u03b5\u2192 0:\nTheorem 2 (From Perturbed SIC to SIC). Consider a sequence \u03b5`, \u03b5` \u2192 0 as `\u2192\u221e , and consider a sequence of minimizers (u\u2217\u03b5` , \u03b7 \u2217 ` ) of L\u03b5`(u, \u03b7), and let (u\n\u2217, \u03b7\u2217) be the limit of this sequence, then (u\u2217, \u03b7\u2217) is a minimizer of L(u, \u03b7).\nInterpretability of SIC. The following corollary shows that SIC can be written in terms of the importance scores of the features, since at optimum the main objective is proportional to the constraint term. It is to the best of our knowledge the first dependency criterion that decomposes in the sum of contributions of each coordinate, and hence it is an interpretable dependency measure. Moreover, \u03b7\u2217j are normalized importance scores of each feature j, and their ranking can be used to assess feature importance. Corollary 1 (Interpretability of Convex SIC ). Let (u\u2217, \u03b7\u2217) be the limit defined in Theorem 2. Define f\u2217(x, y) = \u3008u\u2217,\u03a6\u03c9(x, y)\u3009, and \u2016f\u2217\u2016F = \u2016u\u2217\u2016. We have that\nSIC(L1)2(pxy, pxpy) = 1\n2\n( Epxyf\u2217(x, y)\u2212 Epxpyf\u2217(x, y) ) = \u03bb\n2  dx\u2211 j=1 \u221a Epxpy | \u2202f\u2217(x, y) \u2202xj |2 2 + \u03c1 2 Epxpyf\u2217,2(x, y) + \u03c4 2 ||f\u2217||2F .\nMoreover, \u221a Epxpy | \u2202f\u2217(x,y) \u2202xj |2 = \u03b7\u2217j\u2126S,L1(f\u2217) and \u2211dx j=1 \u03b7j = 1. The terms \u03b7 \u2217 j can be seen as quantifying how much dependency as measured by SIC can be explained by a coordinate j. Ranking of \u03b7\u2217j can be used to rank influence of coordinates.\nThanks to the joint convexity and the smoothness of the perturbed SIC, we can solve convex empirical SIC using alternating minimization on u and \u03b7 or block coordinate descent using first order methods such as gradient descent on u and mirror descent [22] on \u03b7 that are known to be globally convergent in this case (see Appendix A for more details)."
            },
            {
                "heading": "5 Non Convex Neural SIC with Deep ReLU Networks",
                "text": "While Convex SIC enjoys a lot of theoretical properties, a crucial short-coming is the need to choose a feature map \u03a6\u03c9 that essentially goes back to the choice of a kernel in classical kernel methods. As an alternative, we propose to learn the feature map as a deep neural network. The architecture of the network can be problem dependent, but we focus here on a particular architecture: Deep ReLU Networks with biases removed. As we show below, using our sparsity inducing gradient penalties with such networks, results in input sparsity at the level of the witness function f of SIC. This is desirable since it allows for an interpretable model, similar to the effect of Lasso with Linear models, our sparsity inducing gradient penalties result in a nonlinear self-explainable witness function f [23], with explicit sparse dependency on the inputs.\nDeep ReLU Networks with no biases, homogeneity and Input Sparsity via Gradient Penalties. We start by invoking the Euler Theorem for homogeneous functions: Theorem 3 (Euler Theorem for Homogeneous Functions). A continuously differentiable function f is defined as homogeneous of degree k if f(\u03bbx) = \u03bbkf(x),\u2200\u03bb \u2208 R. The Theorem states that f is homogeneous of degree k if and only if kf(x) = \u3008\u2207xf(x), x\u3009 = \u2211dx j=1 \u2202f(x) \u2202xj xj . Now consider deep ReLU networks with biases removed for any number of layers L: FReLu = {f |f(x, y) = \u3008u,\u03a6\u03c9(x)\u3009 , where \u03a6\u03c9(x, y) = \u03c3(WL . . . \u03c3(W2\u03c3(W1[x, y]))), u \u2208 Rm,\u03a6\u03c9 : Rdx+dy \u2192 Rm}, where \u03c3(t) = max(t, 0),Wj are linear weights. Any f \u2208 FReLU is clearly homogeneous of degree 1. As an immediate consequence of Euler Theorem we then have: f(x, y) = \u3008\u2207xf(x, y), x\u3009 + \u3008\u2207yf(x, y), y\u3009. The first term is similar to a linear term in a linear model, the second term can be seen as a bias. Using our sparsity-inducing gradient penalties with such networks guarantees that on average on the support of a dominant measure the gradients with respect to x are sparse. Intuitively, the gradients wrt x act like the weight in linear models, and our sparsity inducing gradient penalty act like the `1 regularization of Lasso. The main advantage compared to Lasso is that we have a highly nonlinear decision function, that has better capacity of capturing dependencies between X and Y .\nNon-convex SIC with Stochastic Block Coordinate Descent (BCD). We define the empirical non convex SIC(L1)2 using this function space FReLu as follows:\nS\u0302IC(L1)2(pxy, pxpy) = \u2212 inf{L\u0302(f\u03b8, \u03b7) : f\u03b8 \u2208 FReLU , \u03b7j , \u03b7j > 0, dx\u2211 j=1 \u03b7j = 1},\nwhere \u03b8 = (vec(W1) . . . vec(WL), u) are the network parameters. Algorithm 3 in Appendix B summarizes our stochastic BCD algorithm for training the Neural SIC. The algorithm consists of SGD updates to \u03b8 and mirror descent updates to \u03b7.\nBoosted SIC. When training Neural SIC, we can obtain different critics f` and importance scores \u03b7`, by varying random seeds or hyper-parameters (architecture, batch size etc). Inspired by importance scores in random forest, we define Boosted SIC as the arithmetic mean or the geometric mean of \u03b7`."
            },
            {
                "heading": "6 FDR Control and the Holdout Randomization Test/ Knockoffs.",
                "text": "Controlling the False Discovery Rate (FDR) in Feature Selection is an important problem for reproducible discoveries. In a nutshell, for a feature selection problem given the ground-truth set of features S , and a feature selection method such as SIC that gives a candidate set S\u0302, our goal is to maximize the TPR (True Positive Rate) or the power, and to keep the False Discovery Rate (FDR) under Control. TPR and FDR are defined as follows:\nTPR := E\n[ #{i : i \u2208 S\u0302 \u2229 S}\n#{i : i \u2208 S}\n] FDR := E [ #{i : i \u2208 S\u0302\\S}\n#{i : i \u2208 S\u0302}\n] . (1)\nWe explore in this paper two methods that provably control the FDR: 1) The Holdout Randomization Test (HRT) introduced in [8], that we specialize for SIC in Algorithm 4; 2) Knockoffs introduced in [9] that can be used with any basic feature selection method such as Neural SIC, and guarantees provable FDR control.\nHRT-SIC. We are interested in measuring the conditional dependency between a feature xj and the response variable y conditionally on the other features noted x\u2212j . Hence we have the following null hypothesis: H0 : xj |= y |x\u2212j \u21d0\u21d2 pxy = pxj |x\u2212jpy|x\u2212jpx\u2212j . In order to simulate the null hypothesis, we propose to use generative models for sampling from xj |x\u2212j (See Appendix D). The principle in HRT [8] that we specify here for SIC in Algorithm 4 (given in Appendix B) is the following: instead of refitting SIC under H0, we evaluate the mean of the witness function of SIC on a holdout set sampled under H0 (using conditional generators for R rounds). The deviation of the mean of the witness function under H0 from its mean on a holdout from the real distribution gives us p-values. We use the Benjamini-Hochberg [24] procedure on those p-values to achieve a target FDR. We apply HRT-SIC on a shortlist of pre-selected features per their ranking of \u03b7j .\nKnockoffs-SIC. Knockoffs [25] work by finding control variables called knockoffs x\u0303 that mimic the behavior of the real features x and provably control the FDR [9]. We use here Gaussian knockoffs [9] and train SIC on the concatenation of [x, x\u0303], i.e we train SIC([X; X\u0303], Y ) and obtain \u03b7 that has now twice the dimension dx, i.e for each real feature j, there is the real importance score \u03b7j and the knockoff importance score \u03b7j+dx . knockoffs-SIC consists in using the statistics Wj = \u03b7j \u2212 \u03b7j+dx and the knockoff filter [9] to select features based on the sign of Wj (See Alg. 5 in Appendix)."
            },
            {
                "heading": "7 Relation to Previous Work",
                "text": "Kernel/Neural Measure of Dependencies. As discussed earlier SIC can be seen as a sparse gradient regularized MMD [3, 7] and relates to the Sobolev Discrepancy of [5, 6]. Feature selection with MMD was introduced in [10] and is based on backward elimination of features by recomputing MMD on the ablated vectors. SIC has the advantage of fitting one critic that has interpretable feature scores. Related to the MMD is the Hilbert Schmidt Independence Criterion (HSIC) and other variants of kernel dependency measures introduced in [2, 21]. None of those criteria has a nonparametric sparsity constraint on its witness function that allows for explainability and feature selection. Other Neural measures of dependencies such as MINE [26] estimate the KL divergence using neural networks, or that of [27] that estimates a proxy to the Wasserstein distance using Neural Networks.\nInterpretability, Sparsity, Saliency and Sensitivity Analysis. Lasso and elastic net [28] are interpretable linear models that exploit sparsity, but are limited to linear relationships. Random forests\n[29] have a heuristic for determining feature importance and are successful in practice as they can capture nonlinear relationships similar to SIC. We believe SIC can potentially leverage the deep learning toolkit for going beyond tabular data where random forests excel, to more structured data such as time series or graph data. Finally, SIC relates to saliency based post-hoc interpretation of deep models such as [30\u201332]. While those method use the gradient information for a post-hoc analysis, SIC incorporates this information to guide the learning towards the important features. As discussed in Section 2.1 many recent works introduce deep networks with input sparsity control through a learned gate or a penalty on the weights of the network [11\u201313]. SIC exploits a stronger notion of sparsity that leverages the relationship between the different covariates."
            },
            {
                "heading": "8 Experiments",
                "text": "Synthetic Data Validation. We first validate our methods and compare them to baseline models in simulation studies on synthetic datasets where the ground truth is available by construction. For this we generate the data according to a model y = f(x) + where the model f(\u00b7) and the noise define the specific synthetic dataset (see Appendix F.1). In particular, the value of y only depends on a subset of features xi, i = 1, . . . , p through f(\u00b7), and performance is quantified in terms of TPR and FDR in discovering them among the irrelevant features. We experiment with two datasets: A) Complex multivariate synthetic data (SinExp), which is generated from a complex multivariate model proposed in [33] Sec 5.3, where 6 ground truth features xi out of 50 generate the output y through a non-linearity involving the product and composition of the cos, sin and exp functions (see Appendix F.1). We therefore dub this dataset SinExp. To increase the difficulty even further, we introduce a pairwise correlation between all features of 0.5. In Fig. 1 we show results for datasets of 125 and 500 samples repeated 100 times comparing performance of our models with the one of two baselines: Elastic Net (EN) and Random Forest (RF). B) Liang Dataset. We show results on the benchmark dataset proposed by [34], specifically the generalized Liang dataset matching most of the setup from [8] Sec 5.1. We provide dataset details and results in Appendix F.1 (Results in Figure 2).\nTPR top 6 FDR top 6\nTPR HRT FDR HRT\nTPR top 6 FDR top 6\nTPR HRT FDR HRT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPo we\nr a nd\nF DR\nElastic Net Random Forest\nTPR top 6 FDR top 6\nTPR HRT FDR HRT\nTPR top 6 FDR top 6\nTPR HRT FDR HRT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMSE + Sobolev Penalty SIC\nDataset SINEXP, n=125 samples\nDataset SINEXP, n=500 samples\nFeature Selection on Drug Response dataset. We consider as a real-world application the Cancer Cell Line Encyclopedia (CCLE) dataset [36], described in Appendix F.2. We study the result of using the normalized importance scores \u03b7j from SIC for (heuristic) feature selection, against features selected by Elastic Net. Table 1 shows the heldout MSE of a predictor trained on selected features, averaged over 100 runs (each run: new randomized 90%/10% data split, NN initialization). The goal here is to quantify the predictiveness of features selected by SIC on its own, without the full randomized testing machinery. The SIC critic and regressor NN were respectively the big_critic and regressor_NN described with training details in Appendix F.3, while the random forest is trained with default hyper parameters from scikit-learn [37]. We can see that, with just \u03b7j , informative features are selected for the downstream regression task, with performance comparable to those selected by ElasticNet, which was trained explicitly for this task. The features selected with high \u03b7j values and their overlap with the features selected by ElasticNet are listed in Appendix F.2 Table 3.\nHIV-1 Drug Resistance with Knockoffs-SIC. The second real-world dataset that we analyze is the HIV-1 Drug Resistance[38], which consists in detecting mutations associated with resistance to a drug type. For our experiments we use all the three classes of drugs: Protease Inhibitors (PIs), Nucleoside Reverse Transcriptase Inhibitors (NRTIs), and Non-nucleoside Reverse Transcriptase Inhibitors (NNRTIs). We use the pre-processing of each dataset (<drug-class, drug-type>) of the knockoff tutorial [39] made available by the authors. Concretely, we construct a dataset (X, X\u0303) of the concatenation of the real data and Gaussian knockoffs [9], and fit SIC([X, X\u0303], Y ). As explained in Section 6, we use in the knockoff filter the statistics Wj = \u03b7j \u2212 \u03b7j+dx , i.e. the difference of SIC importance scores between each feature and its corresponding knockoff. For SIC experiments, we use small_critic architecture (See Appendix F.3 for training details). We use Boosted SIC, by varying the batch sizes in N \u2208 {10, 30, 50}, and computing the geometric mean of \u03b7 produced by those three setups as the feature importance needed for Knockoffs. Results are summarized in Table 2."
            },
            {
                "heading": "9 Conclusion",
                "text": "We introduced in this paper the Sobolev Independence Criterion (SIC), a dependency measure that gives rise to feature importance which can be used for feature selection and interpretable decision making. We laid down the theoretical foundations of SIC and showed how it can be used in conjunction with the Holdout Randomization Test and Knockoffs to control the FDR, enabling reliable discoveries. We demonstrated the merits of SIC for feature selection in extensive synthetic and real-world experiments with controlled FDR."
            },
            {
                "heading": "A Algorithms for Convex SIC",
                "text": "Algorithms and Empirical Convex SIC from Samples. Given samples from the joint and the marginals, it is easy to see that the empirical loss L\u0302\u03b5 can be written in the same way with empirical feature mean embeddings \u00b5\u0302(pxy) = 1N \u2211N i=1 \u03a6\u03c9(xi, yi) and \u00b5\u0302(pxpy) = 1 N \u2211N i=1 \u03a6\u03c9(xi, y\u0303i),\ncovariances C\u0302(pxpy) = 1N \u2211N i=1 \u03a6\u03c9(xi, y\u0303i)\u2297 \u03a6\u03c9(xi, y\u0303i) and derivatives grammians D\u0302j(pxpy) = 1 N \u2211N i=1 \u2202\u03a6\u03c9(xi,y\u0303i) \u2202xj\n\u2297 \u2202\u03a6\u03c9(x,y)\u2202xj . Given the strict convexity of L\u0302\u03b5 jointly in u and \u03b7, alternating optimization as given in Algorithm 1 in Appendix is known to be convergent to a global optima (Theorem 4.1 in [40]). Similarly Block Coordinate Descent (BCD) using first order methods as given in Algorithms 3 and 2 (in Appendix): gradient descent on u and mirror descent on \u03b7 (in order to satisfy the simplex constraint [22]) are also known to be globally convergent (Theo 2 in [41].)\nAlgorithm 1 Alternating Optimization Inputs: \u03b5,\u03bb, \u03c4 , \u03c1 , \u03a6\u03c9 Initialize \u03b7\u0302j = 1dx ,\u2200j, \u03b4\u0302 = \u00b5\u0302(pxy)\u2212 \u00b5\u0302(pxpy) for i = 1 . . .Maxiter do u\u0302\u2190( \u03bb \u2211dx j=1 D\u0302j(pxpy) \u03b7\u0302j + \u03c1C\u0302(pxpy) + \u03c4Im )\u22121 \u03b4\u0302\n\u03b7\u0302j \u2190 \u221a \u3008u\u0302,D\u0302j(pxpy)u\u0302\u3009+\u03b5\u2211dx\nk=1\n\u221a \u3008u\u0302,D\u0302k(pxpy)u\u0302\u3009+\u03b5\nend for Output: u\u0302, \u03b7\u0302\nAlgorithm 2 Block Coordinate Descent Inputs: \u03b5,\u03bb, \u03c4 , \u03c1, \u03b1, \u03b1\u03b7 (learning rates),\u03a6\u03c9 Initialize \u03b7\u0302j = 1dx ,\u2200j , Softmax(z) = ez/ \u2211dx j=1 e zj\nfor i = 1 . . .Maxiter do Gradient step u: u\u0302\u2190 u\u0302\u2212 \u03b1\u2202L\u0302\u03b5(u\u0302,\u03b7\u0302)\u2202u Mirror Descent \u03b7 : logit\u2190 log(\u03b7\u0302)\u2212 \u03b1\u03b7 \u2202L\u0302\u03b5(u\u0302,\u03b7\u0302)\u2202\u03b7 \u03b7\u0302 \u2190 Softmax(logit) {stable implementation of softmax} end for Output: u\u0302, \u03b7\u0302"
            },
            {
                "heading": "B Algorithms for Neural SIC, HRT-SIC and Model-X Knockoff SIC",
                "text": "Algorithm 3 (non convex) Neural SIC(X,Y ) (Stochastic BCD )\nInputs: X,Y dataset X \u2208 RN\u00d7dx , Y \u2208 RN\u00d7dy , such that (xi = Xi,., yi = Yi,.) \u223c pxy Hyperparameters: \u03b5,\u03bb, \u03c4 , \u03c1, \u03b1\u03b8, \u03b1\u03b7 (learning rates) Initialize \u03b7j = 1dx ,\u2200j , Softmax(z) = ez/ \u2211dx j=1 e zj for iter = 1 . . .Maxiter do Fetch a minibatch of size N (xi, yi) \u223c pxy Fetch a minibatch of size N (xi, y\u0303i) \u223c pxpy {y\u0303i obtained by permuting rows of Y } Stochastic Gradient step on \u03b8: \u03b8 \u2190 \u03b8 \u2212 \u03b1\u03b8 \u2202L\u0302(f\u03b8,\u03b7)\u2202\u03b8 {We use ADAM} Mirror Descent \u03b7 : logit\u2190 log(\u03b7)\u2212 \u03b1\u03b7 \u2202L\u0302(f\u03b8,\u03b7)\u2202\u03b7 \u03b7 \u2190 Softmax(logit) {stable implementation of softmax} end for Output: f\u03b8, \u03b7\nAlgorithm 4 HRT With SIC (X,Y )\nInputs: Dtrain = (Xtr, Ytr) , a Heldout set DHoldout = (X,Y ), features Cutoff K SIC: (f\u03b8\u2217 , \u03b7\u2217) = SIC(Dtrain) {Alg. 3} Score of witness on Hold out : S\u2217 = MEAN(f\u03b8\u2217(X,Y )) Conditional Generators Pre-trained conditional Generator : G(x\u2212j , j) predicts Xj |X\u2212j Shortlist : I = INDEXTOPK(\u03b7) {p-values for j \u2208 I; randomizations tests} for j \u2208 I do\nfor r = 1 . . . R do Construct X\u0303 , X\u0303.,k = X.,k\u2200k 6= j and X\u0303.,j = G(X\u2212j , j) {Simulate Null Hyp.} Sj,r = MEAN(f\u03b8\u2217(X\u0303, Y )) {Score of witness function on the Null} end for pj = 1 R+1 ( 1 + \u2211R r=1 1Srj\u2265S\u2217\n) end for discoveries =BH(p,targetFDR) {BenjaminiHochberg Procedure} Output: discoveries\nAlgorithm 5 Model-X Knockoffs FDR control with SIC\nInputs: Dtrain = (Xtr, Ytr) , Model-X knockoff features X\u0303 \u223c ModelX(Xtr), target FDR q Train SIC: (f\u03b8\u2217 , \u03b7) = SIC([Xtr, X\u0303], Y ), {Alg. 3} where [Xtr, X\u0303] is the concatenation of Xtr and knockoffs X\u0303 for j = 1, . . . , dX do\nCompute importance score of j feature: Wj = \u03b7j \u2212 \u03b7j+dx , where \u03b7j+dX is the \u03b7 of feature knockoff X\u0303j\nend for Compute threshold \u03c4 > 0 by setting \u03c4 = min { t > 0 :\n#{j:Wj\u2264\u2212t} #{j:Wj\u2265t} \u2264 q } Output: discoveries {j : Wj > \u03c4}"
            },
            {
                "heading": "C Proofs",
                "text": "Proof of Theorem 1. 1) Let \u03b4 = \u00b5(pxy)\u2212 \u00b5(pxpy). We have\nL(u, \u03b7) = \u2212\u3008u, \u03b4\u3009+ 1 2 \u3008u, (\u03c1C(pxpy) + \u03c4Im)u\u3009+ \u03bb 2 \u2211 j \u3008u,Dj(pxpy)u\u3009 \u03b7j , u \u2208 Rm and \u03b7 \u2208 \u2206dx\nwhere \u2206dx is the probability simplex. L is the sum of a linear tem and quadratic terms (convex in u) and a function of the form\nf(u, \u03b7) = 1\n2 dx\u2211 j=1 u>Aju \u03b7j\nwhere Aj are PSD matrices, and \u03b7 is in the probability simplex (convex). Hence it is enough to show that f is jointly convex. Let g(w, \u03b7) = w\n>Aw \u03b7 , \u03b7 > 0. The Hessian of g(w, \u03b7), Hg has the following\nform:\nHg(w, \u03b7) =\n[ \u22022L\n\u2202w\u2297\u2202w \u22022L \u2202w\u2202\u03b7\n\u22022L \u2202\u03b7\u2202w\n\u22022L \u2202\u03b72\n] = [ A \u03b7 \u2212 Aw \u03b72\n\u2212w >A \u03b72 w>Aw \u03b73 ] Let us prove that for all (w, \u03b7), \u03b7j >,\u2200j0:\n(w\u2032, \u03b7\u2032)>Hg(w, \u03b7)(w\u2032, \u03b7\u2032) \u2265 0,\u2200(w\u2032, \u03b7\u2032), \u03b7\u2032j > 0,\u2200j\nWe have :\n(w\u2032, \u03b7\u2032)>Hg(w, \u03b7)(w\u2032, \u03b7\u2032) = \u3008w\u2032, Aw\u2032\u3009 \u03b7 \u2212 2\u03b7\u2032 \u3008w \u2032, Aw\u3009 \u03b72 + \u03b7\u20322 w>Aw \u03b73\n= 1\n\u03b7\n( \u3008w\u2032, Aw\u2032\u3009 \u2212 2\u03b7 \u2032\n\u03b7 \u3008w\u2032, Aw\u3009+ \u03b7\n\u20322\n\u03b72 w>Aw ) = 1\n\u03b7 \u2225\u2225\u2225\u2225A 12w\u2032 \u2212 \u03b7\u2032\u03b7 A 12w \u2225\u2225\u2225\u22252\n2\n\u2265 0 for \u03b7 > 0\nNow back to f it is easy to see that :\n(w\u2032, \u03b7\u2032)>Hf(w, \u03b7)(w\u2032, \u03b7\u2032) = dx\u2211 j=1 1 \u03b7j \u2225\u2225\u2225\u2225A 12j w\u2032 \u2212 \u03b7\u2032j\u03b7jA 12j w \u2225\u2225\u2225\u22252 2 \u2265 0 for \u03b7 \u2208 \u2206dxj , \u03b7j > 0.\nHence the loss L is jointly convex in (u, \u03b7). Due to discontinuity at \u03b7j = 0 the loss is not continuous .\n2) It is easy to see that the hessian becomes definite:\n(w\u2032, \u03b7\u2032)>HL\u03b5(w, \u03b7)(w \u2032, \u03b7\u2032) = dx\u2211 j=1 1 \u03b7j (\u2225\u2225\u2225\u2225A 12j w\u2032 \u2212 \u03b7\u2032j\u03b7jA 12j w \u2225\u2225\u2225\u22252 2 + \u03b5( \u03b7\u2032j \u03b7j )2 ) > 0 for \u03b7 \u2208 \u2206dxj , \u03b7j , \u03b7 \u2032 j > 0,\nandL\u03b5(u, \u03b7) is jointly strictly convex, u is unconstrained and \u03b7 belongs to a convex set (the probability simplex) and hence admits a unique minimizer.\n3) The unique minimizer satisfies first order optimality conditions for the following Lagragian:\nL (u, \u03b7, \u03be) = L\u03b5(u, \u03b7) + \u03be( \u2211 j \u03b7j \u2212 1)\n\u2202L (u, \u03b7, \u03be)\n\u2202u = \u2212\u03b4 + \u03bb dx\u2211 j=1 Dj(pxpy) \u03b7j + \u03c1C(pxpy) + \u03c4Im u = 0 and\n\u2202L (u, \u03b7, \u03be) \u2202\u03b7j = \u2212\u03bb 2 \u3008u,Dj(pxpy)u\u3009+ \u03b5 \u03b72j + \u03be = 0\nand \u2202L (u, \u03b7, \u03be) \u2202\u03be = \u2211 j \u03b7j \u2212 1 = 0\nHence:\nu\u2217\u03b5 = \u03bb dx\u2211 j=1 Dj(pxpy) \u03b7\u2217j + \u03c1C(pxpy) + \u03c4Im \u22121 (\u00b5(pxy)\u2212 \u00b5(pxpy)) and :\n\u03b7\u2217j,\u03b5 = \u221a \u3008u\u2217\u03b5, Dj(pxpy)u\u2217\u03b5\u3009+ \u03b5\u2211dx\nk=1 \u221a \u3008u\u2217\u03b5, Dk(pxpy)u\u2217\u03b5\u3009+ \u03b5 .\nProof of Theorem 2. The proof follows similar proof in Argryou 2008.\nS\u03b5(u) = L(u\u03b5, \u03b7(u\u03b5)) = \u2212\u3008u, \u03b4\u3009+ 1\n2 \u3008u, (\u03c1C(pxpy) + \u03c4Im)u\u3009+\n\u03bb\n2 \u2211 j \u221a \u3008u,Dj(pxpy)u\u3009+ \u03b5 2\nLet {(u`n , \u03b7`n(u`n)), n \u2208 N} be a limiting subsequence of minimizers of L\u03b5`n (., .) and let (u \u2217, \u03b7\u2217) be its limit as n\u2192\u221e. From the definition of S\u03b5(u), we see that minu S\u03b5(u) decreases as \u03b5 decreases to zero, and admits a limit S\u0304 = minu S0(u). Hence S\u03b5`n \u2192 S\u0304. Note that S\u03b5(u) is continuous in both \u03b5 and u and we have finally S0(u\u2217) = S\u0304, and u\u2217 is a minimizer of S0.\nProof of Corollary 1 . The optimum (u\u2217\u03b5, \u03b7 \u2217 \u03b5 ) satisfies:\n\u2212\u03b4 + \u03bb dx\u2211 j=1 Dj(pxpy) \u03b7j + \u03c1C(pxpy) + \u03c4Im u\u2217\u03b5 = 0\nLet f\u2217(x) = \u3008u,\u03a6\u03c9(x, y)\u3009 and define ||f\u2217\u03b5 ||F = \u2016u\u2217\u03b5\u20162. It follows that \u03b7\u2217j =\u221a Epxpy \u2223\u2223\u2223 \u2202f\u2217\u03b5 (x,y)\u2202xj \u2223\u2223\u22232+\u03b5\u2211 k \u221a Epxpy\n\u2223\u2223\u2223 \u2202f\u2217\u03b5 (x,y)\u2202xk \u2223\u2223\u22232+\u03b5 Note that we have Epxyf\u2217\u03b5 (x, y)\u2212 Epxpyf\u2217\u03b5 (x, y)\n= \u3008\u03b4, u\u2217\u03b5\u3009\n=\n\u2329 u\u2217\u03b5, \u03bb dx\u2211 j=1 Dj(pxpy) \u03b7\u2217j,\u03b5 + \u03c1C(pxpy) + \u03c4Im u\u2217\u03b5 \u232a\n= \u03bb  dx\u2211 j=1 \u221a Epxpy | \u2202f\u2217\u03b5 (x, y) \u2202xj |2 + \u03b5 2 + \u03c1Epxpyf\u2217,2\u03b5 (x, y) + \u03c4 ||f\u2217\u03b5 ||2F\nSIC(L1)2,\u03b5 = Epxyf\u2217\u03b5 (x, y)\u2212 Epxpyf\u2217\u03b5 (x, y)\u2212 1\n2 (\u03bb  dx\u2211 j=1 \u221a Epxpy | \u2202f\u2217\u03b5 (x, y) \u2202xj |2 + \u03b5 2 + \u03c1Epxpyf\u2217,2\u03b5 (x, y) + \u03c4 ||f\u2217\u03b5 ||2F )\n= \u03bb\n2  dx\u2211 j=1 \u221a Epxpy | \u2202f\u2217\u03b5 (x, y) \u2202xj |2 + \u03b5 2 + \u03c1 2 Epxpyf\u2217,2\u03b5 (x, y) + \u03c4 2 ||f\u2217\u03b5 ||2F\n= 1\n2\n( Epxyf\u2217\u03b5 (x, y)\u2212 Epxpyf\u2217\u03b5 (x, y) ) We conclude by taking \u03b5\u2192 0."
            },
            {
                "heading": "D FDR Control with HRT and Conditional Generative Models",
                "text": "The Holdout Randomization Test (HRT) is a principled method to produce valid p-values for each feature, that enables the control over the false discovery of a predictive model [8]. The p-value associated to each feature xj essentially quantifies the result of a conditional independence test with the null hypothesis stating that xj is independent of the output y, conditioned on all the remaining features x\u2212j = (x1, . . . , xj\u22121, xj+1, . . . , xp). This in practice requires the availability of an estimate of the complete conditional of each feature xj , i.e. of P (xj |x\u2212j). HRT then samples the values of xj from this conditional distribution to obtain the p-value associated to it. Taking inspiration from neural network models for conditional generation (see e.g. [42]) we train a neural network to act as a generator of a features xj given the remaining features x\u2212j as inputs, as a replacement for the conditional distributions P (xj |x\u2212j). In all of our tasks, one three-layer neural network with 200 ReLU units and Conditional Batch Normalization (BCN) [43] applied to all hidden layers serves as generator for all features j = 1, . . . , p. A sample from P (xj |x\u2212j) is generated by giving as input to the network an index j indicating the feature to generate, and a sample x\u2212j \u223c P (x\u2212j), represented as a sample from the full joint distribution x \u223c P (x1, . . . , xp), with feature j being masked out. In practice, the index j and x \u223c P (x1, . . . , xp) are given as inputs to the generator, and the neural network model does the masking, and sends the index j to the CBN modules which normalize their inputs using j-dependent centering and normalization parameters. The output of the generator is a nbins-dimensional softmax over bins tessellating the range of the distribution of xj , such that the bins are uniform quantiles of the inverse CDF of the distribution of xj estimated over the training set. In all simulations we used a number of bins nbins = 100.\nGenerators are trained randomly sampling an index j = 1, . . . , p for each sample x in the training set, and minimizing the cross-entropy loss between the output of the generator neural network Gen(j,x) and xj using mini-batch SGD. In particular, we used the Adam optimizer [44] with the default pytorch [45] parameters and learning rate \u03bb = 0.003 which is halved every 20 epochs, and batch size of 128."
            },
            {
                "heading": "E Discussion of SIC: Consistency, Computational Complexity and FDR Control",
                "text": "SIC consistency. In order to recover the correct conditional independence we elected to use FDR control techniques to perform those dependent hypotheses testing (btw coordinates). By combining SIC with HRT and knockoffs we can guarantee that the correct dependency is recovered while the FDR is under control. For the consistency of SIC in the classical sense, one needs to analyze the solution of SIC, when the critic is not constrained to belonging to an RKHS. This can be done by studying the solution of the equivalent PDE corresponding to this problem (which is challenging, but we think can also be managed through the \u03b7- trick). Then one would proceed by finding 1) conditions under which this solution exists in the RKHS, 2) generalization bounds from samples to the population solution in the RKHS. We leave this analysis for future work.\nComputational Complexity of Neural SIC. The cost of training SIC with SGD and mirror descent has the same scaling in the size of the problem as training the base regressor neural network via back-propagation. The only additional overhead is the gradient penalty, where the cost is that of double back-propagation. In our experiments, this added computational cost is not an issue when training is performed on GPU.\nSIC-HRT versus SIC-Knockoffs. For a comparison between HRT and knockoffs, we refer the reader to [8], which shows similar performance for either method in terms of controlling FDR. Each method has its advantages. In HRT most of the computation is in 1) training the generative models, and 2) performing the randomization test, i.e. forwarding the data through the critic and computing p-values for each coordinate for R runs. On the other hand, if knockoff features can be modelled as a multivariate Gaussian, controlling FDR with knockoffs can be done very cheaply, since it does not require randomization tests. If instead knockoff features have to be generated through nonlinear models, knockoffs can be computationally expensive as well (see for example [46])."
            },
            {
                "heading": "F Experimental details",
                "text": "F.1 Synthetic Datasets\nF.1.1 Complex Multivariate Synthetic Dataset (SinExp)\nThe SinExp dataset is generated from a complex multivariate model proposed in [33] Sec 5.3, where 6 features xi out of 50 generate the output y through a non-linearity involving the product and composition of the cos, sin and exp functions, as follows:\ny = sin(x1(x1 + x2)) cos(x3 + x4x5) sin(e x5 + ex6 \u2212 x2).\nWe increase the difficulty even further by introducing a pairwise correlation between all features of 0.5. We perform experiments using datasets of 125 and 500 samples. For each sample size, 100 independent datasets are generated.\nF.1.2 Liang Dataset\nLiang Dataset is a variant of the synthetic dataset proposed by [34]. The dataset prescribes a regression model with 500-dimensional correlated input features x, where the 1-D regression target y depends on the first 40 features only (the last 460 correlated features are ignored). In the original dataset proposed by [34], y depends on 4 features only, this more complex version of the dataset that uses 40 features was proposed by [8]. The target y is computed as follows:\ny = 9\u2211 j=0 [w4jx4j + w4j+1x4j+1 + tanh(w4j+2x4j+2 + w4j+3x4j+3)] + \u03c3 , (2)\nwith \u03c3 = 0.5 and \u223c N (0, 1). As in [8], the 500 features are generated to have 0.5 correlation coefficient with each other,\nxj = (\u03c1+ zj)/2 , j = 1, . . . , 500 , (3)\nwhere \u03c1 and zj are independently generated from N (0, 1).\nOur experimental results are the average over 100 generated datasets, each consisting of 500 train and 500 heldout samples.\nF.2 CCLE Dataset\nThe Cancer Cell Line Encyclopedia (CCLE) dataset [36] provides data about of anti-cancer drug response in cancer cell lines. The dataset contains the phenotypic response measured as the area under the dose-response curve (AUC) for a variety of drugs that were tested against hundreds of cell lines. [36] analyzed each cell to obtain gene mutation and expression features. The total number of data points (cells) is 479. We followed the preprocessing steps by [8] and first screened the genomic features to filter out features with less than 0.1 magnitude Pearson correlation to the AUC. This resulted in a final set of about 7K features. The main goal in this task is to discover the genomic features associated with drug response. Following [8], we perform experiments for the drug PLX4720. Table 3 presents the top-10 genomic features selected by SIC according to \u03b7j values. In Sec. 8, we also present quantitative results that show the effectiveness of these features when used to train regression models.\nF.3 SIC Neural Network descriptions and training details\nThe first critic network used in the experiments (with SinExp and HIV-1 datasets) is a standard three-layer ReLU dropout network with no biases, i.e. small_critic. When using this network, the\ninputs X and Y are first concatenated then given as input to the network. The two first layers have size 100, while the last layer has size 1. We train the network using Adam optimizer with \u03b21 = 0.5, \u03b22 = 0.999, weight_decay=1e-4 learning rate \u03b1\u03b7 = 1e-3 and \u03b1\u03b7 = 0.1, and perform 4000 training iterations/updates, computed with batches of size 100. All NNs used in our experiments were implemented using PyTorch [45].\nsmall_critic( (branchxy): Sequential(\n(0): Linear(in_features=51, out_features=100, bias=False) (1): ReLU() (2): Dropout(p=0.3) (3): Linear(in_features=100, out_features=100, bias=False) (4): ReLU() (5): Dropout(p=0.3) (6): Linear(in_features=100, out_features=1, bias=False)\n) )\nThe critic network used in the experiments with Liang and CCLE datasets contains two different branches that separately process the inputs X (branchx) and Y (branchy), then the output of these two branches are concatenated and processed by a final branch that contains three-layer LeakyReLU network (branchxy). We name this network big_critic (see figure bellow for details about layer sizes). This network is trained with the same Adam settings as above for 4000 updates (Liang) and 8000 updates (CCLE).\nbig_critic( (branchx): Sequential(\n(0): Linear(in_features=500, out_features=100, bias=True) (1): LeakyReLU(negative_slope=0.01) (2): Linear(in_features=100, out_features=100, bias=True) (3): LeakyReLU(negative_slope=0.01)\n) (branchy): Sequential(\n(0): Linear(in_features=1, out_features=100, bias=True) (1): LeakyReLU(negative_slope=0.01) (2): Linear(in_features=100, out_features=100, bias=True) (3): LeakyReLU(negative_slope=0.01)\n) (branchxy): Sequential(\n(0): Linear(in_features=200, out_features=100, bias=True) (1): LeakyReLU(negative_slope=0.01) (2): Linear(in_features=100, out_features=100, bias=True) (3): LeakyReLU(negative_slope=0.01) (4): Linear(in_features=100, out_features=1, bias=True)\n) )\nThe regressor NN used for the downstream regression task in Section 8 is a standard three-layer ReLU dropout network. This regressor NN was trained with the same Adam settings as above for 1000 updates with a batchSize of 16. We did not perform any hyperparameter tuning or model selection on heldout MSE performance.\nregressor_NN( (net): Sequential(\n(0): Linear(in_features=7251, out_features=100, bias=True) (1): ReLU() (2): Dropout(p=0.3) (3): Linear(in_features=100, out_features=100, bias=True) (4): ReLU() (5): Dropout(p=0.3)\n(6): Linear(in_features=100, out_features=1, bias=True) )\n)"
            }
        ],
        "references": [
            {
                "title": "On integral probability metrics, \u03c6-divergences and binary classification",
                "author": [
                    "Bharath K. Sriperumbudur",
                    "Kenji Fukumizu",
                    "Arthur Gretton",
                    "Bernhard Scholkopf",
                    "Gert R.G. Lanckriet"
                ],
                "venue": null,
                "citeRegEx": "1",
                "shortCiteRegEx": "1",
                "year": 2009
            },
            {
                "title": "A kernel statistical test of independence",
                "author": [
                    "A. Gretton",
                    "K. Fukumizu",
                    "CH. Teo",
                    "L. Song",
                    "B. Sch\u00f6lkopf",
                    "AJ. Smola"
                ],
                "venue": "In Advances in neural information processing systems",
                "citeRegEx": "2",
                "shortCiteRegEx": "2",
                "year": 2008
            },
            {
                "title": "A kernel two-sample test",
                "author": [
                    "Arthur Gretton",
                    "Karsten M. Borgwardt",
                    "Malte J. Rasch",
                    "Bernhard Sch\u00f6lkopf",
                    "Alexander Smola"
                ],
                "venue": null,
                "citeRegEx": "3",
                "shortCiteRegEx": "3",
                "year": 2012
            },
            {
                "title": "Kernel mean embedding of distributions: A review and beyond",
                "author": [
                    "Krikamol Muandet",
                    "Kenji Fukumizu",
                    "Bharath Sriperumbudur",
                    "Bernhard Sch\u00f6lkopf"
                ],
                "venue": null,
                "citeRegEx": "4",
                "shortCiteRegEx": "4",
                "year": 2017
            },
            {
                "title": "On gradient regularizers for mmd gans",
                "author": [
                    "Michael Arbel",
                    "Dougal J. Sutherland",
                    "Mikolaj Binkowski",
                    "Arthur Gretton"
                ],
                "venue": "NeurIPS,",
                "citeRegEx": "7",
                "shortCiteRegEx": "7",
                "year": 2018
            },
            {
                "title": "The holdout randomization test: Principled and easy black box feature selection",
                "author": [
                    "W. Tansey",
                    "V. Veitch",
                    "H. Zhang",
                    "R. Rabadan",
                    "D.M. Blei"
                ],
                "venue": "arXiv preprint arXiv:1811.00645,",
                "citeRegEx": "8",
                "shortCiteRegEx": "8",
                "year": 2018
            },
            {
                "title": "Panning for gold: model-x knockoffs for high dimensional controlled variable selection",
                "author": [
                    "Emmanuel Candes",
                    "Yingying Fan",
                    "Lucas Janson",
                    "Jinchi Lv"
                ],
                "venue": null,
                "citeRegEx": "9",
                "shortCiteRegEx": "9",
                "year": 2018
            },
            {
                "title": "Feature selection via dependence maximization",
                "author": [
                    "Le Song",
                    "Alex Smola",
                    "Arthur Gretton",
                    "Justin Bedo",
                    "Karsten Borgwardt"
                ],
                "venue": "J. Mach. Learn. Res.,",
                "citeRegEx": "10",
                "shortCiteRegEx": "10",
                "year": 2012
            },
            {
                "title": "Sparse-input neural networks for high-dimensional nonparametric regression and classification",
                "author": [
                    "Jean Feng",
                    "Noah Simon"
                ],
                "venue": null,
                "citeRegEx": "11",
                "shortCiteRegEx": "11",
                "year": 2017
            },
            {
                "title": "Variable selection via penalized neural network: a drop-out-one loss approach",
                "author": [
                    "Mao Ye",
                    "Yan Sun"
                ],
                "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
                "citeRegEx": "12",
                "shortCiteRegEx": "12",
                "year": 2018
            },
            {
                "title": "Deep supervised feature selection using stochastic gates",
                "author": [
                    "Yutaro Yamada",
                    "Ofir Lindenbaum",
                    "Sahand Negahban",
                    "Yuval Kluger"
                ],
                "venue": null,
                "citeRegEx": "13",
                "shortCiteRegEx": "13",
                "year": 2018
            },
            {
                "title": "Nonparametric sparsity and regularization",
                "author": [
                    "Lorenzo Rosasco",
                    "Silvia Villa",
                    "Sofia Mosci",
                    "Matteo Santoro",
                    "Alessandro Verri"
                ],
                "venue": "J. Mach. Learn. Res.,",
                "citeRegEx": "14",
                "shortCiteRegEx": "14",
                "year": 2013
            },
            {
                "title": "Smoothing noisy data with spline functions",
                "author": [
                    "Grace Wahba"
                ],
                "venue": "Numerische mathematik,",
                "citeRegEx": "15",
                "shortCiteRegEx": "15",
                "year": 1975
            },
            {
                "title": "Improving generalization performance using double backpropagation",
                "author": [
                    "Harris Drucker",
                    "Yann LeCun"
                ],
                "venue": "IEEE Transactions on Neural Networks,",
                "citeRegEx": "16",
                "shortCiteRegEx": "16",
                "year": 1992
            },
            {
                "title": "Improved training of wasserstein gans",
                "author": [
                    "Ishaan Gulrajani",
                    "Faruk Ahmed",
                    "Martin Arjovsky",
                    "Vincent Dumoulin",
                    "Aaron Courville"
                ],
                "venue": null,
                "citeRegEx": "17",
                "shortCiteRegEx": "17",
                "year": 2017
            },
            {
                "title": "Convex multi-task feature learning",
                "author": [
                    "Andreas Argyriou",
                    "Theodoros Evgeniou",
                    "Massimiliano Pontil"
                ],
                "venue": "Mach. Learn.,",
                "citeRegEx": "18",
                "shortCiteRegEx": "18",
                "year": 2008
            },
            {
                "title": "Optimization with Sparsity-Inducing Penalties (Foundations and Trends(R) in Machine Learning)",
                "author": [
                    "Francis Bach",
                    "Rodolphe Jenatton",
                    "Julien Mairal"
                ],
                "venue": "Now Publishers Inc.,",
                "citeRegEx": "19",
                "shortCiteRegEx": "19",
                "year": 2011
            },
            {
                "title": "Canonical ridge and econometrics of joint production",
                "author": [
                    "H.D. Vinod"
                ],
                "venue": "Journal of Econometrics,",
                "citeRegEx": "20",
                "shortCiteRegEx": "20",
                "year": 1976
            },
            {
                "title": "Kernel measures of conditional dependence",
                "author": [
                    "Kenji Fukumizu",
                    "Arthur Gretton",
                    "Xiaohai Sun",
                    "Bernhard Sch\u00f6lkopf"
                ],
                "venue": "In Advances in Neural Information Processing Systems",
                "citeRegEx": "21",
                "shortCiteRegEx": "21",
                "year": 2008
            },
            {
                "title": "Mirror descent and nonlinear projected subgradient methods for convex optimization",
                "author": [
                    "Amir Beck",
                    "Marc Teboulle"
                ],
                "venue": "Oper. Res. Lett.,",
                "citeRegEx": "22",
                "shortCiteRegEx": "22",
                "year": 2003
            },
            {
                "title": "Towards robust interpretability with self-explaining neural networks",
                "author": [
                    "David Alvarez Melis",
                    "Tommi Jaakkola"
                ],
                "venue": "In Advances in Neural Information Processing Systems",
                "citeRegEx": "23",
                "shortCiteRegEx": "23",
                "year": 2018
            },
            {
                "title": "Controlling the false discovery rate: A Practical and powerful approach to multiple testing",
                "author": [
                    "Y. Benjamini",
                    "Y. Hochberg"
                ],
                "venue": "J. Roy. Statist. Soc.,",
                "citeRegEx": "24",
                "shortCiteRegEx": "24",
                "year": 1995
            },
            {
                "title": "Controlling the false discovery rate via knockoffs",
                "author": [
                    "Rina Foygel Barber",
                    "Emmanuel J Cand\u00e8s"
                ],
                "venue": "The Annals of Statistics,",
                "citeRegEx": "25",
                "shortCiteRegEx": "25",
                "year": 2015
            },
            {
                "title": "Mine: Mutual information neural estimation, 2018",
                "author": [
                    "Mohamed Ishmael Belghazi",
                    "Aristide Baratin",
                    "Sai Rajeswar",
                    "Sherjil Ozair",
                    "Yoshua Bengio",
                    "Aaron Courville",
                    "R Devon Hjelm"
                ],
                "venue": null,
                "citeRegEx": "26",
                "shortCiteRegEx": "26",
                "year": 2018
            },
            {
                "title": "Wasserstein dependency measure for representation learning, 2019",
                "author": [
                    "Sherjil Ozair",
                    "Corey Lynch",
                    "Yoshua Bengio",
                    "Aaron van den Oord",
                    "Sergey Levine",
                    "Pierre Sermanet"
                ],
                "venue": null,
                "citeRegEx": "27",
                "shortCiteRegEx": "27",
                "year": 2019
            },
            {
                "title": "The Elements of Statistical Learning",
                "author": [
                    "Trevor Hastie",
                    "Robert Tibshirani",
                    "Jerome Friedman"
                ],
                "venue": null,
                "citeRegEx": "28",
                "shortCiteRegEx": "28",
                "year": 2001
            },
            {
                "title": "Learning important features through propagating activation differences",
                "author": [
                    "Avanti Shrikumar",
                    "Peyton Greenside",
                    "Anshul Kundaje"
                ],
                "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
                "citeRegEx": "30",
                "shortCiteRegEx": "30",
                "year": 2017
            },
            {
                "title": "Deep inside convolutional networks: Visualising image classification models and saliency",
                "author": [
                    "Karen Simonyan",
                    "Andrea Vedaldi",
                    "Andrew Zisserman"
                ],
                "venue": "maps. International Conference on Learning Representations (Workshop Track).,",
                "citeRegEx": "31",
                "shortCiteRegEx": "31",
                "year": 2014
            },
            {
                "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
                "author": [
                    "Sebastian Bach",
                    "Alexander Binder",
                    "Gr\u00e9goire Montavon",
                    "Frederick Klauschen",
                    "Klaus-Robert M\u00fcller",
                    "Wojciech Samek"
                ],
                "venue": "PLoS ONE,",
                "citeRegEx": "32",
                "shortCiteRegEx": "32",
                "year": 2015
            },
            {
                "title": "Sparse-input neural networks for high-dimensional nonparametric regression and classification",
                "author": [
                    "Jean Feng",
                    "Noah Simon"
                ],
                "venue": "arXiv preprint arXiv:1711.07592,",
                "citeRegEx": "33",
                "shortCiteRegEx": "33",
                "year": 2017
            },
            {
                "title": "Bayesian neural networks for selection of drug sensitive genes",
                "author": [
                    "Faming Liang",
                    "Qizhai Li",
                    "Lei Zhou"
                ],
                "venue": "Journal of the American Statistical Association,",
                "citeRegEx": "34",
                "shortCiteRegEx": "34",
                "year": 2018
            },
            {
                "title": "The cancer cell line encyclopedia enables predictive modelling of anticancer drug sensitivity",
                "author": [
                    "Jordi Barretina",
                    "Giordano Caponigro",
                    "Nicolas Stransky",
                    "Kavitha Venkatesan",
                    "Adam A Margolin",
                    "Sungjoon Kim",
                    "Christopher J Wilson",
                    "Joseph Leh\u00e1r",
                    "Gregory V Kryukov",
                    "Dmitriy Sonkin"
                ],
                "venue": null,
                "citeRegEx": "36",
                "shortCiteRegEx": "36",
                "year": 2012
            },
            {
                "title": "Scikitlearn: Machine learning in python",
                "author": [
                    "Fabian Pedregosa",
                    "Ga\u00ebl Varoquaux",
                    "Alexandre Gramfort",
                    "Vincent Michel",
                    "Bertrand Thirion",
                    "Olivier Grisel",
                    "Mathieu Blondel",
                    "Peter Prettenhofer",
                    "Ron Weiss",
                    "Vincent Dubourg"
                ],
                "venue": "Journal of machine learning research,",
                "citeRegEx": "37",
                "shortCiteRegEx": "37",
                "year": 2011
            },
            {
                "title": "Genotypic predictors of human immunodeficiency virus type 1 drug resistance",
                "author": [
                    "Soo-Yon Rhee",
                    "Jonathan Taylor",
                    "Gauhar Wadhera",
                    "Asa Ben-Hur",
                    "Douglas L Brutlag",
                    "Robert W Shafer"
                ],
                "venue": "Proceedings of the National Academy of Sciences,",
                "citeRegEx": "38",
                "shortCiteRegEx": "38",
                "year": 2006
            },
            {
                "title": "R tutorial for knockoffs - 4. https://web.stanford.edu/ group/candes/knockoffs/software/knockoffs/tutorial-4-r.html, 2017",
                "author": [
                    "Matteo Sesia",
                    "Evan Patterson"
                ],
                "venue": null,
                "citeRegEx": "39",
                "shortCiteRegEx": "39",
                "year": 2017
            },
            {
                "title": "Convergence of a block coordinate descent method for nondifferentiable minimization",
                "author": [
                    "P. Tseng"
                ],
                "venue": "J. Optim. Theory Appl.,",
                "citeRegEx": "40",
                "shortCiteRegEx": "40",
                "year": 2001
            },
            {
                "title": "A unified convergence analysis of block successive minimization methods for nonsmooth optimization",
                "author": [
                    "Meisam Razaviyayn",
                    "Mingyi Hong",
                    "Zhi-Quan Luo"
                ],
                "venue": "SIAM Journal on Optimization,",
                "citeRegEx": "41",
                "shortCiteRegEx": "41",
                "year": 2013
            },
            {
                "title": "Conditional image generation with pixelcnn decoders",
                "author": [
                    "Aaron Van den Oord",
                    "Nal Kalchbrenner",
                    "Lasse Espeholt",
                    "Oriol Vinyals",
                    "Alex Graves"
                ],
                "venue": "In Advances in neural information processing systems,",
                "citeRegEx": "42",
                "shortCiteRegEx": "42",
                "year": 2016
            },
            {
                "title": "Learning visual reasoning without strong priors",
                "author": [
                    "Ethan Perez",
                    "Harm de Vries",
                    "Florian Strub",
                    "Vincent Dumoulin",
                    "Aaron Courville"
                ],
                "venue": "arXiv preprint arXiv:1707.03017,",
                "citeRegEx": "43",
                "shortCiteRegEx": "43",
                "year": 2017
            },
            {
                "title": "Adam: A method for stochastic optimization",
                "author": [
                    "Diederik Kingma",
                    "Jimmy Ba"
                ],
                "venue": "In ICLR,",
                "citeRegEx": "44",
                "shortCiteRegEx": "44",
                "year": 2015
            },
            {
                "title": "Automatic differentiation in pytorch",
                "author": [
                    "Adam Paszke",
                    "Sam Gross",
                    "Soumith Chintala",
                    "Gregory Chanan",
                    "Edward Yang",
                    "Zachary DeVito",
                    "Zeming Lin",
                    "Alban Desmaison",
                    "Luca Antiga",
                    "Adam Lerer"
                ],
                "venue": "NIPS-W,",
                "citeRegEx": "45",
                "shortCiteRegEx": "45",
                "year": 2017
            }
        ],
        "abstractText": "We propose the Sobolev Independence Criterion (SIC), an interpretable dependency measure between a high dimensional random variable X and a response variable Y . SIC decomposes to the sum of feature importance scores and hence can be used for nonlinear feature selection. SIC can be seen as a gradient regularized Integral Probability Metric (IPM) between the joint distribution of the two random variables and the product of their marginals. We use sparsity inducing gradient penalties to promote input sparsity of the critic of the IPM. In the kernel version we show that SIC can be cast as a convex optimization problem by introducing auxiliary variables that play an important role in feature selection as they are normalized feature importance scores. We then present a neural version of SIC where the critic is parameterized as a homogeneous neural network, improving its representation power as well as its interpretability. We conduct experiments validating SIC for feature selection in synthetic and real-world experiments. We show that SIC enables reliable and interpretable discoveries, when used in conjunction with the holdout randomization test and knockoffs to control the False Discovery Rate. Code is available at http://github.com/ibm/sic."
    },
    {
        "title": "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections",
        "sections": [
            {
                "heading": null,
                "text": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 600\u2013609, Portland, Oregon, June 19-24, 2011. c\u00a92011 Association for Computational Linguistics"
            },
            {
                "heading": "1 Introduction",
                "text": "Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems. Supervised part-of-speech (POS) taggers, for example, approach the level of inter-annotator agreement (Shen et al., 2007, 97.3% accuracy for English). However, supervised methods rely on labeled training data, which is time-consuming and expensive to generate. Unsupervised learning approaches appear to be a natural solution to this problem, as they require only unannotated text for train\u2217This research was carried out during an internship at Google Research.\ning models. Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.\nTo bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language. This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al., 2000; Xi and Hwa, 2005; Ganchev et al., 2009). Naseem et al. (2009) and Snyder et al. (2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.\nOur work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways. First, we use a novel graph-based framework for projecting syntactic information across language boundaries. To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\u00a73), and then use graph label propagation to project syntactic information from English to the foreign language (\u00a74). Second, we treat the projected labels as features in an unsuper-\n1For simplicity of exposition we refer to the resource-poor language as the \u201cforeign language.\u201d Similarly, we use English as the resource-rich language, but any other language with labeled resources could be used instead.\n600\nvised model (\u00a75), rather than using them directly for supervised training. To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011). Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction. Because there might be some controversy about the exact definitions of such universals, this set of coarse-grained POS categories is defined operationally, by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages. These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.\nWe evaluate our approach on eight European languages (\u00a76), and show that both our contributions provide consistent and statistically significant improvements. Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%)."
            },
            {
                "heading": "2 Approach Overview",
                "text": "The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages. Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus. As discussed in more detail in \u00a73, we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types, while the vertices on the English side are individual word types. Graph construction does not require any labeled data, but makes use of two similarity functions. The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function, designed to indicate how syntactically\n2See Christodoulopoulos et al. (2010) for a discussion of metrics for evaluating unsupervised POS induction systems.\nAlgorithm 1 Bilingual POS Induction Require: Parallel English and foreign language\ndata De and Df , unlabeled foreign training data \u0393f ; English tagger. Ensure: \u0398f , a set of parameters learned using a constrained unsupervised model (\u00a75).\n1: De\u2194f \u2190 word-align-bitext(De,Df ) 2: D\u0302e \u2190 pos-tag-supervised(De) 3: A \u2190 extract-alignments(De\u2194f , D\u0302e) 4: G\u2190 construct-graph(\u0393f ,Df ,A) 5: G\u0303\u2190 graph-propagate(G) 6: \u2206\u2190 extract-word-constraints(G\u0303) 7: \u0398f \u2190 pos-induce-constrained(\u0393f ,\u2206) 8: Return \u0398f\nsimilar the middle words of the connected trigrams are (\u00a73.2). To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (\u00a73.3).3\nSince we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side. To initialize the graph we tag the English side of the parallel text using a supervised model. By aggregating the POS labels of the English tokens to types, we can generate label distributions for the English vertices. Label propagation can then be used to transfer the labels to the peripheral foreign vertices (i.e. the ones adjacent to the English vertices) first, and then among all of the foreign vertices (\u00a74). The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (\u00a75). The following three sections elaborate these different stages is more detail."
            },
            {
                "heading": "3 Graph Construction",
                "text": "In graph-based learning approaches one constructs a graph whose vertices are labeled and unlabeled examples, and whose weighted edges encode the degree to which the examples they link have the same label (Zhu et al., 2003). Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand, using individual words as the vertices throws away the context\n3The word alignment methods do not use POS information.\nnecessary for disambiguation; on the other hand, it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences. Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning. More recently, Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model. They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types, and edge weights based on distributional similarity, to improve a supervised conditional random field tagger."
            },
            {
                "heading": "3.1 Graph Vertices",
                "text": "We extend Subramanya et al.\u2019s intuitions to our bilingual setup. Because the information flow in our graph is asymmetric (from English to the foreign language), we use different types of vertices for each language. The foreign language vertices (denoted by Vf ) correspond to foreign trigram types, exactly as in Subramanya et al. (2010). On the English side, however, the vertices (denoted by Ve) correspond to word types. Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams. Furthermore, we do not connect the English vertices to each other, but only to foreign language vertices.4\nThe graph vertices are extracted from the different sides of a parallel corpus (De, Df ) and an additional unlabeled monolingual foreign corpus \u0393f , which will be used later for training. We use two different similarity functions to define the edge weights among the foreign vertices and between vertices from different languages."
            },
            {
                "heading": "3.2 Monolingual Similarity Function",
                "text": "Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010). We briefly review it here for completeness. We define a symmetric similarity function K(ui, uj) over two for-\n4This is because we are primarily interested in learning foreign language taggers, rather than improving supervised English taggers. Note, however, that it would be possible to use our graph-based framework also for completely unsupervised POS induction in both languages, similar to Snyder et al. (2009).\neign language vertices ui, uj \u2208 Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1. Each feature concept is akin to a random variable and its occurrence in the text corresponds to a particular instantiation of that random variable. For each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5, we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (PMI) between the two.5 The similarity between two trigram types is given by summing over the PMI values over feature instantiations that they have in common. This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them. Finally, note that while most feature concepts are lexicalized, others, such as the suffix concept, are not.\nGiven this similarity function, we define a nearest neighbor graph, where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices. We use N (u) to denote the neighborhood of vertex u, and fixed n = 5 in our experiments."
            },
            {
                "heading": "3.3 Bilingual Similarity Function",
                "text": "To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments. Since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the English sentences De\n5Note that many combinations are impossible giving a PMI value of 0; e.g., when the trigram type and the feature instantiation don\u2019t have words in common.\nand their foreign language translations Df .6 Label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence (> 0.9) alignments De\u2194f .\nBased on these high-confidence alignments we can extract tuples of the form [u \u2194 v], where u is a foreign trigram type, whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts."
            },
            {
                "heading": "3.4 Graph Initialization",
                "text": "So far the graph has been completely unlabeled. To initialize the graph for label propagation we use a supervised English tagger to label the English side of the bitext.7 We then simply count the individual labels of the English tokens and normalize the counts to produce tag distributions over English word types. These tag distributions are used to initialize the label distributions over the English vertices in the graph. Note that since all English vertices were extracted from the parallel text, we will have an initial label distribution for all vertices in Ve."
            },
            {
                "heading": "3.5 Graph Example",
                "text": "A very small excerpt from an Italian-English graph is shown in Figure 1. As one can see, only the trigrams [suo incarceramento ,], [suo iter ,] and [suo carattere ,] are connected to English words. In this particular case, all English vertices are labeled as nouns by the supervised tagger. In general, the neighborhoods can be more diverse and we allow a soft label distribution over the vertices. It is worth noting that the middle words of the Italian trigrams are nouns too, which exhibits the fact that the similarity metric connects types having the same syntactic category. In the label propagation stage, we propagate the automatic English tags to the aligned Italian trigram types, followed by further propagation solely among the Italian vertices. 6We ran six iterations of IBM Model 1 (Brown et al., 1993), followed by six iterations of the HMM model (Vogel et al., 1996) in both directions. 7We used a tagger based on a trigram Markov model (Brants, 2000) trained on the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), for its fast speed and reasonable accuracy (96.7% on sections 22-24 of the treebank, but presumably much lower on the (out-of-domain) parallel corpus)."
            },
            {
                "heading": "4 POS Projection",
                "text": "Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language. We use label propagation in two stages to generate soft labels on all the vertices in the graph. In the first stage, we run a single step of label propagation, which transfers the label distributions from the English vertices to the connected foreign language vertices (say, V lf ) at the periphery of the graph. Note that because we extracted only high-confidence alignments, many foreign vertices will not be connected to any English vertices. This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui \u2208 Vf aligns to English words vy tagged with label y:\nri(y) =\n\u2211 vy\n#[ui \u2194 vy]\u2211 y\u2032 \u2211 vy\u2032 #[ui \u2194 vy\u2032 ] (1)\nThe second stage consists of running traditional label propagation to propagate labels from these peripheral vertices V lf to all foreign language vertices\nin the graph, optimizing the following objective: C(q) = \u2211\nui\u2208Vf\\V lf ,uj\u2208N (ui)\nwij\u2016qi \u2212 qj\u20162\n+ \u03bd \u2211\nui\u2208Vf\\V lf\n\u2016qi \u2212 U\u20162\ns.t. \u2211\ny\nqi(y) = 1 \u2200ui\nqi(y) \u2265 0 \u2200ui, y qi = ri \u2200ui \u2208 V lf (2)\nwhere the qi (i = 1, . . . , |Vf |) are the label distributions over the foreign language vertices and \u00b5 and \u03bd are hyperparameters that we discuss in \u00a76.4. We use a squared loss to penalize neighboring vertices that have different label distributions: \u2016qi \u2212 qj\u20162 =\u2211\ny(qi(y)\u2212qj(y))2, and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y . It can be shown that this objective is convex in q.\nThe first term in the objective function is the graph smoothness regularizer which encourages the distributions of similar vertices (large wij) to be similar. The second term is a regularizer and encourages all type marginals to be uniform to the extent that is allowed by the first two terms (cf. maximum entropy principle). If an unlabeled vertex does not have a path to any labeled vertex, this term ensures that the converged marginal for this vertex will be uniform over all tags, allowing the middle word of such an unlabeled vertex to take on any of the possible tags.\nWhile it is possible to derive a closed form solution for this convex objective function, it would require the inversion of a matrix of order |Vf |. Instead, we resort to an iterative update based method. We formulate the update as follows:\nq (m) i (y) = ri(y) if ui \u2208 V l f \u03b3i(y)\n\u03bai otherwise\n(3)\nwhere \u2200ui \u2208 Vf \\ V lf , \u03b3i(y) and \u03bai are defined as: \u03b3i(y) = \u2211\nuj\u2208N (ui)\nwijq (m\u22121) j (y) + \u03bd U(y) (4)\n\u03bai = \u03bd + \u2211\nuj\u2208N (ui)\nwij (5)\nWe ran this procedure for 10 iterations."
            },
            {
                "heading": "5 POS Induction",
                "text": "After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x\u2212 x x+ over the left and right context words:\np(y|x) =\n\u2211 x\u2212,x+\nqi(y)\u2211 x\u2212,x+,y\u2032 qi(y \u2032)\n(6)\nWe then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value \u03c4 :\ntx(y) = { 1 if p(y|x) \u2265 \u03c4 0 otherwise\n(7)\nWe describe how we choose \u03c4 in \u00a76.4. This vector tx is constructed for every word in the foreign vocabulary and will be used to provide features for the unsupervised foreign language POS tagger.\nWe develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010). For a sentence x and a state sequence z, a first order Markov model defines a distribution:\nP\u0398(X = x,Z = z) = P\u0398(Z1 = z1)\u00b7\u220f|x| i=1 P\u0398(Zi+1 = zi+1 | Zi = zi)\ufe38 \ufe37\ufe37 \ufe38\ntransition\n\u00b7\nP\u0398(Xi = xi | Zi = zi)\ufe38 \ufe37\ufe37 \ufe38 emission\n(8)\nIn a traditional Markov model, the emission distribution P\u0398(Xi = xi | Zi = zi) is a set of multinomials. The feature-based model replaces the emission distribution with a log-linear model, such that:\nP\u0398(X = x | Z = z) = exp \u0398>f(x, z)\u2211\nx\u2032\u2208Val(X)\nexp \u0398>f(x\u2032, z)\n(9) where Val(X) corresponds to the entire vocabulary. This locally normalized log-linear model can look at various aspects of the observation x, incorporating overlapping features of the observation. In our experiments, we used the same set of features as BergKirkpatrick et al. (2010): an indicator feature based\non the word identity x, features checking whether x contains digits or hyphens, whether the first letter of x is upper case, and suffix features up to length 3. All features were conjoined with the state z.\nWe trained this model by optimizing the following objective function:\nL(\u0398) = N\u2211\ni=1 log \u2211 z P\u0398(X = x (i),Z = z(i))\n\u2212C\u2016\u0398\u201622 (10)\nNote that this involves marginalizing out all possible state configurations z for a sentence x, resulting in a non-convex objective. To optimize this function, we used L-BFGS, a quasi-Newton method (Liu and Nocedal, 1989). For English POS tagging, BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977).8 Moreover, this route of optimization outperformed a vanilla HMM trained with EM by 12%.\nWe adopted this state-of-the-art model because it makes it easy to experiment with various ways of incorporating our novel constraint feature into the log-linear emission model. This feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx. The function \u03bb : F \u2192 C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in \u00a76.2:\nft(x, z) = log(tx(y)), if \u03bb(z) = y (11)\nNote that when tx(y) = 1 the feature value is 0 and has no effect on the model, while its value is \u2212\u221e when tx(y) = 0 and constrains the HMM\u2019s state space. This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold \u03c4 on the posterior distribution of tags for a given word type (Eq. 7). It would have therefore also been possible to use the integer programming (IP) based approach of 8See \u00a73.1 of Berg-Kirkpatrick et al. (2010) for more details about their modification of EM, and how gradients are computed for L-BFGS.\nRavi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. However, we do not explore this possibility in the current work."
            },
            {
                "heading": "6 Experiments and Results",
                "text": "Before presenting our results, we describe the datasets that we used, as well as two baselines."
            },
            {
                "heading": "6.1 Datasets",
                "text": "We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side. The availability of these resources guided our selection of foreign languages. For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). The parallel data came from the Europarl corpus (Koehn, 2005) and the ODS United Nations dataset (UN, 2006). Taking the intersection of languages in these resources, and selecting languages with large amounts of parallel data, yields the following set of eight Indo-European languages: Danish, Dutch, German, Greek, Italian, Portuguese, Spanish and Swedish.\nOf course, we are primarily interested in applying our techniques to languages for which no labeled resources are available. However, we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach. We paid particular attention to minimize the number of free parameters, and used the same hyperparameters for all language pairs, rather than attempting language-specific tuning. We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available."
            },
            {
                "heading": "6.2 Part-of-Speech Tagset and HMM States",
                "text": "We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC\n9We extracted only the words and their POS tags from the treebanks. 10Available at http://code.google.com/p/universal-pos-tags/.\n(punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words). While there might be some controversy about the exact definition of such a tagset, these 12 categories cover the most frequent part-of-speech and exist in one form or another in all of the languages that we studied.\nFor each language under consideration, Petrov et al. (2011) provide a mapping \u03bb from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags. The supervised POS tagging accuracies (on this tagset) are shown in the last row of Table 2. The taggers were trained on datasets labeled with the universal tags.\nThe number of latent HMM states for each language in our experiments was set to the number of fine tags in the language\u2019s treebank. In other words, the set of hidden states F was chosen to be the fine set of treebank tags. Therefore, the number of fine tags varied across languages for our experiments; however, one could as well have fixed the set of HMM states to be a constant across languages, and created one mapping to the universal POS tagset."
            },
            {
                "heading": "6.3 Various Models",
                "text": "To provide a thorough analysis, we evaluated three baselines and two oracles in addition to two variants of our graph-based approach. We were intentionally lenient with our baselines:\n\u2022 EM-HMM: A traditional HMM baseline, with multinomial emission and transition distributions estimated by the Expectation Maximization algorithm. We evaluated POS tagging accuracy using the lenient many-to-1 evaluation approach (Johnson, 2007).\n\u2022 Feature-HMM: The vanilla feature-HMM of Berg-Kirkpatrick et al. (2010) (i.e. no additional constraint feature) served as a second baseline. Model parameters were estimated with L-BFGS and evaluation again used a greedy many-to-1 mapping.\n\u2022 Projection: Our third baseline incorporates bilingual information by projecting POS tags directly across alignments in the parallel data. For unaligned words, we set the tag to the most frequent tag in the corresponding treebank. For\neach language, we took the same number of sentences from the bitext as there are in its treebank, and trained a supervised feature-HMM. This can be seen as a rough approximation of Yarowsky and Ngai (2001).\nWe tried two versions of our graph-based approach:\n\u2022 No LP: Our first version takes advantage of our bilingual graph, but extracts the constraint feature after the first stage of label propagation (Eq. 1). Because many foreign word types are not aligned to an English word (see Table 3), and we do not run label propagation on the foreign side, we expect the projected information to have less coverage. Furthermore we expect the label distributions on the foreign to be fairly noisy, because the graph constraints have not been taken into account yet.\n\u2022 With LP: Our full model uses both stages of label propagation (Eq. 2) before extracting the constraint features. As a result, we are able to extract the constraint feature for all foreign word types and furthermore expect the projected tag distributions to be smoother and more stable.\nOur oracles took advantage of the labeled treebanks:\n\u2022 TB Dictionary: We extracted tagging dictionaries from the treebanks and and used them as constraint features in the feature-based HMM. Evaluation was done using the prespecified mappings.\n\u2022 Supervised: We trained the supervised model of Brants (2000) on the original treebanks and mapped the language-specific tags to the universal tags for evaluation."
            },
            {
                "heading": "6.4 Experimental Setup",
                "text": "While we tried to minimize the number of free parameters in our model, there are a few hyperparameters that need to be set. Fortunately, performance was stable across various values, and we were able to use the same hyperparameters for all languages.\nWe used C = 1.0 as the L2 regularization constant in (Eq. 10) and trained both EM and L-BFGS for 1000 iterations. When extracting the vector\ntx used to compute the constraint feature from the graph, we tried three threshold values for \u03c4 (see Eq. 7). Because we don\u2019t have a separate development set, we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3. For seven out of eight languages a threshold of 0.2 gave the best results for our final model, which indicates that for languages without any validation set, \u03c4 = 0.2 can be used. For graph propagation, the hyperparameter \u03bd was set to 2 \u00d7 10\u22126 and was not tuned. The graph was constructed using 2 million trigrams; we chose these by truncating the parallel datasets up to the number of sentence pairs that contained 2 million trigrams."
            },
            {
                "heading": "6.5 Results",
                "text": "Table 2 shows our complete set of results. As expected, the vanilla HMM trained with EM performs the worst. The feature-HMM model works better for all languages, generalizing the results achieved for English by Berg-Kirkpatrick et al. (2010). Our \u201cProjection\u201d baseline is able to benefit from the bilingual information and greatly improves upon the monolingual baselines, but falls short of the \u201cNo LP\u201d model by 2.5% on an average. The \u201cNo LP\u201d model does not outperform direct projection for German and Greek, but performs better for six out of eight languages. Overall, it gives improvements ranging from 1.1% for German to 14.7% for Italian, for an average improvement of 8.3% over the unsupervised feature-HMM model. For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.\nOur full model (\u201cWith LP\u201d) outperforms the unsupervised baselines and the \u201cNo LP\u201d setting for all\nlanguages. It falls short of the \u201cProjection\u201d baseline for German, but is statistically indistinguishable in terms of accuracy. As indicated by bolding, for seven out of eight languages the improvements of the \u201cWith LP\u201d setting are statistically significant with respect to the other models, including the \u201cNo LP\u201d setting.11 Overall, it performs 10.4% better than the hitherto state-of-the-art feature-HMM baseline, and 4.6% better than direct projection, when we macro-average the accuracy over all languages."
            },
            {
                "heading": "6.6 Discussion",
                "text": "Our full model outperforms the \u201cNo LP\u201d setting because it has better vocabulary coverage and allows the extraction of a larger set of constraint features. We tabulate this increase in Table 3. For all languages, the vocabulary sizes increase by several thousand words. Although the tag distributions of the foreign words (Eq. 6) are noisy, the results confirm that label propagation within the foreign language part of the graph adds significant quality for every language.\nFigure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags. While the first three models get three to four tags wrong, our best model gets only one word wrong and is the most accurate among the four models for this example. Examining the word fidanzato for the \u201cNo LP\u201d and \u201cWith LP\u201d models is particularly instructive. As Figure 1 shows, this word has no high-confidence alignment in the Italian-English bitext. As a result, its POS tag needs to be induced in the \u201cNo LP\u201d case, while the\n11A word level paired-t-test is significant at p < 0.01 for Danish, Greek, Italian, Portuguese, Spanish and Swedish, and p < 0.05 for Dutch.\nGold:\nsi trovava in un parco con il fidanzato Paolo F. , 27 anni , rappresentante\nEM-HMM:\nFeature-HMM:\nNo LP:\nWith LP:\nCONJ NOUN DET DET NOUN ADP DET NOUN . NOUN . NUM NOUN . NOUN PRON VERB ADP DET NOUN CONJ DET NOUN NOUN NOUN . ADP NOUN . VERB\nPRON VERB ADP DET NOUN ADP DET NOUN NOUN NOUN . NUM NOUN . NOUN\nVERB VERB ADP DET NOUN ADP DET ADJ NOUN ADJ . NUM NOUN . NOUN VERB VERB ADP DET NOUN ADP DET NOUN NOUN NOUN . NUM NOUN . NOUN\nFigure 2: Tags produced by the different models along with the reference set of tags for a part of a sentence from the Italian test set. Italicized tags denote incorrect labels.\ncorrect tag is available as a constraint feature in the \u201cWith LP\u201d case."
            },
            {
                "heading": "7 Conclusion",
                "text": "We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages. Because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs. Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data, but have translations into a resource-rich language. Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models."
            },
            {
                "heading": "Acknowledgements",
                "text": "We would like to thank Ryan McDonald for numerous discussions on this topic. We would also like to\nthank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data. Finally, we thank Kuzman Ganchev and the three anonymous reviewers for helpful suggestions and comments on earlier drafts of this paper."
            }
        ],
        "references": [
            {
                "title": "Head-transducer models for speech translation and their automatic acquisition from bilingual data",
                "author": [
                    "Hiyan Alshawi",
                    "Srinivas Bangalore",
                    "Shona Douglas."
                ],
                "venue": "Machine Translation, 15.",
                "citeRegEx": "Alshawi et al\\.,? 2000",
                "shortCiteRegEx": "Alshawi et al\\.",
                "year": 2000
            },
            {
                "title": "Maximum margin semi-supervised learning for structured variables",
                "author": [
                    "Yasemin Altun",
                    "David McAllester",
                    "Mikhail Belkin."
                ],
                "venue": "Proc. of NIPS.",
                "citeRegEx": "Altun et al\\.,? 2005",
                "shortCiteRegEx": "Altun et al\\.",
                "year": 2005
            },
            {
                "title": "Painless unsupervised learning with features",
                "author": [
                    "Taylor Berg-Kirkpatrick",
                    "Alexandre B. C\u00f4t\u00e9",
                    "John DeNero",
                    "Dan Klein."
                ],
                "venue": "Proc. of NAACL-HLT.",
                "citeRegEx": "Berg.Kirkpatrick et al\\.,? 2010",
                "shortCiteRegEx": "Berg.Kirkpatrick et al\\.",
                "year": 2010
            },
            {
                "title": "TnT - a statistical part-of-speech tagger",
                "author": [
                    "Thorsten Brants."
                ],
                "venue": "Proc. of ANLP.",
                "citeRegEx": "Brants.,? 2000",
                "shortCiteRegEx": "Brants.",
                "year": 2000
            },
            {
                "title": "The mathematics of statistical machine translation: parameter estimation",
                "author": [
                    "Peter F. Brown",
                    "Vincent J. Della Pietra",
                    "Stephen A. Della Pietra",
                    "Robert L. Mercer."
                ],
                "venue": "Computational Linguistics, 19.",
                "citeRegEx": "Brown et al\\.,? 1993",
                "shortCiteRegEx": "Brown et al\\.",
                "year": 1993
            },
            {
                "title": "CoNLL-X shared task on multilingual dependency parsing",
                "author": [
                    "Sabine Buchholz",
                    "Erwin Marsi."
                ],
                "venue": "Proc. of CoNLL.",
                "citeRegEx": "Buchholz and Marsi.,? 2006",
                "shortCiteRegEx": "Buchholz and Marsi.",
                "year": 2006
            },
            {
                "title": "Syntax: A Generative Introduction (Introducing Linguistics)",
                "author": [
                    "Andrew Carnie."
                ],
                "venue": "Blackwell Publishing.",
                "citeRegEx": "Carnie.,? 2002",
                "shortCiteRegEx": "Carnie.",
                "year": 2002
            },
            {
                "title": "Two decades of unsupervised POS induction: How far have we come? In Proc",
                "author": [
                    "Christos Christodoulopoulos",
                    "Sharon Goldwater",
                    "Mark Steedman."
                ],
                "venue": "of EMNLP.",
                "citeRegEx": "Christodoulopoulos et al\\.,? 2010",
                "shortCiteRegEx": "Christodoulopoulos et al\\.",
                "year": 2010
            },
            {
                "title": "Maximum likelihood from incomplete data via the EM algorithm",
                "author": [
                    "Arthur P. Dempster",
                    "Nan M. Laird",
                    "Donald B. Rubin."
                ],
                "venue": "Journal of the Royal Statistical Society, Series B, 39.",
                "citeRegEx": "Dempster et al\\.,? 1977",
                "shortCiteRegEx": "Dempster et al\\.",
                "year": 1977
            },
            {
                "title": "Dependency grammar induction via bitext projection constraints",
                "author": [
                    "Kuzman Ganchev",
                    "Jennifer Gillenwater",
                    "Ben Taskar."
                ],
                "venue": "Proc. of ACL-IJCNLP.",
                "citeRegEx": "Ganchev et al\\.,? 2009",
                "shortCiteRegEx": "Ganchev et al\\.",
                "year": 2009
            },
            {
                "title": "Why doesn\u2019t EM find good HMM POS-taggers? In Proc",
                "author": [
                    "Mark Johnson."
                ],
                "venue": "of EMNLP-CoNLL.",
                "citeRegEx": "Johnson.,? 2007",
                "shortCiteRegEx": "Johnson.",
                "year": 2007
            },
            {
                "title": "Europarl: A parallel corpus for statistical machine translation",
                "author": [
                    "Philipp Koehn."
                ],
                "venue": "MT Summit.",
                "citeRegEx": "Koehn.,? 2005",
                "shortCiteRegEx": "Koehn.",
                "year": 2005
            },
            {
                "title": "On the limited memory BFGS method for large scale optimization",
                "author": [
                    "Dong C. Liu",
                    "Jorge Nocedal."
                ],
                "venue": "Mathematical Programming, 45.",
                "citeRegEx": "Liu and Nocedal.,? 1989",
                "shortCiteRegEx": "Liu and Nocedal.",
                "year": 1989
            },
            {
                "title": "Building a large annotated corpus of English: the Penn treebank",
                "author": [
                    "Mitchell P. Marcus",
                    "Mary Ann Marcinkiewicz",
                    "Beatrice Santorini."
                ],
                "venue": "Computational Linguistics, 19.",
                "citeRegEx": "Marcus et al\\.,? 1993",
                "shortCiteRegEx": "Marcus et al\\.",
                "year": 1993
            },
            {
                "title": "Multilingual part-of-speech tagging: Two unsupervised approaches",
                "author": [
                    "Tahira Naseem",
                    "Benjamin Snyder",
                    "Jacob Eisenstein",
                    "Regina Barzilay."
                ],
                "venue": "JAIR, 36.",
                "citeRegEx": "Naseem et al\\.,? 2009",
                "shortCiteRegEx": "Naseem et al\\.",
                "year": 2009
            },
            {
                "title": "Using universal linguistic knowledge to guide grammar induction",
                "author": [
                    "Tahira Naseem",
                    "Harr Chen",
                    "Regina Barzilay",
                    "Mark Johnson."
                ],
                "venue": "Proc. of EMNLP.",
                "citeRegEx": "Naseem et al\\.,? 2010",
                "shortCiteRegEx": "Naseem et al\\.",
                "year": 2010
            },
            {
                "title": "Possible and Probable Languages: A Generative Perspective on Linguistic Typology",
                "author": [
                    "Frederick J. Newmeyer."
                ],
                "venue": "Oxford University Press.",
                "citeRegEx": "Newmeyer.,? 2005",
                "shortCiteRegEx": "Newmeyer.",
                "year": 2005
            },
            {
                "title": "The CoNLL 2007 shared task on dependency parsing",
                "author": [
                    "Joakim Nivre",
                    "Johan Hall",
                    "Sandra K\u00fcbler",
                    "Ryan McDonald",
                    "Jens Nilsson",
                    "Sebastian Riedel",
                    "Deniz Yuret."
                ],
                "venue": "Proceedings of CoNLL.",
                "citeRegEx": "Nivre et al\\.,? 2007",
                "shortCiteRegEx": "Nivre et al\\.",
                "year": 2007
            },
            {
                "title": "A universal part-of-speech tagset",
                "author": [
                    "Slav Petrov",
                    "Dipanjan Das",
                    "Ryan McDonald."
                ],
                "venue": "ArXiv:1104.2086.",
                "citeRegEx": "Petrov et al\\.,? 2011",
                "shortCiteRegEx": "Petrov et al\\.",
                "year": 2011
            },
            {
                "title": "Minimized models for unsupervised part-of-speech tagging",
                "author": [
                    "Sujith Ravi",
                    "Kevin Knight."
                ],
                "venue": "Proc. of ACL-IJCNLP.",
                "citeRegEx": "Ravi and Knight.,? 2009",
                "shortCiteRegEx": "Ravi and Knight.",
                "year": 2009
            },
            {
                "title": "Guided learning for bidirectional sequence classification",
                "author": [
                    "Libin Shen",
                    "Giorgio Satta",
                    "Aravind Joshi."
                ],
                "venue": "Proc. of ACL.",
                "citeRegEx": "Shen et al\\.,? 2007",
                "shortCiteRegEx": "Shen et al\\.",
                "year": 2007
            },
            {
                "title": "Unsupervised multilingual grammar induction",
                "author": [
                    "Benjamin Snyder",
                    "Tahira Naseem",
                    "Regina Barzilay."
                ],
                "venue": "Proc. of ACL-IJCNLP.",
                "citeRegEx": "Snyder et al\\.,? 2009",
                "shortCiteRegEx": "Snyder et al\\.",
                "year": 2009
            },
            {
                "title": "Efficient graph-based semi-supervised learning of structured tagging models",
                "author": [
                    "Amar Subramanya",
                    "Slav Petrov",
                    "Fernando Pereira."
                ],
                "venue": "Proc. of EMNLP.",
                "citeRegEx": "Subramanya et al\\.,? 2010",
                "shortCiteRegEx": "Subramanya et al\\.",
                "year": 2010
            },
            {
                "title": "HMM-based word alignment in statistical translation",
                "author": [
                    "Stephan Vogel",
                    "Hermann Ney",
                    "Christoph Tillmann."
                ],
                "venue": "Proc. of COLING.",
                "citeRegEx": "Vogel et al\\.,? 1996",
                "shortCiteRegEx": "Vogel et al\\.",
                "year": 1996
            },
            {
                "title": "A backoff model for bootstrapping resources for non-English languages",
                "author": [
                    "Chenhai Xi",
                    "Rebecca Hwa."
                ],
                "venue": "Proc. of HLT-EMNLP.",
                "citeRegEx": "Xi and Hwa.,? 2005",
                "shortCiteRegEx": "Xi and Hwa.",
                "year": 2005
            },
            {
                "title": "Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora",
                "author": [
                    "David Yarowsky",
                    "Grace Ngai."
                ],
                "venue": "Proc. of NAACL.",
                "citeRegEx": "Yarowsky and Ngai.,? 2001",
                "shortCiteRegEx": "Yarowsky and Ngai.",
                "year": 2001
            },
            {
                "title": "Semi-supervised learning using gaussian fields and harmonic functions",
                "author": [
                    "Xiaojin Zhu",
                    "Zoubin Ghahramani",
                    "John D. Lafferty."
                ],
                "venue": "Proc. of ICML.",
                "citeRegEx": "Zhu et al\\.,? 2003",
                "shortCiteRegEx": "Zhu et al\\.",
                "year": 2003
            }
        ],
        "abstractText": "We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (BergKirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm."
    },
    {
        "title": "A Low Power, High Throughput, Fully Event-Based Stereo System",
        "sections": [
            {
                "heading": null,
                "text": "We introduce a stereo correspondence system implemented fully on event-based digital hardware, using a fully graph-based non von-Neumann computation model, where no frames, arrays, or any other such data-structures are used. This is the first time that an end-to-end stereo pipeline from image acquisition and rectification, multi-scale spatiotemporal stereo correspondence, winner-take-all, to disparity regularization is implemented fully on event-based hardware. Using a cluster of TrueNorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by Dynamic Vision Sensors (DVS), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes. Experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of DVS sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects. System evaluation on event-based sequences demonstrates a \u223c 200 \u00d7 improvement in terms of power per pixel per disparity map compared to the closest stateof-the-art, and maximum latencies of up to 11ms from spike injection to disparity map ejection."
            },
            {
                "heading": "1. Introduction",
                "text": "Sparsity and parallel asynchronous computation are two key principles of information processing in the brain. They allow to solve complex tasks using a tiny fraction of the energy consumed by stored-program computers [64]. While the successful artificial neural networks may not operate the same way as the brain, both of them utilize highly parallel and hierarchical architectures that gradually abstract input data to more meaningful concepts [8, 51, 16]. However, event-based computation has not been equally adopted [4].\n\u2217equal contribution. \u2020Work done as an intern at IBM Research - Almaden. Cognitive Anteater Robotics Lab (CARL), University of California, Irvine\nAnother barrier for sparse computation are traditional sensors, such as frame-based cameras, which provide regular inputs. For autonomous vehicles, drones, and satellites, energy consumption is a challenge [6]. Event-based processing dramatically reduces power consumption by computing only what is new while omitting unchanged input parts.\nRecently developed event-based cameras such as Dynamic Vision Sensor (DVS) [37, 10] and ATIS [50], inspired by the biological retina, encode pixel illumination changes as events. These sensors solve two major drawbacks of frame-based cameras. First, temporal resolution of frame-based applications is limited by the camera frame rate, usually 30 frames per second. Event-based cameras can generate events at microsecond resolution. Second, consecutive frames in videos are usually highly redundant, which waste downstream data transfer, computing resources and power. Since events are sparse, event-based cameras lead to better downstream resource usage. Moreover, eventbased cameras have high dynamic range (\u223c 100 dB), which is useful for real world variations in lighting conditions.\nTo achieve the low energy and high temporal resolution benefits of event-based inputs, computations must be performed asynchronously. To benefit from sparse and asynchronous computation, neuromorphic processors have been developed [44, 24, 30, 9, 56]. These processors represent input events as spikes and process them in parallel using a large neuron population. They are stimulus-driven and the propagation delay of an event through the neuron layers is usually a few milliseconds, suitable for many real-time applications. For example, the TrueNorth neuromorphic chip [44] has been used for high throughput Convolutional neural networks (CNNs) [22], character recognition [53], optic flow [11], saliency [3], and gesture recognition [2].\nDepth perception is an important task for autonomous mobile agents to navigate in the real world. The speed and low power requirements of these applications can be effectively met using event-based sensors. Event-based stereo provides additional advantages over other depth estimation methods that increase accuracy and save energy, such as high temporal resolution, high dynamic range, and robustness to interference with other agents.\nSeveral methods have been proposed to solve event-\nbased stereo correspondence. Most global methods [40, 17, 49, 45] are derived from the Marr and Poggio cooperative stereo algorithm [42]. The algorithm assumes depth continuity and often event-based implementations are not tested with objects tilted in depth. Local methods can be parallelized and find corresponding events using either local features over a spatiotemporal window or event-to-event features [13, 58, 52, 32, 57]. However, most approaches use non-event-based hardware, such as CPU or DSP.\nWe propose a fully neuromorphic event-based stereo disparity algorithm. A live-feed version of the system running on nine TrueNorth chips is shown to calculate 400 disparity maps per second, and the ability to increase this up to 2,000 disparities per second (subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as DVS. The main advantages of the proposed method, compared to the related work [17, 49, 45, 52, 57], are simultaneous end-to-end neuromorphic disparity calculation, low power, high throughput, low latency (9-11 ms), and linear scalability to multiple neuromorphic processors for larger input sizes. Compared to frame-based computation, in the asynchronous, event-based computation supported by TrueNorth, at each time cycle, in general only neurons that have input spikes are computed, and only spike events \u201c1\u201d are communicated. When the data in a cycle is sparse, as is the case with a DVS sensor, most neurons would not compute for most of the time, resulting in low active power [44]. This processing differs from traditional architectures that use frame-buffers and other conventional data structures; where same memory fetching and computation is repeated for each pixel every frame, independent of scene activity.\nThe proposed event-based disparity method is implemented using a stereo pair of DAVIS sensors [10] (a version of DVS) and nine TrueNorth NS1e boards [53]. However, the method is applicable to other spiking neuromorphic architectures, and it is also tested offline on larger models using a TrueNorth simulator. Input rectification, spatiotemporal scaling, feature matching, search for best matches, morphological erosion and dilation, and bidirectional consistency check are all performed on TrueNorth, for a fully neuromorphic disparity solution. With respect to the most relevant state-of-the-art approach [17], our method uses \u223c 200\u00d7 less power per pixel per disparity map. We also release the event-based stereo dataset used, which includes Kinect-based registered ground-truth."
            },
            {
                "heading": "2. Related work",
                "text": "Frame-based stereo disparity methods calculate matching cost using a spatial similarity metric [25, 27, 29] or a cost function learned from a dataset (see reviews [62, 55, 34, 63]). CNNs [35] have been used to learn stereo matching cost [66, 46]. Ground truth disparity maps from benchmark frame-based datasets [27, 54, 26, 43] are used to train these\nmodels, followed by sparse-to-dense conversions [18, 5]. Feature based matching techniques, such as color, edge, histogram, and SIFT [39] based matching, produce sparse disparity maps [28, 38, 21, 61].\nIn contrast, event-based stereo correspondence literature is relatively new. Mahowald and Delbru\u0308ck [41] implemented the Marr and Poggio cooperative stereo algorithm [42], a global approach, in an analog VLSI circuit. The algorithm converges well when object surfaces are fronto-parallel and candidate matches injected to the network are close together [40, 17]. Later Mahowald [40] modified the VLSI embodied algorithm to solve tilted depth maps using a network of analog valued disparity units, which linearly interpolates the cooperative network output.\nHowever, most of the recent event-based implementations of the cooperative algorithm do not consider depth gradients [47, 48, 23, 17]. Piatkowska et al. [49] inject neighborhood similarity of candidate matches into the cooperative network. Dikov et al. [17] use six SpiNNaker [24] processor boards to implement the cooperative network for 106 \u00d7 106 pixels of stereo event data. Osswald et al. [45] propose an FPGA based implementation of spiking neurons as the nodes of the cooperative network. Xie et al. [65] employ message passing on a Markov Random Field with depth continuity for a global solution.\nLocal event-based stereo correspondence approaches are area-based or time-based. Area-based methods assume that object shapes appear identically on left and right sensors. Camun\u0303as-Mesa et al. [13, 12] propose to match edge orientations in event frames accumulated over 50 ms. Schraml et al. [60, 58] propose DSP implementation of a spatiotemporal similarity method using two live event sensors [37]. Belbachir et al. [7] use a rotating pair of event-based line (vertical) sensors in static scenes and render events from each rotation to an edge map [33], which is subsequently processed using a frame-based panoramic stereo algorithm [36].\nTime-based methods utilize event timestamps for matching. Although spike dynamics vary among pixels and sensors [52] and events cannot be matched based on exact timestamps. Rogister et al. [52, 14] propose to use event-toevent constraints for calculating matching cost, such as time window, distance to the epipolar line, ordering constraint, and polarity. Kogler et al. [32, 31] calculate similarity as the\ninverse of temporal distance and average them within each depth plane. The proposed method and its FPGA implementations [20, 19] are equivalent to the cooperative stereo algorithm [42] with noisy time difference inputs. Schraml et al. [59, 57] propose a cost function for the rotating stereo panorama setup in [7] based on temporal event difference."
            },
            {
                "heading": "3. Event-based hardware",
                "text": "Our implementation uses a pair of synchronized DAVIS240C cameras [10], connected via Ethernet to a cluster of TrueNorth NS1e boards (Fig. 1). The use of DAVIS sensors improve speed, power, dynamic range, and computational requirements. As shown in Fig. 2, fast moving objects are more challenging for frame-based cameras.\nThe IBM TrueNorth is a reconfigurable, non-von Neumann neuromorphic chip containing 1 million spiking neurons and 256 million synapses distributed across 4096 parallel, event-driven, neurosynaptic cores [44]. Cores are tiled in a 64 \u00d7 64 array, embedded in a fully asynchronous network-on-chip. The chip consumes 70mW when operating at a 1 ms computation tick and normal workloads. Depending on event dynamics and network architecture, faster tick period is possible, which we take advantage of in this work to achieve as low as 0.5 ms per tick, thus doubling the maximum throughput achievable. Each neurosynaptic core connects 256 inputs to 256 neurons using a crossbar of 256 \u00d7 256 binary synapses with a lookup table of weights for 8 bits of precision, plus a sign bit. A neuron state variable, called membrane potential, integrates synaptically weighted input events with an optional leak decay. Each neuron can generate an output event deterministically, if the membrane potential V (t) exceeds a threshold; or stochastically, with a probability that is a function of the difference between the membrane potential and its threshold [2, 15]. The membrane potential is updated at each tick t to V (t) = V (t\u2212 1) + \u2202V (t) \u2202t , followed by the application of an activation function an(V (t)) where\nan(V (t)) =\n{\n1, if V (t) \u2265 n 0, otherwise (1)\nEach neuron is assigned an initial membrane potential\nV (0). Furthermore, upon producing an event, a neuron is reset to a user-specified value. Unless specified otherwise, we assume initial membrane potentials and reset values of zero. TrueNorth programs are written in the Corelet Programming Language \u2014 a hierarchical, compositional, object-oriented language [1]."
            },
            {
                "heading": "4. Stereo correspondence on TrueNorth",
                "text": "The proposed local event-based stereo correspondence algorithm is implemented end-to-end as a neuromorphic event-based algorithm. This consists of systems of equations defining the behavior of TrueNorth neurons, encased in modules called corelets [1], and the subsequent composition of the inputs and outputs of these modules. Fig. 3 depicts the sequence of operations performed by the corelets using inputs from stereo event sensors."
            },
            {
                "heading": "4.1. Rectification",
                "text": "The stereo rectification is defined by a pair of functions L, R which map each pixel in the left and right sensor\u2019s rectified space to a pixel in the left and right sensor\u2019s native resolution respectively. On TrueNorth, this is implemented using |H| \u00b7 |W | splitter neurons per sensor & polarity channel, arranged in an |H| \u00d7 |W | retinotopic map. The events at each rectified pixel p \u2208 H \u00d7\nW \u00d7{L,R}\u00d7{+,\u2212, {+,\u2212}} are generated through splitter neurons which replicate corresponding sensor pixels. Their membrane potential V splp (t) is defined by \u2202V splp (t) \u2202t = I(t\u2212 1; p\u2032) where I(t; p\u2032) \u2192 {0, 1} denotes whether a sensor event is produced at time t and the sensor pixel p\u2032 corresponding to the rectified pixel p. a1(V spl p (t)) defines the activation of the corresponding neuron. Potentials are initialized to zero and set to also reset to zero upon spiking."
            },
            {
                "heading": "4.2. Multiscale temporal representation",
                "text": "The event rate of an event-based sensor depends on factors, such as scene contrast, sensor bias parameters, and object velocity. To add invariance across event rates, we accumulate spikes over various temporal scales through the use of temporally overlapping sliding windows. These temporal scales are implemented through the use of splitter neurons which cause each event to appear at its corresponding pixel multiple times, depending on the desired temporal scale, or through the use of temporal ring buffer mechanisms, which lead to lower event rates. The ring buffer is implemented by storing events in membrane potentials of memory cell neurons in a circular buffer, and through the use of control neurons which spike periodically to polarize appropriate memory cell neurons. Buffers can encode the input at various temporal scales. For example at a scale T = 5 the buffer denotes if an event occurred at the corresponding pixel during the last 5 ticks (logical disjunction).\nA control neuron that produces events with period T and phase \u03c6 is defined by a neuron aT (V ctrl \u03c6 ) that satisfies \u2202V ctrl\u03c6 (t)\n\u2202t = 1, V (0) = \u03c6 and resets to zero upon pro-\nducing an event. Through populations of such neurons one can also define aT (V ctrl [\u03c6,\u03b8]) corresponding to phase intervals [\u03c6, \u03b8] (where \u03b8 \u2212 \u03c6 + 1 \u2264 T ), defining periodic intervals of events. Such control neurons are used to probe (prb) or reset (rst) neuron membrane potentials. A memory cell neuron is a recurrent neuron which accepts as input either its own output (so that it does not lose its stored value whenever the neuron is queried for its stored value), input axons to set the neuron value and control axons for resetting and querying the memory cell. In more detail the output at index r \u2208 {0, ..., T +1} of a T +2 size memory cell ring-buffer at a given pixel p, is multiplexed via two copies (m \u2208 {0, 1}) and is defined as a2(V mem p,m,r) where\n\u2202V memp,m,r(t+ 1)\n\u2202t = [ \u2212 aT+2(V\nrst s\u0302+1(t))\n+([a1(V spl p (t))] r r\u0302 \u2228 [a2(V mem p,m,r(t\u2212 1))] m t\u0302 )\n+[aT+2(V prb [3\u2212r,T+2\u2212r](t))] m t\u0302\n\u2212[aT+2(V rst [2\u2212r,T+1\u2212r](t))] 1\u2212m t\u0302 ]+ (2)\nwhere probe/reset (prb/rst) control neurons are used, r\u0302 = t mod (T + 2), s\u0302 = T + 2\u2212 r mod (T + 2), t\u0302 = t mod 2,\n\u2228 is logical disjunction1,\n[x]rr\u0302 =\n{\nmax {0, x}, if r = r\u0302 0, otherwise (3)\nand [x]+ def = [x]11 defines a ReLU function. Eq. 2 defines a ring-buffer with T + 2 memory cells, where probe pulses periodically and uniformly query T of the T+2 cells for the stored memory contents at each tick, where m = 0 neurons are probed at odd ticks and m = 1 neurons are probed at even ticks. Reset pulses control when to reset one of the T + 2 memory cells to zero in preparation of a new input. Notice that new inputs (a1(V spl p (\u00b7))) are always routed to the cell r that was reset in the previous tick. The probe pulses result in the creation of an output event if during the last T ticks a1(V spl p (\u00b7)) produced an event. After a probe event, a reset event decrements the previous +1 membrane potential increase, followed by the restoring of the memory event output during the last probe (a2(V mem p,m,r(t\u2212 1)))."
            },
            {
                "heading": "4.3. Morphological erosion and dilation",
                "text": "Binary morphological erosion and dilation is optionally applied on the previous module\u2019s outputs to denoise the image. Given a 2-D neighborhood N(p) centered around each pixel p, the erosion neuron\u2019s membrane potential V ep is guided by the system of equations \u2202V ep (t) \u2202t = [1 \u2212 |N(p)| + \u2211\nq\u2208N(p)\n\u2211\nm\n\u2228\nr a2(V mem q,m,r (t \u2212 1))]+ and\nuses an a1 activation function. Similarly, dilation neurons V dp with receptive fields N(p) evolve according to \u2202V dp (t)\n\u2202t =\n\u2211\nq\u2208N(p) a1(V e q (t\u2212 1)) where a1 is also used as\nthe dilation neurons\u2019 activation function. The neuron potentials are initialized to zero and set to also reset to zero upon producing a spike. In practice 3 \u00d7 3 pixel neighborhoods are used. At each tick, erosion and dilation neurons output the minimum and maximum value respectively, of their receptive fields. Cascades of erosion and dilation neurons, are used to denoise retinotopic binary inputs (Fig. 3)."
            },
            {
                "heading": "4.4. Multiscale spatiotemporal features",
                "text": "Each feature extracted around a rectified pixel p is a concatenation of event patches, extracted from one or more spatiotemporal scales. Spatial scaling consists of spatially sub-sampling each output map of the temporal scale phase (Sec. 4.2/4.3), as specified in the corelet parameters, to apply the window matching (Sec. 4.5) on the sub-sampled data. This results in spatiotemporal coordinate tensors XL,p, XR,p defining the coordinates where events form feature vectors. The ith of these coordinates is represented by neuron activations a1(V L{+,\u2212}\nX (i) L,p\n(t)) and a1(V R{+,\u2212}\nX (i) R,p\n(t)) in\n1disjunction is implemented by sending input events to the same neuron\ninput axon, effectively merging any input events to a single input event.\nthe left and right sensor\u2019s positive (+) or negative (\u2212) polarity channel.2"
            },
            {
                "heading": "4.5. Hadamard product for matching",
                "text": "Given a pair of spatiotemporal coordinate tensors XL,p, XR,q centered at coordinates p, q in the left and right rectified image respectively and representing K coordinates each, we calculate the binary Hadamard product fL(p, t) \u25e6 fR(q, t) associated with the corresponding patches at time t, where fL(p, t) = \u220f i{a1(V L\nX (i) L,p\n(t))} \u2208 {0, 1}K and\nfR(q, t) = \u220f i{a1(V R\nX (i) R,q\n(t))} \u2208 {0, 1}K . The product is\ncalculated in parallel across multiple neurons, as K pairwise logical AND operations of corresponding feature vector entries, resulting in (a1(V dot p,q,1), ...,a1(V dot p,q,K)) where \u2202V dotp,q,i(t)\n\u2202t = [a1(V\nL\nX (i) L,p\n(t \u2212 1)) + a1(V R\nX (i) R,q\n(t \u2212 1)) \u2212 1]+\nThe population code representation of the Hadamard product output is converted to a thermometer code3, which is passed to the winner-take-all circuit described below."
            },
            {
                "heading": "4.6. Winner-Take-All system",
                "text": "The winner-take-all (WTA) system is a feed-forward neural network that takes as input D thermometer code representations of the Hadamard products for D distinct candidate disparity levels, and finds the disparity with the largest value, at every tick. For designing a scalable and compact WTA system on a neuromorphic hardware, we introduced a novel encoding technique for inputs. In a binary eventbased system, numbers can be efficiently coded using base4 representation where each digit is encoded using a 3-bits thermometer code. We denote it as Quaternary Thermometer Code (QTC). Note that a thermometer code of length 2n bits can be represented by a QTC of length 3 \u2217 \u2308n/2\u2309 bits. For example, values between 0\u2013255 are represented by a QTC of 12 bits. While it takes a few more bits than an 8 bits binary code, it allows designing a feed-forward WTA network comprising only four cascaded subnetworks, compared to eight for a binary representation, requiring fewer hardware resources as well as half the latency. Latency is further improved with larger bases, but the growth in thermometer code length for each digit results in consuming more hardware resources. Table 1 shows binary, base-4 and QTC representation of different decimal numbers.\nWe assume a maximum thermometer code length of 4B+1 \u2265 K for some B \u2208 N. Then for any \u03b1 \u2208 {0, 1, 2}, \u03b2\u2208{0, 1, ..., B}, we define the conversion of candidate disparity level d \u2208 {0, ..., D \u2212 1} to a QT-coded membrane potential V CNV\u03b1,\u03b2,d (t) as\n2for notational simplicity we henceforth drop the +,\u2212 superscripts: the left and right sensors could produce distinct event streams based on event polarity, or could merge events in a single polarity-agnostic stream. 3e.g., given a population code (1, 1, 0, 1, 0) for value 3, its thermometer code is the right-aligned juxtaposition of all events: (0, 0, 1, 1, 1).\n\u2202V CNV\u03b1,\u03b2,d (t)\n\u2202t = [\n\u2211\ni\u2208U(\u03b2)\nvid(t\u22121)\u2212 \u2211\ni\u2208U(\u03b2+1)\n4 vid(t\u22121)\u2212\u03b1]+\n(4) where vid(t) is the i-th element of the input thermometer code4 for dth disparity level at time t and U(\u03b2) = {n \u2208 N : n \u2261 0 (mod 4\u03b2), 1\u2264 n< 4B+1}. All the conversion neurons use an a1 activation function and reset to 0 membrane potential upon spiking. Notice that (a1(V CNV 2,\u03b2,d (t)), a1(V CNV 1,\u03b2,d (t)), a1(V CNV 0,\u03b2,d (t))) is a length-3 thermometer code representation of a value in {0, 1, 2, 3}, representing the \u03b2th digit in the base-4 representation of vd(t\u22121).\nFor a set of QT-coded inputs, the WTA system is realized by a cascade of (B+1) feed-forward pruning networks where each of the pruning networks process only 3-bits of the QT codes and prune the inputs not equal to the bit-wise maximum of corresonding 3-bits thermometer codes from all inputs. Now starting from the most significant bits, all the inputs smaller than the maximum will be pruned at different stages and only the winner(s) will survive at the output of the last cascade network. The membrane potential V WTA\u03b2,d of stage \u03b2 and disparity index d is given by,\n\u2202V WTA\u03b2,d (t)\n\u2202t = [4 \u00b7W\u03b2,d(t\u22121)+\n2 \u2211\n\u03b1=0\n[a1(V CNV \u03b1,B\u2212\u03b2,d(t\u2212\u03b2))\n\u2212 max d\u0304\u2208{d\u2032|W\u03b2,d\u2032 (t\u22121)>0}\n{a1(V CNV \u03b1,B\u2212\u03b2,d\u0304(t\u2212\u03b2))}]\u2212 3]+, (5)\nwhere W\u03b2,d(t)=\n{\na1(V WTA \u03b2\u22121,d (t)), \u2200\u03b2> 0, 1, if \u03b2 = 0 (6)\nNote that the function W\u03b2,d(t) represents the candidate status of the d-th input at the end of \u03b2-th stage. Initially all the\n4the i variable indexing (vi d ) starts from the right of the thermometer\ncode vd of (a1(V dot p,q,1), ...,a1(V dot p,q,K )) \u2208 {0, 1}K . The dependence of vd and d on pixels p, q is implicit and is not shown to simplify notation.\ninputs are winning candidates (W0,d(t) = 1) and the status changes after the input is pruned at any stage indicating it is out of the competition and the selection process continues with remaining candidates. As an illustration, winner is computed from the example set of numbers in Table 1 and the winner selection process is shown in Table 2."
            },
            {
                "heading": "4.7. Bidirectional consistency check",
                "text": "A left-right consistency check is then performed to verify that for each left-rectified pixel p matched to right-rectified pixel q, it is also the case that right-rectified pixel q gets matched to left-rectified pixel p. This is achieved using two parallel WTA streams. Stream 1 calculates the winner disparities for left-to-right matching, and stream 2 calculates the winner disparities of right-to-left matching. The outputs of each stream are represented by D retinotopic maps expressed in a fixed resolution (Dvi,j,d(t), d \u2208 {0, ..., D \u2212 1}, v \u2208 {L,R}), where events represent the retinotopic winner disparities for that stream. The streams are then merged to produce the disparity map D L,R i,j,d(t) = a1(V L,R i,j,d (t)) where\n\u2202V L,Ri,j,d (t)\n\u2202t = [DLi,j,d(t\u2212 1) +D R i,j\u2212d,d(t\u2212 1)+\na1(V spl\n(i,j,L,\u00b7)(t\u2212 t\u0302))\u2212 2]+ (7)\nwhere t\u0302 is the propagation delay of the first layer splitter output events until the left-right consistency constraint merging takes place. This enforces that an output disparity\nis produced at time-stamp t and pixel (i, j) only for leftrectified pixel (i, j), where an event was produced at t\u2212 t\u0302."
            },
            {
                "heading": "5. Experiments",
                "text": ""
            },
            {
                "heading": "5.1. Datasets",
                "text": "We evaluate the performance of the system on sequences of random dot stereograms (RDS) representing a rotating synthetic 3D object (Fig. 4a-f), and two real world sets of sequences, consisting of a fast rotating fan (Fig. 4g-m) and a rotating toy butterfly (Fig. 4n-u) captured using the DAVIS stereo cameras. The synthetic dataset provides dense disparity estimates, which are difficult to acquire with the sparse event based cameras. The dataset is generated by assigning to each left sensor pixel a random event with a 50% probability per polarity. Similarly, each right sensor pixel is assigned a value by projecting it to the 3D scene and reprojecting the corresponding data-point to the left camera coordinate frame to find the closest pixel value. Self-occluded pixels are assigned random values.\nFor the non-synthetic datasets, a Kinect [67] is used to extract ground truth of the scene structure. This also entails a calibration process for transforming the undistorted Kinect coordinate frame to the undistorted DAVIS sensor coordinate frame. The fan sequence is useful for testing the ability of the algorithm to operate on rapidly moving objects. Varying orientations of the revolving fan add continuously varying depth gradient to the dataset. Ground truth\nis extracted in terms of the plane in 3D space representing the blades\u2019 plane of rotation (Fig. 4m). The butterfly sequence tests the ability of the algorithm to operate on nonrigid objects which are rapidly rotating in a circular plane approximately perpendicular to the y-axis. Ground truth is extracted in terms of the coordinates of the circle spanned by the rotating butterfly (Fig. 4p). Nine Fan sequences (3 distances \u00d7 3 orientations) and three Butterfly sequences (3 distances) are used. The dataset, with Kinect ground-truth, is at: http://ibm.biz/StereoEventData."
            },
            {
                "heading": "5.2. Evaluation",
                "text": "On the synthetic dataset we measure the average absolute disparity error, and the average recall, which is defined as the fraction of pixels where a disparity measurement was found. On the non-synthetic data, performance is measured in terms of precision, which is defined as the median relative error \u2016x\u2212x\u2032\u2016 \u2016x\u2032\u2016 between each 3D coordinate x extracted in the DAVIS frame using the neuromorphic algorithm, and the corresponding ground coordinate x\u2032 in the aligned Kinect coordinate frame. Performance is also reported in terms of the recall, defined herein as the percentage of DAVIS pixels containing events, where a disparity estimate was also extracted. We tested a suite of sixty stereo disparity networks generated with ranges of spatiotemporal scales, denoising parameters, kernel match thresholds, with/without left-right consistency constraints etc."
            },
            {
                "heading": "5.3. Power measurement",
                "text": "Power is measured using the same process described in [2]. We calculate the power consumed by an n-chip system by measuring power on a single TrueNorth chip model running on an NS1t board with a high event rate input generated by the fan sequence. This board has circuitry to measure the power consumed by a TrueNorth chip. We multiply the power value by n to extrapolate the power consumed by an n-chip system. Measurements are reported at supply\nvoltages of 0.8V, 1.0V. Total chip power is the sum of passive power, computed by multiplying the idle power by the fraction of the chip\u2019s cores under use, and active power \u2014 computed by subtracting idle power from the total power measured when the system is accepting input events ."
            },
            {
                "heading": "5.4. Results",
                "text": "The RDS is tested on a model using 3 \u00d7 5 spatial windows, left-right consistency constraints, no morphological erosion/dilation after rectification, and 31 disparity levels (0-30) plus a \u2018no-disparity\u2019 indicator (often occurring due to self-occlusions). We also experiment with a postprocessing phase with erosion and dilation applied to output disparity maps in order to better regularize the output. Average disparity error and recall before regularization is 0.19/0.66 and post-regularization is 0.04/0.63. We observe major improvements due to the regularization, often occurring in self-occluded regions. Errors increase in slanted regions due to foreshortening effects. The left-right consistency constraint decreases false predictions in those regions.\nThe evaluation on the non-synthetic dataset was done under the practical constraints of the availability of a limited number of NS1e boards on which non-simulated models could be run, as well as the need to process the full DAVIS inputs at as high of a throughput as possible. The models that run on live DAVIS input are operated at spike injection rate of up to 2,000Hz (a new input every 1/2,000 seconds) and disparity map throughput of 400Hz at a 0.5ms tick period (400 distinct disparity maps produced every second) across a cluster of 9 TrueNorth chips. Single chip passive/active power on a characteristic model and input is 34.4mW/35.8mW (0.8V) and 82.1mW/56.4mW (1.0V).\nRunning a model at the full 2,000Hz throughput comes at the expense of an increased neuron count. By adding a multiplexing spiking network to the network, we are able to reuse each feature-extraction/WTA circuit to process the disparities for 5 different pixels, effectively decreasing the maximum disparity map throughput from 2,000Hz to 400Hz, requiring fewer chips to process the full image (9 TrueNorth chips). We tested the maximum disparity map throughput achievable, by executing a one-chip model on a cropped input, with no multiplexing (one disparity map ejected per tick) at a 0.5ms tick period, achieving the 2,000Hz disparity map throughput. We tested sixty models on the TrueNorth simulator which provides a spike-forspike equivalent behavior to the chip. We achieved best relative errors of 5 \u2212 11.6% and 7.3 \u2212 8% on the Fan and Butterfly sequence respectively (Fig. 5). We also observe qualitatively good performance (Fig. 6). It is observed that the temporal scale has a higher effect on accuracy than spatial scale. Left-right consistency constraints are typically present in the best performing fan-sequence models, but not so in the Butterfly sequences. Distance and orientation do not have a significant effect on performance. See supplementary materials for more details."
            },
            {
                "heading": "6. Discussion",
                "text": "We have introduced an advanced neuromorphic 3D vision system uniting a pair of DAVIS cameras with multiple TrueNorth processors, to create an end-to-end, scalable, event-based stereo system. By using a spiking neural network, with low-precision weights, we have shown that the system is capable of injecting event streams and ejecting disparity maps at high throughputs, low latencies, and low power. The system is highly parameterized and can operate with other event based sensors such as ATIS [50] or DVS [37]. Table 3 compares our approach with the literature on event based disparity. Comparative advantages are low power, multi-resolution disparity calculation, scalability to live sensor feed with large input sizes, and evaluation using synthetic as well as real world fast movements and depth gradients, in neuromorphic, non von-Neumann hardware. The implemented neuromorphic stereo disparity system achieves these advantages, while consuming \u223c 200\u00d7 less power per pixel per disparity map compared to the stateof-the-art [17]. The homogeneous computational substrate provides the first example of a fully end-to-end low-power, high throughput fully event-based neuromorphic stereo system capable of running on live input event streams, using a fully graph-based computation model, where no frames, arrays or other such data-structures are used."
            }
        ],
        "references": [
            {
                "title": "et al",
                "author": [
                    "A. Amir",
                    "P. Datta",
                    "W.P. Risk",
                    "A.S. Cassidy",
                    "J.A. Kusnitz",
                    "S.K. Esser",
                    "A. Andreopoulos",
                    "T.M. Wong",
                    "M. Flickner",
                    "R. Alvarez-Icaza"
                ],
                "venue": "Cognitive computing programming paradigm: a corelet language for composing networks of neurosynaptic cores. In International Joint Conference on Neural Networks (IJCNN)",
                "citeRegEx": "1",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "et al",
                "author": [
                    "A. Amir",
                    "B. Taba",
                    "D. Berg",
                    "T. Melano",
                    "J. McKinstry",
                    "C. Di Nolfo",
                    "T. Nayak",
                    "A. Andreopoulos",
                    "G. Garreau",
                    "M. Mendoza"
                ],
                "venue": "A low power, fully event-based gesture recognition system. In IEEE Conference on Computer Vision and Pattern Recognition",
                "citeRegEx": "2",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "et al",
                "author": [
                    "A. Andreopoulos",
                    "B. Taba",
                    "A.S. Cassidy",
                    "R. Alvarez-Icaza",
                    "M. Flickner",
                    "W.P. Risk",
                    "A. Amir",
                    "P. Merolla",
                    "J.V. Arthur",
                    "D.J. Berg"
                ],
                "venue": "Visual saliency on networks of neurosynaptic cores. IBM Journal of Research and Development, 59(2/3):9\u20131",
                "citeRegEx": "3",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "50 years of object recognition: Directions forward",
                "author": [
                    "A. Andreopoulos",
                    "J.K. Tsotsos"
                ],
                "venue": "Computer Vision and Image Understanding, 117(8):827\u2013891",
                "citeRegEx": "4",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "The fast bilateral solver",
                "author": [
                    "J.T. Barron",
                    "B. Poole"
                ],
                "venue": "European Conference on Computer Vision",
                "citeRegEx": "5",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Autonomous vehicle technologies for small fixedwing uavs",
                "author": [
                    "R.W. Beard",
                    "D.B. Kingston",
                    "M. Quigley",
                    "D. Snyder",
                    "R. Christiansen",
                    "W. Johnson",
                    "T.W. McLain",
                    "M.A. Goodrich"
                ],
                "venue": "JACIC, 2(1):92\u2013108",
                "citeRegEx": "6",
                "shortCiteRegEx": null,
                "year": 2005
            },
            {
                "title": "A novel hdr depth camera for real-time 3d 360 panoramic vision",
                "author": [
                    "A.N. Belbachir",
                    "S. Schraml",
                    "M. Mayerhofer",
                    "M. Hofst\u00e4tter"
                ],
                "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), pages 425\u2013432. IEEE",
                "citeRegEx": "7",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Representation learning: A review and new perspectives",
                "author": [
                    "Y. Bengio",
                    "A. Courville",
                    "P. Vincent"
                ],
                "venue": "IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u2013 1828",
                "citeRegEx": "8",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "Neurogrid: A mixedanalog-digital multichip system for large-scale neural simulations",
                "author": [
                    "B.V. Benjamin",
                    "P. Gao",
                    "E. McQuinn",
                    "S. Choudhary",
                    "A.R. Chandrasekaran",
                    "J.-M. Bussat",
                    "R. Alvarez-Icaza",
                    "J.V. Arthur",
                    "P.A. Merolla",
                    "K. Boahen"
                ],
                "venue": "Proceedings of the IEEE, 102(5):699\u2013716",
                "citeRegEx": "9",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "A 240\u00d7 180 130 db 3 \u03bcs latency global shutter spatiotemporal vision sensor",
                "author": [
                    "C. Brandli",
                    "R. Berner",
                    "M. Yang",
                    "S.-C. Liu",
                    "T. Delbruck"
                ],
                "venue": "IEEE Journal of Solid-State Circuits, 49(10):2333\u20132341",
                "citeRegEx": "10",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Event-based optical flow on neuromorphic hardware",
                "author": [
                    "T. Brosch",
                    "H. Neumann"
                ],
                "venue": "proceedings of the 9th EAI International Conference on Bio-inspired Information and Communications Technologies (formerly BIONETICS) on 9th EAI International Conference on Bio-inspired Information and Communications Technologies (formerly BIONET- ICS), pages 551\u2013558. ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)",
                "citeRegEx": "11",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Event-driven stereo vision with orientation filters",
                "author": [
                    "L. Camunas-Mesa",
                    "T. Serrano-Gotarredona",
                    "B. Linares- Barranco",
                    "S. Ieng",
                    "R. Benosman"
                ],
                "venue": "IEEE International Symposium on Circuits and Systems (ISCAS), pages 257\u2013260",
                "citeRegEx": "12",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "On the use of orientation filters for 3d reconstruction in event-driven stereo vision",
                "author": [
                    "L.A. Camu\u00f1as-Mesa",
                    "T. Serrano-Gotarredona",
                    "S.H. Ieng",
                    "R.B. Benosman",
                    "B. Linares-Barranco"
                ],
                "venue": "Frontiers in neuroscience, 8",
                "citeRegEx": "13",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Eventbased 3d reconstruction from neuromorphic retinas",
                "author": [
                    "J. Carneiro",
                    "S.-H. Ieng",
                    "C. Posch",
                    "R. Benosman"
                ],
                "venue": "Neural Networks, 45:27\u201338",
                "citeRegEx": "14",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "et al",
                "author": [
                    "A.S. Cassidy",
                    "P. Merolla",
                    "J.V. Arthur",
                    "S.K. Esser",
                    "B. Jackson",
                    "R. Alvarez-Icaza",
                    "P. Datta",
                    "J. Sawada",
                    "T.M. Wong",
                    "V. Feldman"
                ],
                "venue": "Cognitive computing building block: A versatile and efficient digital neuron model for neurosynaptic cores. In International Joint Conference on Neural Networks (IJCNN)",
                "citeRegEx": "15",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "How does the brain solve visual object recognition? Neuron",
                "author": [
                    "J.J. DiCarlo",
                    "D. Zoccolan",
                    "N.C. Rust"
                ],
                "venue": "73(3):415\u2013 434",
                "citeRegEx": "16",
                "shortCiteRegEx": null,
                "year": 2012
            },
            {
                "title": "Spiking cooperative stereo-matching at 2 ms latency with neuromorphic hardware",
                "author": [
                    "G. Dikov",
                    "M. Firouzi",
                    "F. R\u00f6hrbein",
                    "J. Conradt",
                    "C. Richter"
                ],
                "venue": "Conference on Biomimetic and Biohybrid Systems, pages 119\u2013137. Springer",
                "citeRegEx": "17",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Sparse stereo disparity map densification using hierarchical image segmentation",
                "author": [
                    "S. Drouyer",
                    "S. Beucher",
                    "M. Bilodeau",
                    "M. Moreaud",
                    "L. Sorbier"
                ],
                "venue": "International Symposium on Mathematical Morphology and Its Applications to Signal and Image Processing, pages 172\u2013184. Springer",
                "citeRegEx": "18",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Event-driven stereo vision algorithm based on silicon retina sensors",
                "author": [
                    "F. Eibensteiner",
                    "H.G. Brachtendorf",
                    "J. Scharinger"
                ],
                "venue": "Radioelektronika (RADIOELEKTRONIKA)",
                "citeRegEx": "19",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "A highperformance hardware architecture for a frameless stereo vision algorithm implemented on a fpga platform",
                "author": [
                    "F. Eibensteiner",
                    "J. Kogler",
                    "J. Scharinger"
                ],
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 623\u2013630",
                "citeRegEx": "20",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "High-speed segmentation-driven high-resolution matching",
                "author": [
                    "F. Ekstrand",
                    "C. Ahlberg",
                    "M. Ekstr\u00f6m",
                    "G. Spampinato"
                ],
                "venue": "Seventh International Conference on Machine Vision",
                "citeRegEx": "21",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "et al",
                "author": [
                    "S.K. Esser",
                    "P.A. Merolla",
                    "J.V. Arthur",
                    "A.S. Cassidy",
                    "R. Appuswamy",
                    "A. Andreopoulos",
                    "D.J. Berg",
                    "J.L. McKinstry",
                    "T. Melano",
                    "D.R. Barch"
                ],
                "venue": "Convolutional networks for fast, energy-efficient neuromorphic computing. Proceedings of the National Academy of Sciences",
                "citeRegEx": "22",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Asynchronous event-based cooperative stereo matching using neuromorphic silicon retinas",
                "author": [
                    "M. Firouzi",
                    "J. Conradt"
                ],
                "venue": "Neural Processing Letters, 43(2):311\u2013326",
                "citeRegEx": "23",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Overview of the spinnaker system architecture",
                "author": [
                    "S.B. Furber",
                    "D.R. Lester",
                    "L.A. Plana",
                    "J.D. Garside",
                    "E. Painkras",
                    "S. Temple",
                    "A.D. Brown"
                ],
                "venue": "IEEE Transactions on Computers, 62(12):2454\u20132467",
                "citeRegEx": "24",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "Relaxing symmetric multiple windows stereo using markov random fields",
                "author": [
                    "A. Fusiello",
                    "U. Castellani",
                    "V. Murino"
                ],
                "venue": "EMMCVPR, pages 91\u2013104. Springer",
                "citeRegEx": "25",
                "shortCiteRegEx": null,
                "year": 2001
            },
            {
                "title": "Are we ready for autonomous driving? the kitti vision benchmark suite",
                "author": [
                    "A. Geiger",
                    "P. Lenz",
                    "R. Urtasun"
                ],
                "venue": "Computer Vision and Pattern Recognition (CVPR)",
                "citeRegEx": "26",
                "shortCiteRegEx": null,
                "year": 2012
            },
            {
                "title": "Evaluation of cost functions for stereo matching",
                "author": [
                    "H. Hirschmuller",
                    "D. Scharstein"
                ],
                "venue": "Computer Vision and Pattern Recognition (CVPR)",
                "citeRegEx": "27",
                "shortCiteRegEx": null,
                "year": 2007
            },
            {
                "title": "A region and feature-based matching algorithm for dynamic object recognition",
                "author": [
                    "H. Huang",
                    "Q. Wang"
                ],
                "venue": "IEEE International Conference on Intelligent Computing and Intelligent Systems (ICIS)",
                "citeRegEx": "28",
                "shortCiteRegEx": null,
                "year": 2010
            },
            {
                "title": "A fast stereo matching algorithm suitable for embedded real-time systems",
                "author": [
                    "M. Humenberger",
                    "C. Zinner",
                    "M. Weber",
                    "W. Kubinger",
                    "M. Vincze"
                ],
                "venue": "Computer Vision and Image Understanding, 114(11):1180\u20131202",
                "citeRegEx": "29",
                "shortCiteRegEx": null,
                "year": 2010
            },
            {
                "title": "A vlsi array of low-power spiking neurons and bistable synapses with spiketiming dependent plasticity",
                "author": [
                    "G. Indiveri",
                    "E. Chicca",
                    "R. Douglas"
                ],
                "venue": "IEEE transactions on neural networks, 17(1):211\u2013221",
                "citeRegEx": "30",
                "shortCiteRegEx": null,
                "year": 2006
            },
            {
                "title": "Eventbased stereo matching approaches for frameless address event stereo data",
                "author": [
                    "J. Kogler",
                    "M. Humenberger",
                    "C. Sulzbachner"
                ],
                "venue": "Advances in Visual Computing, pages 674\u2013 685",
                "citeRegEx": "31",
                "shortCiteRegEx": null,
                "year": 2011
            },
            {
                "title": "Address-event based stereo vision with bio-inspired silicon retina imagers",
                "author": [
                    "J. Kogler",
                    "C. Sulzbachner",
                    "M. Humenberger",
                    "F. Eibensteiner"
                ],
                "venue": "Advances in theory and applications of stereo vision. InTech",
                "citeRegEx": "32",
                "shortCiteRegEx": null,
                "year": 2011
            },
            {
                "title": "Bio-inspired stereo vision system with silicon retina imagers",
                "author": [
                    "J. Kogler",
                    "C. Sulzbachner",
                    "W. Kubinger"
                ],
                "venue": "Computer Vision Systems, pages 174\u2013183",
                "citeRegEx": "33",
                "shortCiteRegEx": null,
                "year": 2009
            },
            {
                "title": "Review of stereo vision algorithms: from software to hardware",
                "author": [
                    "N. Lazaros",
                    "G.C. Sirakoulis",
                    "A. Gasteratos"
                ],
                "venue": "International Journal of Optomechatronics, 2(4):435\u2013462",
                "citeRegEx": "34",
                "shortCiteRegEx": null,
                "year": 2008
            },
            {
                "title": "Gradientbased learning applied to document recognition",
                "author": [
                    "Y. LeCun",
                    "L. Bottou",
                    "Y. Bengio",
                    "P. Haffner"
                ],
                "venue": "Proceedings of the IEEE, 86(11):2278\u20132324",
                "citeRegEx": "35",
                "shortCiteRegEx": null,
                "year": 1998
            },
            {
                "title": "Stereo reconstruction from multiperspective panoramas",
                "author": [
                    "Y. Li",
                    "H.-Y. Shum",
                    "C.-K. Tang",
                    "R. Szeliski"
                ],
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(1):45\u201362",
                "citeRegEx": "36",
                "shortCiteRegEx": null,
                "year": 2004
            },
            {
                "title": "A 128 x 128 120db 30mw asynchronous vision sensor that responds to relative intensity change",
                "author": [
                    "P. Lichtsteiner",
                    "C. Posch",
                    "T. Delbruck"
                ],
                "venue": "IEEE International Solid-State Circuits Conference",
                "citeRegEx": "37",
                "shortCiteRegEx": null,
                "year": 2006
            },
            {
                "title": "Efficient stereo matching algorithm with edge-detecting",
                "author": [
                    "J. Liu",
                    "X. Sang",
                    "C. Jia",
                    "N. Guo",
                    "Y. Liu",
                    "G. Shi"
                ],
                "venue": "Optoelectronic Imaging and Multimedia Technology III",
                "citeRegEx": "38",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Distinctive image features from scaleinvariant keypoints",
                "author": [
                    "D.G. Lowe"
                ],
                "venue": "International journal of computer vision, 60(2):91\u2013110",
                "citeRegEx": "39",
                "shortCiteRegEx": null,
                "year": 2004
            },
            {
                "title": "VLSI analogs of neuronal visual processing: a synthesis of form and function",
                "author": [
                    "M. Mahowald"
                ],
                "venue": "PhD thesis, California Institute of Technology",
                "citeRegEx": "40",
                "shortCiteRegEx": null,
                "year": 1992
            },
            {
                "title": "Cooperative stereo matching using static and dynamic image features",
                "author": [
                    "M. Mahowald",
                    "T. Delbr\u00fcck"
                ],
                "venue": "Analog VLSI implementation of neural systems, 80:213\u2013238",
                "citeRegEx": "41",
                "shortCiteRegEx": null,
                "year": 1989
            },
            {
                "title": "et al",
                "author": [
                    "D. Marr",
                    "T. Poggio"
                ],
                "venue": "Cooperative computation of stereo disparity. From the Retina to the Neocortex, pages 239\u2013243",
                "citeRegEx": "42",
                "shortCiteRegEx": null,
                "year": 1976
            },
            {
                "title": "Object scene flow for autonomous vehicles",
                "author": [
                    "M. Menze",
                    "A. Geiger"
                ],
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3061\u20133070",
                "citeRegEx": "43",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "et al",
                "author": [
                    "P.A. Merolla",
                    "J.V. Arthur",
                    "R. Alvarez-Icaza",
                    "A.S. Cassidy",
                    "J. Sawada",
                    "F. Akopyan",
                    "B.L. Jackson",
                    "N. Imam",
                    "C. Guo",
                    "Y. Nakamura"
                ],
                "venue": "A million spiking-neuron integrated circuit with a scalable communication network and interface. Science, 345(6197):668\u2013673",
                "citeRegEx": "44",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "A spiking neural network model of 3d perception for eventbased neuromorphic stereo vision systems",
                "author": [
                    "M. Osswald",
                    "S.-H. Ieng",
                    "R. Benosman",
                    "G. Indiveri"
                ],
                "venue": "Scientific reports",
                "citeRegEx": "45",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Look wider to match image patches with convolutional neural networks",
                "author": [
                    "H. Park",
                    "K.M. Lee"
                ],
                "venue": "IEEE Signal Processing Letters",
                "citeRegEx": "46",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Asynchronous stereo vision for event-driven dynamic stereo sensor using an adaptive cooperative approach",
                "author": [
                    "E. Piatkowska",
                    "A. Belbachir",
                    "M. Gelautz"
                ],
                "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 45\u201350",
                "citeRegEx": "47",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "Cooperative and asynchronous stereo vision for dynamic vision sensors",
                "author": [
                    "E. Piatkowska",
                    "A.N. Belbachir",
                    "M. Gelautz"
                ],
                "venue": "Measurement Science and Technology",
                "citeRegEx": "48",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Improved cooperative stereo matching for dynamic vision sensors with ground truth evaluation",
                "author": [
                    "E. Piatkowska",
                    "J. Kogler",
                    "N. Belbachir",
                    "M. Gelautz"
                ],
                "venue": "IEEE Computer Vision and Pattern Recognition Workshops (CVPRW)",
                "citeRegEx": "49",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "A qvga 143 db dynamic range frame-free pwm image sensor with lossless pixel-level video compression and time-domain cds",
                "author": [
                    "C. Posch",
                    "D. Matolin",
                    "R. Wohlgenannt"
                ],
                "venue": "IEEE Journal of Solid-State Circuits, 46(1):259\u2013275",
                "citeRegEx": "50",
                "shortCiteRegEx": null,
                "year": 2011
            },
            {
                "title": "Hierarchical models of object recognition in cortex",
                "author": [
                    "M. Riesenhuber",
                    "T. Poggio"
                ],
                "venue": "Nature neuroscience, 2(11):1019\u2013 1025",
                "citeRegEx": "51",
                "shortCiteRegEx": null,
                "year": 1999
            },
            {
                "title": "Asynchronous event-based binocular stereo matching",
                "author": [
                    "P. Rogister",
                    "R. Benosman",
                    "S.-H. Ieng",
                    "P. Lichtsteiner",
                    "T. Delbruck"
                ],
                "venue": "IEEE Transactions on Neural Networks and Learning Systems, 23(2):347\u2013353",
                "citeRegEx": "52",
                "shortCiteRegEx": null,
                "year": 2012
            },
            {
                "title": "et al",
                "author": [
                    "J. Sawada",
                    "F. Akopyan",
                    "A.S. Cassidy",
                    "B. Taba",
                    "M.V. Debole",
                    "P. Datta",
                    "R. Alvarez-Icaza",
                    "A. Amir",
                    "J.V. Arthur",
                    "A. Andreopoulos"
                ],
                "venue": "Truenorth ecosystem for brain-inspired computing: scalable systems, software, and applications. In High Performance Computing, Networking, Storage and Analysis, SC16: International Conference for, pages 130\u2013 141. IEEE",
                "citeRegEx": "53",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "High-resolution stereo datasets with subpixel-accurate ground truth",
                "author": [
                    "D. Scharstein",
                    "H. Hirschm\u00fcller",
                    "Y. Kitajima",
                    "G. Krathwohl",
                    "N. Ne\u0161i\u0107",
                    "X. Wang",
                    "P. Westling"
                ],
                "venue": "German Conference on Pattern Recognition, pages 31\u201342. Springer",
                "citeRegEx": "54",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms",
                "author": [
                    "D. Scharstein",
                    "R. Szeliski"
                ],
                "venue": "International journal of computer vision, 47(1-3):7\u201342",
                "citeRegEx": "55",
                "shortCiteRegEx": null,
                "year": 2002
            },
            {
                "title": "A wafer-scale neuromorphic hardware system for large-scale neural modeling",
                "author": [
                    "J. Schemmel",
                    "D. Briiderle",
                    "A. Griibl",
                    "M. Hock",
                    "K. Meier",
                    "S. Millner"
                ],
                "venue": "IEEE International Symposium on Circuits and systems (ISCAS)",
                "citeRegEx": "56",
                "shortCiteRegEx": null,
                "year": 2010
            },
            {
                "title": "An eventdriven stereo system for real-time 3-d 360 panoramic vision",
                "author": [
                    "S. Schraml",
                    "A.N. Belbachir",
                    "H. Bischof"
                ],
                "venue": "IEEE Transactions on Industrial Electronics, 63(1):418\u2013 428",
                "citeRegEx": "57",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Dynamic stereo vision system for real-time tracking",
                "author": [
                    "S. Schraml",
                    "A.N. Belbachir",
                    "N. Milosevic",
                    "P. Sch\u00f6n"
                ],
                "venue": "IEEE International Symposium on Circuits and Systems (ISCAS)",
                "citeRegEx": "58",
                "shortCiteRegEx": null,
                "year": 2010
            },
            {
                "title": "Eventdriven stereo matching for real-time 3d panoramic vision",
                "author": [
                    "S. Schraml",
                    "A. Nabil Belbachir",
                    "H. Bischof"
                ],
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 466\u2013474",
                "citeRegEx": "59",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Smartcam for realtime stereo vision-address-event based embedded system",
                "author": [
                    "S. Schraml",
                    "P. Sch\u00f6n",
                    "N. Milosevic"
                ],
                "venue": "VISAPP (2), pages 466\u2013471",
                "citeRegEx": "60",
                "shortCiteRegEx": null,
                "year": 2007
            },
            {
                "title": "Vision based autonomous vehicle navigation with self-organizing map feature matching technique",
                "author": [
                    "K. Sharma",
                    "K.-y. Jeong",
                    "S.-G. Kim"
                ],
                "venue": "In International Conference on Control, Automation and Systems (ICCAS),",
                "citeRegEx": "61",
                "shortCiteRegEx": "61",
                "year": 2011
            },
            {
                "title": "Review of stereo vision algorithms and their suitability for resource-limited systems",
                "author": [
                    "B. Tippetts",
                    "D.J. Lee",
                    "K. Lillywhite",
                    "J. Archibald"
                ],
                "venue": "Journal of Real-Time Image Processing, 11(1):5\u201325",
                "citeRegEx": "62",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Evaluation of stereo algorithms for 3d object recognition",
                "author": [
                    "F. Tombari",
                    "F. Gori"
                ],
                "venue": "IEEE International Conference on Computer Vision Workshops (ICCV Workshops)",
                "citeRegEx": "63",
                "shortCiteRegEx": null,
                "year": 2011
            },
            {
                "title": "The computer and the brain",
                "author": [
                    "J. Von Neumann"
                ],
                "venue": "Yale University Press",
                "citeRegEx": "64",
                "shortCiteRegEx": null,
                "year": 2012
            },
            {
                "title": "Event-based stereo depth estimation using belief propagation",
                "author": [
                    "Z. Xie",
                    "S. Chen",
                    "G. Orchard"
                ],
                "venue": "Frontiers in Neuroscience, 11:535",
                "citeRegEx": "65",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Stereo matching by training a convolutional neural network to compare image patches",
                "author": [
                    "J. Zbontar",
                    "Y. LeCun"
                ],
                "venue": "Journal of Machine Learning Research, 17(1-32):2",
                "citeRegEx": "66",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Microsoft kinect sensor and its effect",
                "author": [
                    "Z. Zhang"
                ],
                "venue": "IEEE multimedia, 19(2):4\u201310",
                "citeRegEx": "67",
                "shortCiteRegEx": null,
                "year": 2012
            }
        ],
        "abstractText": "We introduce a stereo correspondence system implemented fully on event-based digital hardware, using a fully graph-based non von-Neumann computation model, where no frames, arrays, or any other such data-structures are used. This is the first time that an end-to-end stereo pipeline from image acquisition and rectification, multi-scale spatiotemporal stereo correspondence, winner-take-all, to disparity regularization is implemented fully on event-based hardware. Using a cluster of TrueNorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by Dynamic Vision Sensors (DVS), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes. Experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of DVS sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects. System evaluation on event-based sequences demonstrates a \u223c 200 \u00d7 improvement in terms of power per pixel per disparity map compared to the closest stateof-the-art, and maximum latencies of up to 11ms from spike injection to disparity map ejection."
    },
    {
        "title": "High quality, lightweight and adaptable TTS using LPCNet",
        "sections": [
            {
                "heading": null,
                "text": "The modular setup of the system allows for simple adaptation to new voices with a small amount of data.\nWe first demonstrate the ability of the system to produce high quality speech when trained on large, high quality datasets. Following that, we demonstrate its adaptability by mimicking unseen voices using 5 to 20 minutes long datasets with lower recording quality. Large scale Mean Opinion Score quality and similarity tests are presented, showing that the system can adapt to unseen voices with quality gap of 0.12 and similarity gap of 3% compared to natural speech for male voices and quality gap of 0.35 and similarity of gap of 9 % for female voices. Index Terms: DNN TTS, Neural TTS, speech synthesis, voice adaptation, voice conversion, LPCNet"
            },
            {
                "heading": "1. Introduction",
                "text": "In recent years we are experiencing a dramatic improvement of the synthesized speech quality in TTS systems, with the introduction of systems that are based on neural networks (NN). A major improvement in quality was achieved by using attention based models such as Tacotron [1] and by replacing vocoders with a NN based waveform generators such as WaveNet [2].\nA useful feature of systems with trainable models is the ability to adapt the TTS to an unseen voice using a small amount of training data (from a few seconds to an hour of speech). This is usually done by training the system on a large number of speakers, and providing a speaker embedding vector as one of the system\u2019s inputs. Using this approach allows later retraining of only a subsets of the model parameters or prediction of the speaker embedding vector [3], [4], [5].\nThe drawback of this approach is that the resulting systems use large NN models. Furthermore, a multi-speaker model usually needs much more trainable parameters than a single speaker model. This may lead to a computationally heavy and slow synthesis process even on a strong GPU. Such requirements pose a severe problem for practical TTS system that require very low latency for a dialog with a human.\nIn our previous paper [6] we introduced a NN based TTS system with two trainable modules for prosody prediction and acoustic features prediction. This system used the WORLD\nvocoder [7]. We demonstrated that this TTS allows simple adaptation to new voices. This was carried out by retraining NN models that had already been trained using a large highquality voice, on a small amount of data from the new voice. Although the quality of this system was better in many cases than similar concatenative TTS, it was still limited by the quality of the WORLD vocoder.\nRecently, an efficient neural vocoder called LPCNet was introduced [8]. The LPCNet inference runs faster than realtime on a single CPU while producing a high quality speech output. LPCNet uses cepstrum representing spectral envelopes, pitch and pitch correlation as input features. This makes it a simple alternative to other vocoders, e.g. WORLD, which work with similar features.\nIn this paper we show that we can get a considerable quality improvement by modifying a TTS system that produced the WORLD vocoder parameters [6] to predict parameters for LPCNet [8]. As in the previous work [6], we conduct multiple adaptation experiments, applied on multiple VCTK voices [9] and show that the new system has much better quality and similarity to the target voices but can still run much faster than real-time in a single-CPU mode."
            },
            {
                "heading": "2. System architecture",
                "text": "An overview of our new TTS system is presented in Figure 1. The system is a cascade of a rule-based front-end, a NN based prosody generator, a NN synthesizer and an LPCNet decoder.\nWe adopted the front-end block which is used in the IBM Watson TTS engine and is described in detail in [10]. The front-end performs a grapheme-to-phoneme conversion, represents each word with a set of positional and categorical linguistic features and associates the features with the phonemes contained within the word.\nThe prosody generator is described in section 2.1. It emits a sequence of sub-phoneme elements, including duration, pitch and intensity values. Each sub-phoneme element represents either a heading, a middle or a trailing part of a phoneme.\nThe synthesizer is described in section 2.2. It represents each sub-phoneme element by several consecutive frames according to the element\u2019s duration and generates an acoustic feature vector for each frame.\nFinally, an LPCNet block (section 2.3) is used to convert the stream of the acoustic feature vectors to a speech signal.\nThe prosody generator, synthesizer and LPCNet blocks use neural-net models for generating their output. Each block has its own model which is trained independently for each voice. Hence, the system is modular and provides easy control, flexibility and adaptability at the component level.\nFor each voice, the training and adaptation phases include the following data pre-processing steps:\n1. A grapheme-to-phoneme conversion using the frontend block. 2. Forced alignment of audio at the sub-phoneme level using proprietary acoustic modeling and speech recognition tools. 3. Extraction of textual features for prosody modeling using the front-end block. 4. Pitch detection for prosody modeling using a proprietary tool. 5. Cepstra and residual extraction using the LPCNet feature extraction tool."
            },
            {
                "heading": "2.1. Prosody generator",
                "text": "In the current work, the prosody generation and adaptation network follows the one presented in our previous work [6], where one can refer to for more details. It generates a 4- dimensional prosody vector per TTS unit, comprising the unit\u2019s log-duration, initial log-pitch, final log-pitch and logenergy. The TTS units correspond to roughly 1/3 of a phone and result from forced-alignments with 3-state hidden Markov models. The input features, derived from the TTS Front End, are comprised of 1-hot coded categorical features and standard positional features [10].\nIn this architecture the prosody adaptation to unseen speaker is based on a Variational Auto Encoder (VAE)\nutterance prosody embedding, averaged over all the speaker utterances [6], as presented on figure 2. In the current work we used multi-speaker baseline models for prosody adaptation to unseen voices, as it resulted in better quality than the single speaker models."
            },
            {
                "heading": "2.2. Synthesizer",
                "text": "The synthesis process begins by resampling the phonetic data and pitch to 10msec frames based on their duration predicted by the prosody generator. The sub-phoneme labels are represented by 32 element vectors, using a trainable embedding table.\nTime dependencies and local context are extracted by convolution layers. The convolution is performed over time on the phonetic vector and the pitch curves independently with a window size of 0.32sec (forward and backward in time).\nA longer time dependent context is extracted by an LSTM layer that merges the phonetic and pitch context. Following this are 3 fully connected layers with RELU non-linearity.\nFrom the top layer we generate by linear transformations the speech parameters that the LPCNet requires as input: 18 cepstral vector, pitch and a pitch correlation parameters with first and second derivatives for all (total of 60 parameters).\nThe final parameters which we use as input for the LPCNet are found by solving the Maximum Likelihood Parameter Generation (MLPG) equations [12]. We also apply a formant enhancement filter on the cepstral coefficients Ck, k=1\u2026N to compensate for the NN averaging and to improve the speech quality similar to [13]. The enhancement starts by multiplication of the high-order coefficients:\nC 'k={ C k k<K\u03b1 Ck k\u2265K (1) We choose \u03b1=1.4 and K=2. This can cause changes to the energy of the output, so we have to normalize it. Let E[C] be the energy of the signal derived from the coefficients C. To calculate E[C] we convert back from cepstrum to power spectrum and apply the inverse pre-emphasis filter. The energy is now the integration of this power spectrum.\nFinally, to compensate for the energy change, we apply:\nC '0=C0+\u221aN log10( E [C] E[C' ] ) (2)\nThe architecture of the synthesizer is shown in figure 3. The size of the layers is: phonetic embedding: 32, phonetic\nconvolution: 128, pitch convolution: 32, LSTM: 512 and full: 512.\nThe network is trained using an aligned corpus where the inputs are the frame based phonetic labels and pitch values, while the outputs are the corresponding LPCNet parameters. We use MSE loss function on all output parameters.\nWe first train two single-speaker models form large male and female datasets (see section 3.1). Those are used as the base models for the adaptation experiments (section 3.2).\nTo adapt the model to a smaller unseen voice, we first initialize the training with the weights of the base model of the same gender. Then, the model is trained on a small target voice. A held-out validation set is used as a stop criterion for the adaptation to avoid over-fitting."
            },
            {
                "heading": "2.3. LPCNet decoder",
                "text": "The LPCNet decoder [8] is a WaveRNN [14] variant that uses a NN model to generate speech samples from equidistant-intime input of cepstrum, pitch and pitch correlation parameters. Unlike other waveform generative models, such as WaveNet and WaveRNN, the LPCNet uses its NN to predict the LPC residual (the vocal source signal) and then apply to it an LPC filter calculated from the cepstrum.\nThis has the advantages of better control over the output of the spectral shape since it depends directly on the LPC filter shape. The model is also more robust to the predicted residual errors since any high frequency noise is also shaped by the LPC filter.\nIn this work we used the code published by the Mozilla team on Github1 with some adjustments:\n1. We replaced the pitch and pitch correlation values with values that were produced by our tools in order to maintain data consistency over all blocks.\n2. We removed any data augmentation. 3. We added validation score over held-out data to the\ntraining procedure. This score was used to select the best model and served as a training stop criteria.\nThe LPCNet model was reported to perform well in speaker independent setting, when trained on multi-speaker datasets [8], however, we experimentally found that its performance further improves when retraining the initial multi-speaker same-gender model with the target voice specific data. The validation score (evaluated on 10% of held out validation data) was used to avoid over-fitting."
            },
            {
                "heading": "3. Experiments",
                "text": "Speech samples from the following experiments are available online at http://ibm.biz/IS2019TTS"
            },
            {
                "heading": "3.1. High quality voices",
                "text": "For the first experiment we built a male and a female highquality TTS systems. We used two proprietary datasets that were originally created for building a product level concatenative TTS system. The male dataset contains 13 hours of speech and the female dataset contains 22 hours of speech. Both were produced by native US English speakers and\n1https://github.com/mozilla/LPCNet\nrecorded in a professional studio. The audio was recorded sentence by sentence.\nFor each of those voices we built the following single speaker TTS systems: 1. A WORLD based system at 22KHz as described in [6]. 2. An LPCNet based system at 16KHz as described in the\nprevious section. 3. Tacotron2 based TTS with WaveNet decoder at 22Khz [1]\nWe used each one of these systems to synthesize a set of 40 held-out sentences and compared them by a MOS test to the original recordings. Because of the differences in sample rates, all of the samples were down-sampled to 16Khz and normalized to the same energy. The tests were performed using the Amazon Mechanical Turk (AMT) platform with 50- 80 anonymous and untrained subjects participating in several evaluation sessions, constructed so that each sentence is evaluated by 30 distinct subjects. The quality MOS deployed a 5 points scale. The score for each system was calculated as the average over all its sentences. Table 1 shows the results of those tests. For the female voice the statistical significance of the difference between the LPCNet and the Tacotron systems is small (i.e. high p-value).\nWe can see from these results that the LPCNet model has a huge impact on the quality compared to the WORLD system. We can also see that even though the LPCNet system has much lower complexity than the Tacotron2 like system, it gets close to it in quality.\nOne should note relatively low MOS scores for the original natural samples, which can be explained by the assumption that the listeners subjectively judged speaker pleasantness together with the speech quality and naturalness. Hence, the evaluation should be based on relative comparison between the different systems and the original samples."
            },
            {
                "heading": "3.2. Voice adaptation",
                "text": "In this experiment we selected 4 male and 4 female US English speakers from the VCTK [9] corpus. We created 3 datasets out of each voice: the first contains the entire available data (19 \u2013 24 minutes of audio with average of 22 minutes), and the others two contain a random subset of the audio with total duration of 5 and 10 minutes.\nFrom each one of these datasets we created a single voice TTS system. The networks for the acoustic features and the LPCNet where adapted from the corresponding, same gender networks that where trained in section 3.1. The prosody network was adapted from a multi-speaker baseline model (that was originally trained on high-quality voices and VCTK voices).\nIn addition, we also built a WORLD based TTS for each voice by adapting the WORLD based acoustic feature networks from the corresponding same gender networks of section 3.1 using the full voice data.\nFrom each one of these systems we synthesized 40 sentences using text that was excluded from all the datasets. We evaluated each system's quality with MOS tests as in section 3.1. For reference, the tests also included 40 samples from the original VCTK datasets. We noticed that the original samples usually do not contain full sentences but rather short phrases. This factor, combined with the fact that VCTK comprises unprofessional speakers with varying voice pleasantness led, most probably, to the relatively low scores the original recordings received. The results of this test are summarized in Table 2. The statistical significance of the difference between the 5m and 10m LPCNet results is small.\nTo measure the similarity of the synthesized voices to the original voices we performed additional subjective listening tests on AMT. In these tests a subject is presented with a pair of samples that convey different text messages. The subject is asked to rate their voice similarity, using a 4-point scale adopted from the Voice Conversion Challenge (VCC) [15] [16], and utilized in our previous experiments [6]. We performed two tests: one with only male voices and the second with only female voices. For reference, in each test we also checked the similarity of pairs of natural speech samples from the same speaker and of pairs of natural speech from different speakers of the same gender. The results are summarized in table 3. For each system we show the average score (on the scale 1-4) and the percentage of votes, which indicated that the two presented samples were from the same speaker (option 3 or 4). The statistical significance is small for the differences between all the male LPCNet systems and also for the difference between the female LPCNet 5m and 20m systems.\nWe can compare these results to those of the VCC 2018 Hub task [15]. Although the task setup and the listening tests conditions are a bit different we can see that our system MOS and similarity score are comparable to those of the best VCC system (N10 with quality of 4.06 and Similarity of 85% where the corresponding scores for the original speech are 4.67 and 95%).\nFigure 4 shows the results for each voice. To compensate for the variability between the voices, we have normalize the\nMOS scores for each voice in the range from 1 to the score of natural samples of this voice. The similarity scores for each voice, were normalized to the range between the scores of natural samples from different and same speakers. To clarify, the normalization ranges are different for each voice."
            },
            {
                "heading": "3.3. Performance",
                "text": "The slowest block of this TTS is the LPCNet. We found that it runs about 4 times faster than real-time on a 2.8GHz i7 CPU (no GPU was used). When adding the rest of the blocks we found that we can synthesize about 3 time faster than real-time on a CPU."
            },
            {
                "heading": "4. Conclusions",
                "text": "We have presented in this article a new TTS system that addresses the challenging goals of producing high quality speech while operating at faster than real-time rate without an expensive GPU support. The system is built around three NN models for generating the prosody, acoustic features and the final speech signal.\nWe tested this system using two proprietary TTS voice datasets and demonstrated that our system produces high quality speech that is comparable to larger and much slower Tacotron2 + Wavenet systems.\nThe task of creating a high-quality TTS system out of a smaller set of audio data is even more challenging. We have shown that our system can perform well even with datasets as small as 5-20 minutes of audio. We demonstrated that when we reduce the size of the training data, there is some graceful degradation to the quality, but we are still able to maintain good similarity to the original speaker.\nFor future work, we plan to allow voice modifications by adding control over voice parameters such as pitch, breathiness and vocal tract."
            },
            {
                "heading": "5. References",
                "text": "[1] J. Shen et al., \"Natural TTS Synthesis by Conditioning Wavenet\non MEL Spectrogram Predictions,\" 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Calgary, AB, 2018, pp. 4779-4783. [2] A. van den Oord, et al. \u201cWaveNet: A generative model for raw audio\u201d. ArXiv:1609.03499, 2016.\n[3] S. O. Arik, J. Chen, K. Peng, W. Ping, and W. Zhouh, \u201cNeural Voice Cloning with a Few Samples\u201d, arXiv preprint arXiv:1802.06006, 2018. [4] Ye Jia, et al., \u201cTransfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis\u201d, arXiv preprint arXiv:1806.04558, 2018. [5] Yutian Chen, et al. \u201cSample Efficient Adaptive Text-to-Speech\u201d. arXiv preprint, arXiv:1809.10460 [6] Z. Kons, S. Shechtman, A. Sorin, R. Hoory, C. Rabinovitz and E. Da Silva Morais, \"Neural TTS Voice Conversion,\" 2018 IEEE Spoken Language Technology Workshop (SLT), Athens, Greece, 2018, pp. 290-296. [7] M. Morise, F. Yokomori, K. Ozawa, WORLD: \u201cA VocoderBased High-Quality Speech Synthesis System for Real-Time Applications\u201d, IEICE TRANSACTIONS on Information and Systems, 99(7):1877\u20131884, 2016. [8] J. Valin and J. Skoglund, \"LPCNET: Improving Neural Speech Synthesis through Linear Prediction,\" ICASSP 2019, Brighton, United Kingdom, 2019, pp. 5891-5895 [9] Veaux, Christophe; Yamagishi, Junichi; MacDonald, Kirsten. (2017). \u201cCSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit\u201d, [sound]. University of Edinburgh. The Centre for Speech Technology Research (CSTR). https://doi.org/10.7488/ds/1994. [10] L. R. Fernandez, A. Rendel, B. Ramabhadran, and R. Hoory, \"Using Deep Bidirectional Recurrent Neural Networks for Prosodic-Target Prediction in a Unit-Selection Text-to-Speech System\", in INTERSPEECH 2015 \u2013 96th Annual Conference of the International Speech Communication Association, September 6-10, Dresden, Germany, Proceedings, 2015, pp. 1606\u20131610 [11] D. P. Kingma and M.Welling, \u201cAuto-encoding variational Bayes,\u201d in Proceedings of the International Conference on Learning Representations (ICLR-14), 2014. [12] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi and T. Kitamura, \"Speech parameter generation algorithms for HMMbased speech synthesis,\" 2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings, Istanbul, Turkey, 2000, pp. 1315-1318 vol.3. [13] Z. Wu, O. Watts, and S. King, \u201cMerlin: An open sourceneural network speech synthesis system,\u201d in 9th ISCASpeech Synthesis Workshop (2016), Sep. 2016, pp. 218\u2013223. [14] Nal Kalchbrenner, et al. \"Efficient Neural Audio Synthesis\", arXiv:1802.08435 [15] Lorenzo-Trueba, Jaime; et al. (2018). \u201cThe Voice Conversion Challenge 2018: database and results\u201d, [sound]. The Centre for Speech Technology Research, The University of Edinburgh, UK. https://doi.org/10.7488/ds/2337. [16] M. Wester, Z. Wu, and J. Yamagishi, \u201cAnalysis of the voice conversion challenge 2016 evaluation results\u201d, In Interspeech, pp. 1637\u20131641, 09 2016."
            }
        ],
        "references": [
            {
                "title": "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions",
                "author": [
                    "J. Shen"
                ],
                "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Calgary, AB, 2018, pp. 4779-4783.",
                "citeRegEx": "1",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "WaveNet: A generative model for raw audio",
                "author": [
                    "A. van den Oord"
                ],
                "venue": null,
                "citeRegEx": "2",
                "shortCiteRegEx": "2",
                "year": 2016
            },
            {
                "title": "Zhouh, \u201cNeural Voice Cloning with a Few Samples",
                "author": [
                    "S.O. Arik",
                    "J. Chen",
                    "K. Peng",
                    "W. Ping"
                ],
                "venue": "arXiv preprint arXiv:1802.06006,",
                "citeRegEx": "3",
                "shortCiteRegEx": "3",
                "year": 2018
            },
            {
                "title": "Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis",
                "author": [
                    "Ye Jia"
                ],
                "venue": "arXiv preprint arXiv:1806.04558,",
                "citeRegEx": "4",
                "shortCiteRegEx": "4",
                "year": 2018
            },
            {
                "title": "Neural TTS Voice Conversion",
                "author": [
                    "Z. Kons",
                    "S. Shechtman",
                    "A. Sorin",
                    "R. Hoory",
                    "C. Rabinovitz",
                    "E. Da Silva Morais"
                ],
                "venue": "2018 IEEE Spoken Language Technology Workshop (SLT), Athens, Greece, 2018, pp. 290-296.",
                "citeRegEx": "6",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "WORLD: \u201cA Vocoder- Based High-Quality Speech Synthesis System for Real-Time Applications",
                "author": [
                    "M. Morise",
                    "F. Yokomori",
                    "K. Ozawa"
                ],
                "venue": "IEICE TRANSACTIONS on Information and Systems,",
                "citeRegEx": "7",
                "shortCiteRegEx": "7",
                "year": 2016
            },
            {
                "title": "LPCNET: Improving Neural Speech Synthesis through Linear Prediction",
                "author": [
                    "J. Valin",
                    "J. Skoglund"
                ],
                "venue": "ICASSP 2019, Brighton, United Kingdom, 2019, pp. 5891-5895",
                "citeRegEx": "8",
                "shortCiteRegEx": null,
                "year": 2019
            },
            {
                "title": "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit\u201d, [sound",
                "author": [
                    "Christophe Veaux",
                    "Junichi Yamagishi",
                    "Kirsten. MacDonald"
                ],
                "venue": "University of Edinburgh. The Centre for Speech Technology Research (CSTR)",
                "citeRegEx": "9",
                "shortCiteRegEx": "9",
                "year": 2017
            },
            {
                "title": "Deep Bidirectional Recurrent Neural Networks for Prosodic-Target Prediction in a Unit-Selection Text-to-Speech System\", in INTERSPEECH 2015",
                "author": [
                    "L.R. Fernandez",
                    "A. Rendel",
                    "B. Ramabhadran",
                    "R. Hoory",
                    "\"Using"
                ],
                "venue": "Annual Conference of the International Speech Communication Association,",
                "citeRegEx": "10",
                "shortCiteRegEx": "10",
                "year": 2015
            },
            {
                "title": "Auto-encoding variational Bayes",
                "author": [
                    "D.P. Kingma",
                    "M.Welling"
                ],
                "venue": "Proceedings of the International Conference on Learning Representations (ICLR-14), 2014.",
                "citeRegEx": "11",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Speech parameter generation algorithms for HMMbased speech synthesis",
                "author": [
                    "K. Tokuda",
                    "T. Yoshimura",
                    "T. Masuko",
                    "T. Kobayashi",
                    "T. Kitamura"
                ],
                "venue": "2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings, Istanbul, Turkey, 2000, pp. 1315-1318 vol.3.",
                "citeRegEx": "12",
                "shortCiteRegEx": null,
                "year": 2000
            },
            {
                "title": "Merlin: An open sourceneural network speech synthesis system",
                "author": [
                    "Z. Wu",
                    "O. Watts",
                    "S. King"
                ],
                "venue": "9th ISCASpeech Synthesis Workshop (2016), Sep. 2016, pp. 218\u2013223.",
                "citeRegEx": "13",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "The Voice Conversion Challenge 2018: database and results\u201d, [sound]. The Centre for Speech Technology Research, The University of Edinburgh, UK. https://doi.org/10.7488/ds/2337",
                "author": [
                    "Lorenzo-Trueba",
                    "Jaime"
                ],
                "venue": null,
                "citeRegEx": "15",
                "shortCiteRegEx": "15",
                "year": 2018
            },
            {
                "title": "Analysis of the voice conversion challenge 2016 evaluation results",
                "author": [
                    "M. Wester",
                    "Z. Wu",
                    "J. Yamagishi"
                ],
                "venue": "In Interspeech, pp. 1637\u20131641,",
                "citeRegEx": "16",
                "shortCiteRegEx": "16",
                "year": 2016
            }
        ],
        "abstractText": "We present a lightweight adaptable neural TTS system with high quality output. The system is composed of three separate neural network blocks: prosody prediction, acoustic feature prediction and Linear Prediction Coding Net as a neural vocoder. This system can synthesize speech with close to natural quality while running 3 times faster than real-time on a standard CPU. The modular setup of the system allows for simple adaptation to new voices with a small amount of data. We first demonstrate the ability of the system to produce high quality speech when trained on large, high quality datasets. Following that, we demonstrate its adaptability by mimicking unseen voices using 5 to 20 minutes long datasets with lower recording quality. Large scale Mean Opinion Score quality and similarity tests are presented, showing that the system can adapt to unseen voices with quality gap of 0.12 and similarity gap of 3% compared to natural speech for male voices and quality gap of 0.35 and similarity of gap of 9 % for female voices."
    },
    {
        "title": "DIMSIM: An Accurate Chinese Phonetic Similarity Algorithm based on Learned High Dimensional Encoding",
        "sections": [
            {
                "heading": null,
                "text": "Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 444\u2013453 Brussels, Belgium, October 31 - November 1, 2018. c\u00a92018 Association for Computational Linguistics\n444"
            },
            {
                "heading": "1 Introduction",
                "text": "Performing the mental gymnastics of transforming \u2018I\u2019m hear\u2019 to \u2018I\u2019m here,\u2019 or, \u2018I can\u2019t so buttons\u2019 to \u2018I can\u2019t sew buttons,\u2019 is familiar to anyone who has encountered autocorrected text messages, punny social media posts, or just friends with bad grammar. Although at first glance it may seem that phonetic similarity can only be quantified for audible words, this problem is often present in purely textual spaces, such as social media posts or text messages. Incorrect homophones and synophones, whether used in error or in jest, pose challenges for a wide range of NLP tasks, such as named entity identification, text normalization and spelling correction (Chung et al., 2011; Xia et al., 2006; Toutanova and Moore, 2002; Twiefel et al., 2014; Lee et al., 2013; Kessler, 2005). These tasks must therefore successfully transform incorrect words or phrases (\u2018hear\u2019,\u2019so\u2019) to their phonetically similar correct counterparts (\u2019here\u2019,\u2019sew\u2019), which in turn requires a robust representation of phonetic similarity between word pairs. A reli-\nable approach for generating phonetically similar words is equally crucial for Chinese text (Xia et al., 2006).\nUnfortunately, most existing phonetic similarity algorithms such as Soundex (Archives and Administration, 2007) and Double Metaphone (DM) Philips (2000) are motivated by English and designed for Indo-European languages. Words are encoded to approximate phonetic presentations by ignoring vowels (except foremost ones), which is appropriate where phonetic transcription consists of a sequence of phonemes, such as for English. In contrast, the speech sound of a Chinese character is represented by a single syllable in Pinyin consisting of two or three parts: an initial (optional), a final or compound finals, and tone 1 (Table 1). As a result, phonetic similarity approaches designed for Indo-European languages often fall short when applied to Chinese text. Note that we use Pinyin as the phonetic representation because it is a widely accepted Romanization system (San, 2007; ISO, 2015) of Chinese syllables, used to teach pronunciation of standard Chinese. Table 2 shows two sentences from Chinese microblogs, containing informal words derived from phonetic transcription. The DM and Soundex encodings for\n1Chinese has five tones, represented on a 1-5 scale.\nnear-homonyms of \u559c\u6b22 from Table 2 are shown in Table 3. Since both DM and Soundex ignore vowels and tones, words with dissimilar pronunciations are incorrectly assigned to the same encoding (e.g. \u7a00\u996d and \u6cc4\u6124), while true nearhomonyms are encoded much further apart (e.g. \u7a00\u996d and \u559c\u6b22). On the other hand, additional candidates with similar phonetic distances such as \u5fc3xin1\u70e6fan2\uff0c\u897fxi1\u65b9fang1 for\u7a00\u996d should be generated, for consumption by downstream applications such as text normalization.\nThe example highlights the importance of considering all Pinyin components and their characteristics when calculating Chinese phonetic similarity (Xia et al., 2006). One recent work (Yao, 2015) manually assigns a single numerical number to encode and derive phonetic similarity. However, this single-encoding approach is inaccurate since the phonetic distances between Pinyins are not captured well in a one dimensional space. Figure 1 illustrates the similarities between a subset of initials. Initial groups \u201cz, c\u201d, \u201czh, ch\u201d, \u201cz, zh\u201d and \u201czh, ch\u201d are all similar, which cannot be captured using a one dimensional representation (e.g., an encoding of \u201czh=0,z=1,c=2,ch=3\u201d fails to identify the \u201czh, ch\u201d pair as similar.) ALINE (Kondrak, 2003) is another illustration of the challenge of manually assigning numerical values in order to accurately represent the complex relative phonetic similarity relationships across various languages. Therefore, given the perceptual nature of the problem of phonetic similarity, it is critical to learn the distances based on as much empirical data as possible (Kessler, 2005), rather than using a manually encoded metric.\nThis paper presents DIMSIM, a learned ndimensional phonetic encoding for Chinese along with a phonetic similarity algorithm, which uses the encoding to generate and rank phonetically\nsimilar words. To address the complexity of relative phonetic similarities in Pinyin components, we propose a supervised learning approach to learn n dimensional encodings for finals and initials where n can be easily extended from one to two or higher dimensions. The learning model derives accurate encodings by jointly considering Pinyin linguistic characteristics, such as place of articulation and pronunciation methods, as well as high quality annotated training data sets. We compare DIMSIM to Double Metaphone(DM), Minimum edit distance(MED) and ALINE demonstrating that DIMSIM outperforms these algorithms by 7.5X on mean reciprocal rank, 1.4X on precision and 1.5X on recall on a real-world dataset. Our contributions are:\n1. An encoding for Chinese Pinyin leveraging Chinese pronunciation characteristics.\n2. A simple and effective phonetic similarity algorithm to generate and rank phonetically similar Chinese words.\n3. An implementation and a comprehensive evaluation showing the effectiveness of DIMSIM over the state-of-the-art algorithms.\n4. A package release of the implemented algorithm and a constructed dataset of Chinese words with phonetic corrections.2"
            },
            {
                "heading": "2 Generating Phonetic Candidates",
                "text": "DIMSIM generates ranked candidate words with similar pronunciation to a seed word. Similarity is measured by a phonetic distance metric based on n-dimensional encodings, as introduced below."
            },
            {
                "heading": "2.1 Phonetic Comparison for Pinyin",
                "text": "An important characteristic of Pinyin is that the three components, initial, final and tone, can be independently phonetically compared. For example, the phonetic similarity of the finals \u201cie\u201d and \u201cue\u201d is identical in the Pinyin pairs {\u201cxie2\u201d,\u201cxue2\u201d} and {\u201clie2\u201d,\u201clue2\u201d}, in spite of the varying initials. English, by contrast, does not have this characteristic. Consider as an example, the letter group \u201cough,\u201d which is pronounced quite differently in \u201crough,\u201d \u201cthrough\u201d and \u201cthough.\u201d\nNote that depending on the initials, a final of same written form can represent different finals. For instance, u\u0308 is written as u after j, q and x; uo is written as o after b, p,m, f or w. There are a total\n2https://github.com/System-T/DimSim.\nof six rewritten rules in Pinyin (ISO, 2015). Since these rules are fixed, we preprocess the Pinyins according to these rules, transforming them into the original form for our internal representation (e.g., we represent ju as ju\u0308 and bo as buo.)"
            },
            {
                "heading": "2.2 Measuring Phonetic Similarity",
                "text": "DIMSIM represents a given word w as a list of characters {ci|1 \u2264 i \u2264 K} where K is the number of characters and pci denotes the Pinyin of ith character. The initial, final, and tone components of pci are denoted as p I ci , p F ci , and p T ci , respectively.\nFormally, the phonetic similarity S between the pronunciation of ci and c\u2032i is computed using Manhattan distance as the sum of the distances between the three pairs of components, as follows:\n\u2211 1\u2264i\u2264K S(ci, c \u2032 i) = \u2211 1\u2264i\u2264K {Sp(pIci , pIc\u2032i)+\nSp(p F ci , p F c\u2032i ) + ST (p T ci , p T c\u2032i )}\n(1)\nManhattan distance is an appropriate metric since the three components are independent. Any single change does not affect more than one component, and any change affecting several components is the result of multiple independent and additive changes. It follows that the similarity between two words is computed as the sum of the phonetic distances of characters. For example, the Pinyins of \u201c\u7ae5\u978b\u201d and \u201c\u540c\u5b66\u201d are \u201ctong2xie2\u201d and \u201ctong2xue2\u201d. The distance between \u201c\u7ae5(tong2)\u201d and \u201c\u540c(tong2)\u201d is zero; the distance between \u201c\u978b(xie2)\u201d and \u201c\u5b66(xue2)\u201d is calculated as S( \u201c\u978b\u201d , \u201c\u5b66\u201d ) = Sp(x, x) + Sp(ie, ue) + ST (2, 2). Although the characters \u201c\u978b(xie2)\u201d and \u201c\u5b66(xue2)\u201d are completely different, their Pinyins only differ in their finals."
            },
            {
                "heading": "2.3 Learning Pinyin Encodings",
                "text": "The next task is to compute encodings for initials, finals, and tones. While tonal similarity is easily handled (see Section 2.4), pairwise similarity for initials and finals is more complex. We adopt a supervised learning approach to obtain these encodings, using linguistic characteristics combined with a labeled dataset. The latter consists of word pairs, with specific pairs of initials or finals manually annotated for phonetic similarity. The set of annotated pairs between initials and finals are then used to learn the n-dimensional encodings of initials and finals, which will in turn be used for generating phonetically similar candidates.\n3\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nConfidential Review Copy. DO NOT DISTRIBUTE.\nFor instance, u\u0308 is written as u after j, q, x. uo is written as o after b, p, m, f or w. There are a total of six rewritten rules in Pinyin (ISO, 2015). Since these rules are fixed, it is straightforward to preprocess the Pinyins according to these rules to turn them into the original form of Pinyins as a internal representation before conducting phonetic comparison. For example, we represent ju as ju\u0308, bo as buo. After the preprocessing step, we independently compare ompon nts.\n2.2 Measuring Phonetic Similarity\nDIMSIM represents a given Chinese word w as a list of Chinese characters {ci|1  i  K} where K is the umber of characters in w and pci denotes the Pinyin of ith character. The initial, final, and tone components of the Pinyin pci are denoted as pIci , p F ci , and p T ci , respectively.\nFormally, the phonetic similarity S between the pronunciation of two characters, ci and c0i is computed using Manhattan distance as the sum of the distances between the three pairs of components, as follows:\nX\n1iK S(ci, c\n0 i) =\nX\n1iK {Sp(pIci , p I c0i )+ Sp(p F ci , p F c0i ) + ST (p T ci , p T c0i )} (1)\nManhattan distance is an appropriate metric since the three components are independent. A single change in a Pinyin is therefore a change to the initial, the final, or the tone, but not to more than one of the components simultaneously. A change that affects more than one component is the result of multiple independent and therefore additive changes.\nFollowing the same logic, the phonetic similarity between two words w and w0 is computed as the sum of the distances between the Pinyins. For example, the Pinyins of \u201c\u00c2\u00e3\u201d and \u201c f\u201d are \u201ctong2xie2\u201d and \u201ctong2xue2\u201d respectively. The distance between \u201c\u00c2(tong2)\u201d and \u201c (tong2)\u201d is zero; the distance between \u201c\u00e3(xie2)\u201d and \u201cf(xue2)\u201d is calculated as S(ie, ue) = Sp(x, x)+Sp(ie, ue)+ST (2, 2). We see that although the characters are completely different, \u201c\u00e3(xie2)\u201d and \u201cf(xue2)\u201d only differ in their fina s, b t not thei initials and tones.\n2.3 Learning Pinyin Encodings\nTherefore, the next task is to generate an accurate representation of phonetic similarity for every pair of initials, finals, and tones. As there are only 5\nTable 4: Table of Pinyin initials (colors denote clusters).\ntones in Chinese, pairwise tonal similarity is easily handled (see section 2.4). However, pairwise similarity for initials and finals is more complex and must be learned. We use a supervised machine learning approach that uses Pinyin linguistic characteristics combined with manually labeled data sets of phonetic similarity. The training data sets consist of word pairs that highlight a pair of initials (or finals), and are used as the context for an annotator-provided phonetic similarity score. The manually labeled scores are transformed into similarity scores. The set of initials (or finals) is then mapped to the n-dimensional encodings by minimizing the difference between the resulting pairwise distances, and the distances obtained from the training data sets.\n2.3.1 Generating Similar Word Pairs Phonetically similar word pairs are used to create annotations representing the phonetic similarity of a pair of initials, or finals. Chinese has 253 pairs of initials and 666 pairs of finals. Manually annotating each pair similarity requires a very large number of examples: assuming ten or twenty word pairs are provided as context for each pair, the task quickly blows up to nine or eighteen thousand annotations. We observe that the phonetic similarity of Chinese Pinyin is greatly impacted by the pronunciation methods and the place of articulation. Leveraging known Pinyin linguistic characteristics can improve the accuracy of our model and reduce the size of the annotation task. Specifically, this is done by grouping the Pinyin components into initial clusters according to the Pinyin pronunciation tables (ISO, 2015) and only annotating the pairs within each cluster along with a single pairwise distance between clusters.\nTable 4 shows the the clustering of initials according to the Pinyin linguistic characteristics. We partition initials into 12 clusters, consisting"
            },
            {
                "heading": "2.3.1 Generating Similar Word Pairs",
                "text": "Phonetically sim lar wor pairs are used t create an otations repres nting the phon tic simil rity of initials, or finals. Chinese has 253 pairs of initials and 666 pairs of finals. Annotating examples of all these pairs is labor intensive and error-prone. Assuming twenty word pairs are provided as context per pair, the task quickly blows up to eighteen thousand annotations. However, we observe that the phonetic similarity of Pinyin is greatly impacted by the pronunciation methods and the place of articulation - this allows us to improve the accuracy and simplify the annotation task. Specifically, this is done by grouping Pinyin components into initial clusters and only annotating pairs within each cluster, and represent tive cluster pairs.\nFigure 2 partitions initials nto 12 clusters, consisti g of \u201cbp\u201d,\u201cdt\u201d,\u201cgk\u201d,\u201chf\u201d,\u201cnl\u201d,\u201cr\u201d, \u201cjqx\u201d, \u201czcs\u201d, \u201czhchsh\u201d, \u201cm\u201d ,\u201cy\u201d and \u201cw\u201d, based on the pronunciatio method and the place of rticulation. \u201cf\u201d and \u201ch\u201d are grouped together as they are both fricative and sound very similar, especially for people from the southeast of China (Zhishihao, 2017). We then eliminate the comparison of pairs that are highly similar or highly dissimilar. For example, as the semivowel initials \u201cy\u201d and \u201cw\u201d are dissimilar to all other initials, we label every initial pair containing one of them with the lowest possible score. To compare between clusters, we randomly choose one initial from each cluster and generate just those comparison pairs. The number of pairs of initials decreases from 253 to 59.\nWe use a similar method for finals, partitioning them into six groups by the six basic vowels (\u201c ,o,e,i,u,u\u0308\u201d) (e.g., \u201ci,in,ing\u201d are clustered together.) We t n use ed t distance and common sequence length con traints to g ide the pair generation; specifically, we compare a pair of finals if the edit distance between them is 1 or 2. Since\nthe length of finals on average is two, an edit distance of three means a complete change to the final, resulting in pairs with the lowest similarity. To compare finals across clusters, since the edit distance between any such pair is at least two, we compare pairs only when the length of the common sequence is at least two (for example, \u201cian\u201d and \u201cuan\u201d), and otherwise assign the lowest possible similarity to the pairs. This drops the number of comparison pairs of finals down to 113.\nAfter generating the comparison pairs, we create word pairs whose Pinyins only differ in the these pairs. We identify and account for several confounding factors that may affect annotation: 1) the position of the character containing the initial or final being compared; 2) the word length; and 3) the combination of initials and finals. Since most Chinese words are of length two, we only generate word pairs of length two for this task. Providing word pairs of length greater than two would not make much difference to learned encodings as long as word pairs are representative.\nFor a given initial (or final) pair (p1, p2), such as (b, p), we first generate the all possible Pinyins with a component of p1 such as bao and bing. For each Pinyin py, we retrieve all the words with length two in the dictionary which also have first or second character with the same py. Example words for py=\u201cbao\u2032\u2032 include\u5305bao1\u88b1fu2. For each created word w, we change the initial (or final) from p1 to p2, retrieve the corresponding words from the dictionary and generate the word pairs to compare. One such example is (\u5305bao1\u88b1fu2, \u6ce1pao4\u8299fu2). Finally, from the full list we randomly select five word pairs that vary the first character, and five word pairs that vary the second character.\nWe invite three native Chinese speakers to perform the annotations. For each word pair, the annotators give a label on a 7 point scale representing their agreement, where the labels range from \u2019Completely Disagree\u2019 (1) to \u2019Completely Agree\u2019 (7). We calculate Krippendorff\u2019s \u03b1 (Hayes and Krippendorff, 2007) for the initials and finals annotations to be 0.69 and 0.54, representing the inter-annotator agreement. For each word pair, we use Equation 2 to calculate the distance \u03b8 with the average value \u03c6 of labels across the annotators. Equation 2 inverts the labels so that the output can be used as a distance metric (phonetically similar initials or finals are closer together), and scales the\nresult to more accurately measure phonetic similarities. The parameters a and b are set 4 and 104 by default, but we also show that the performance of our method is not sensitive to the parameter settings (see Section 3.2).\n\u03b8(\u03c6) = 1/a\u03c6 \u2217 b (2)"
            },
            {
                "heading": "2.3.2 Learning Model",
                "text": "Once the average distances between pairs are computed from the annotated data sets, we define a constrained optimization to compute encodings of the initials and finals. The final goal is to map each initial (or final) to an n-dimensional point.\nThe distance Sp of a pair p of points (x1, x2, ..., xn), (y1, y2, ..., yn) is calculated using Euclidean distance as shown in equation 3.\nSp = \u221a \u2211 1\u2264i\u2264n (xi \u2212 yi)2 (3)\nThe model aims to minimize the sum of the absolute differences between the Euclidean distances of component pairs and the average distances obtained from the annotated training data across all pairs for initials (or finals) C. We also incorporate a penalty function, \u03c4p, for pairs deviating from the manually annotated distance \u03b8 so that more phonetically similar pairs are penalized more highly (we discuss \u03c4 further in Section 3.2). Equation 4 represents the cost function:\nmin \u2211 p\u2208C |S2p \u2212 \u03b82p| \u2217 \u03c4p (4)\nOne main advantage of our learning model is that it is generic and can easily extend to any ndimensional space. Based on the structured of Table 2, we intuit that extending beyond one dimension will yield more accurate encodings. Figures 3 and 4 visualize the computed encodings of initials when setting n=1 and n=2 We see that when n = 2, the locations of initial coordinates align well with Table 2,. In particular, the twelve groups are clustered in a pattern that is defined in Section 2.3.1. For example, \u201cbp,gk,jqx\u201d are separated into different clusters. However, while Table 2 indicated the basic clusters for the initials, our learned model goes further than Table 2 by actually quantifying the inter- and intra-cluster similarities. Specifically, clusters \u201cc, ch, j, q, x\u201d are tighter than clusters \u201cc, c, h\u201d and \u201cd, t\u201d, whereas the clusters \u201cm\u201d and \u201cn, r, l\u201d are well separated from other clusters. Interestingly, the learning algorithms organically discovers new clusters that are not reflected in Table 2; namely that \u201cr,n\u201d and \u201cr,l\u201d are pairs of phonetically similar initials.\nWhen n = 1, the learned model collapses the coordinates into one dimension (Figure 3). We observe that the predefined clusters are not well aligned, and many clusters are mixed together (e.g., \u201cbp,gk,nl,dt\u201d), preventing DIMSIM from considering variations within a cluster to be more similar than variations between clusters. Visually comparing Figures 3 and 4 gives the intuition for why DIMSIM with n = 2 performs better than DIMSIM with n = 1, which is in turn reflected in our evaluation results. Section 3 presents the effects that varying the number of dimensions has on evaluation results."
            },
            {
                "heading": "2.4 Phonetic Tone Similarity",
                "text": "There are five tones in Chinese, represented by a tone number scale ranging from 1 to 5. It is simple to use tone numbers for tone encodings and the difference between the tones of two Pinyins as the raw measure of distance, ranging in value from 1 to 5 (e.g., ST (xue2, xue4) = 4\u2212 2 = 2). One exception is that we encode tone 3 as the numerical value of 2.5 since tone 3 is more similar to tone 2 compared to tone 4 according to the relative pitch changes of the four tones (ISO, 2015). However, this measure must first be scaled to be comparable to the pairwise phonetic distances of initials and finals. There is an additional constraint: any pairwise difference in initials or finals must have\nInput : Word w, Threshold th,Dict dict; Output: Words outws; begin\npys = getPinyins(w,dict); headPys = getSimPinyins(pys(0), th); headWords = getWordswithHeadPy(headPys, dict); for cw \u2208 headWords do\nif cw.size 6= w.size then continue; end sim = getSimilarity(cw,w); if sim \u2264 th then\noutws.add(cw); end\nend sortByAscSim(outws); return outws;\nend Algorithm 1: Generating phonetic candidates.\na greater negative effect on the phonetic similarity between characters than any difference in tones. For example, S(xue1,lue1)<S(xue1,xue5) even though xue1 and xue5 are at opposite ends of the tone scale. We therefore scale ST such that Max(ST ) <Min(Sp)."
            },
            {
                "heading": "2.5 Candidate Generation and Ranking",
                "text": "Having determined the phonetic encodings and the mechanism to compute the phonetic similarity using learned phonetic encodings, we now describe how to generate and rank similar candidates in Algorithm 1. Given a word w, a similarity threshold th, and a Chinese Pinyin dictionary dict, we retrieve the Pinyin py of w from dict. We derive a list of Pinyins Pys whose similarity to py falls within the threshold th. These are used to generate a list of words with the same Pinyin in Pys and the same number of characters as w. We calculate the similarity of each candidate word with w using Equation 1 and filter out candidates that fall outside the similarity threshold th. Thus, th is a parameter that affects the precision and recall of the generated candidates. A larger th generates more candidates, increasing recall while decreasing precision.3 Finally, we output the candidates ranked in ascending order by similarity distance.\n3We study the impact of varying th in Section 3."
            },
            {
                "heading": "3 Evaluation",
                "text": "We collect 350 words from social media (Wu, 2016), and annotate each with 1-3 phonetically similar words. We use a communitymaintained free dictionary to map characters to Pinyins (CEDict, 2016). We compare DIMSIM with Double Metaphone (DM) (Philips, 2000), ALINE (Kondrak, 2003) and Minimum edit distance (MED) (Navarro, 2001) in terms of precision (P), recall (R), and average Mean Reciprocal Rank (MRR) (Voorhees and et al., 1999). We calculate recall automatically using the the full test set of word pairs (Wu, 2016). Since downstream applications will only consider a limited number of candidates in practice, we evaluate precision via a manual annotation task on the top-ranked candidates generated by each approach. DM considers word spelling, pronunciation and other miscellaneous characteristics to encode the word into a primary and a secondary code. DM as one of the baselines is known to perform poorly at ranking the candidates (Carstensen, 2005) since only two codes are used. We therefore use our method (Equation 1) to rank the DM-generated candidates, to create a second baseline, DM-rank.4 The third baseline, ALINE, measures phonetic similarity based on manually coded multi-valued articulatory features weighted by their relative importance with respect to feature salience (again, manually determined). MED, the last baseline, computes similarity as the minimum-weight series of edit operations that transforms one sound component into another."
            },
            {
                "heading": "3.1 The Effectiveness of DIMSIM",
                "text": "Recall and MRR: We compare DIMSIM to DM, DM-rank, ALINE and MED. DIMSIM1 and DIMSIM2 denotes DIMSIM encoding dimension n = 1 and n = 2, respectively. As shown in Figure 5, DIMSIM2 improves recall by factors of 1.5, 1.5, 1.3 and 1.2, and improves MRR by factors of 7.5, 1.4, 1.03 and 1.2 over DM, DM-Rank, ALINE and MED, respectively. DM performs relatively poorly, as it is designed for English, and does not accurately reflect Chinese pronunciation. Ranking DM candidates using the DIMSIM phonetic distance defined in Equation 1 improves its average MRR by a factor of 5.5. However, even DM-Rank is outperformed by the simple MED\n4We do not compare with Soundex as DM is accepted to be an improved phonetic similarity algorithm over Soundex.\nbaseline, demonstrating the inherent problem with DM\u2019s coarse encodings. While ALINE has a similar recall to DIMSIM, it performs worse on MRR than DIMSIM2 because it does not have a direct representation of compound vowels for Pinyin. It measures distance between compound vowels using phonetic features of basic vowels which leads to inaccuracy. In turn, MED struggles with representing accurate phonetic distances between initials, since most initials are of length 1, and the edit distance between any two characters of length 1 is identical. In contrast, DIMSIM encodes initials and finals separately, and thus even a 1-dimensional encoding (DIMSIM1) outperforms the other baselines. Finally, the intuition of Figures 3 and 4 is reflected in the data, as DIMSIM2 outperforms DIMSIM1 by 14% (MRR).\nPrecision and MRR: Here we evaluate the quality of the candidate ranking since in practice, downstream applications consider only a small number of possible candidates for every word. We ask two native Chinese speakers to annotate the quality of the generated candidates. Choosing 100 words randomly from the test set, we use DMrank, MED, ALINE and DIMSIM2 to generate top-K candidates for each seed word (K = 5).5 The annotators mark each candidate as phonetically similar to the seed word (1) or not (0), also marking the one candidate they believe to be the most similar-sounding (2), which may be any of\n5We do not evaluate DM and DIMSIM1 as they perform worse than DM-Rank and DIMSIM2, respectively.\nthe top-K candidates. We then compute precision and average MRR using the obtained annotations. We achieve inter-level agreement(ILA) of 0.75 for P and ILA of 0.84 for average MRR. DIMSIM once again outperforms MED and DM-Rank by up to 1.4X for precision and 1.24X for MRR. Since the only criteria for picking the best top-K candidate is phonetic similarity, this demonstrates that DIMSIM ranks the most phonetically similar candidates higher than the other baselines."
            },
            {
                "heading": "3.2 Impact of Scoring and Penalty Functions",
                "text": "We study the sensitivity of DIMSIM to varying the scoring and penalty functions, using recall and average MRR for evaluation. Table 4 shows four different scoring functions \u03b8 and penalty functions \u03c4 (including the variation of not using a penalty\nfunction) to convert the annotator scores \u03c6 to pairwise distances S, following Equation 4.\nFigure 7 depicts the values of the four scoring functions \u03b8 as a function of the annotator scores on a log 10 scale, to demonstrate the effect of varying a and b, as well as using \u03c6 as the base or exponent. Figure 8 demonstrates how sensitive our model is to the different combinations of scoring and penalty functions. We see that although Recall is entirely insensitive to the variations, the performance of MRR is impacted. There is a clear preference for the variations on the \u201cdiagonal\u201d of Table 4: F11, F22, F33, F44, but the nearidentical performance of these variations demonstrates DIMSIM\u2019s robustness to the particular scoring and penalty functions used. Note that not using a penalty function impacts MRR significantly."
            },
            {
                "heading": "3.3 Impact of the Encoding Dimensions",
                "text": "As demonstrated above, encoding initials and finals into a two-dimensional space is more effective than a one-dimensional space. Figure 9 presents the results of continuing to increase the number of dimensions, n = [1, 4]. We observe that recall is barely affected, with all variations able to successfully identify the targeted words 98% to 99% of the time. We also see that moving from n=1 to n=2 increases the average MRR by 1.14X . However, further increasing the number of dimensions to n>2 no longer improves average MRR, indicating that learning a two-dimensional encoding is enough to capture the phonetic relationships between Pinyin components."
            },
            {
                "heading": "3.4 Impact of the Distance Threshold",
                "text": "We examine how the similarity distance threshold (th) impacts DIMSIM by varying th from 2 to 4096 (Figure 10) (using the scoring function F22). As th increases, recall increases from 0.75 to 0.99, converging when th reaches 2048. By increasing th DIMSIM matches more characters that are simi-\nlar to the first character of the given word, which in turn increases the number of candidates within the distance. Thus, the probability of including the labeled gold standard words in the results increases. MMR is less sensitive to th, converging when th reaches 128. However, the generated set of candidate words is reduced too much for th < 128, hurting the performance of MMR. To ensure both high recall and MRR we set th = 2500."
            },
            {
                "heading": "3.5 Impact of Number of Candidates",
                "text": "While generating more candidates improves the recall, presenting too many candidates to a downstream application is not desirable. To find a balance, we study the impact of varying the upper limit of the number of generated candidates nc from 2 to 2048 (Figure 11). We find that MRR converges at 64 candidates, while recall takes longer; however, setting the upper limit at 64 candidates already achieves almost 98% recall, suggesting it as a reasonable cutoff in practice. Unless otherwise mentioned, we set nc = 1, 000 for experiments, to isolate the impact of this parameter."
            },
            {
                "heading": "3.6 Error Analysis",
                "text": "We analyze and summarize three types of errors made by DIMSIM. The first occurs when targeted words are out of vocabulary(OOV). For instance, for the original word \u201c\u836f\u4e38\u201d , the targed word is\n\u201c\u8981\u5b8c\u201d which is OOV. As is commonly the case in text normalization applications which convert informal language to well-formed terms, our method works as long as the targeted words are in the dictionary. This shortcoming is generally alleviated by adding new terms to the dictionary. Second, DIMSIM cannot derive phonetic candidates from dialects that are not encoded in our mapping table. For example, for \u201c\u51bb(dong4)\u849c(suan4)\u201d , the targeted word \u201c\u5f53(dang1)\u9009(xuan2)\u201d is obtained using the pronunciation of southern Fujian dialect. However, our approach can easily be extended to incorporate and capture such variants by learning mapping tables for each dialect and using them to generate corresponding candidates. Finally, we constrain DIMSIM to not identify candidates that differ in length from the seed word, as we observe that most transcriptions have the same word length - though some corner cases do occur."
            },
            {
                "heading": "4 Related Work",
                "text": "There is a plethora of work focusing on the phonetic similarities between words and characters (Archives and Administration, 2007; Mokotoff, 1997; Taft, 1970; Philips, 1990, 2000; Elsner et al., 2013). These algorithms encode words with similar pronunciation into the same code. For example, Soundex (Archives and Administration, 2007) converts words into fixed length code through a mapping table of initial groups to ordinal numbers. These algorithms fail to capture Chinese phonetic similarity since the conversion rules do not consider pronunciation properties of Pinyin. Linguists in the phonetic and phonology community have also proposed several phonetic comparison algorithms (Kessler, 2005; Mak and Barnard, 1996; Nerbonne and Heeringa, 1997; Ladefoged, 1969; Kondrak, 2003) for determining the similarity between speech forms. However, as features of articulatory phonetics are manually assigned, these algorithms fall short in capturing the perceptual essence of phonetic similarity through empirical data (Kessler, 2005). In contrast, DIMSIM achieves high accuracy by learning the encodings both from high quality training data sets and linguistic Pinyin features.\nSeveral works in Named Entity translation (Lin and Chen, 2002; Lam et al., 2004; Kuo et al., 2007; Chung et al., 2011) focus on learning the phonetic similarity between English and Chinese automatically. These approaches first represent English and\nChinese words in basic phoneme units and apply edit distance algorithms to compute the similarity. Training frameworks are then used to learn the similarity. However, the phonetic similarity used in these systems cannot be applied to Chinese words since Pinyin has its own specific characteristics, which do not easily map to English, for determining phonetic similarity. Another main application of phonetic similarity algorithms is text normalization (Xia et al., 2006; Li et al., 2003; Han et al., 2012; Sonmez and Ozgur, 2014; Qian et al., 2015), where phonetic similarity is measured by a combination of initial and final similarities. However, the encodings used in these approaches are too coarse-grained, yielding low F1 measures. DIMSIM learns separate high dimensional encodings for initials and finals, and uses them to calculate and rank the distances between Pinyin representations of Chinese word pairs. Karl Stratos (Stratos, 2017) proposes a sub-character architecture to deal with the data sparsity problem in Korean language processing by breaking down each Korean character into a small set of primitive phonetic units. However, this work does not address the problem of the phonetic similarity and is thus orthogonal to DIMSIM."
            },
            {
                "heading": "5 Conclusion",
                "text": "Motivated by phonetic transcription as a widely observed phenomenon in Chinese social media and informal language, we have designed an accurate phonetic similarity algorithm. DIMSIM generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of Pinyin initial, final, and tone components. Using a real world dataset, we demonstrate that DIMSIM effectively improves MRR by 7.5X , recall by 1.5X and precision by 1.4X over existing approaches.\nThe original motivation for this work was to improve the quality of downstream NLP tasks, such as named entity identification, text normalization and spelling correction. These tasks all share a dependency on reliable phonetic similarity as an intermediate step, especially for languages such as Chinese where incorrect homophones and synophones abound. We therefore plan to extend this line of work by applying DIMSIM to downstream applications, such as text normalization."
            }
        ],
        "references": [
            {
                "title": "The Soundex Indexing System",
                "author": [
                    "National Archives",
                    "Records Administration."
                ],
                "venue": "https://www.archives.gov/research/ census/soundex.html.",
                "citeRegEx": "Archives and Administration.,? 2007",
                "shortCiteRegEx": "Archives and Administration.",
                "year": 2007
            },
            {
                "title": "An Introduction to Double Metaphone and the Principles Behind Soundex",
                "author": [
                    "Adam Carstensen."
                ],
                "venue": "http://www.b-eye-network. com/view/1596.",
                "citeRegEx": "Carstensen.,? 2005",
                "shortCiteRegEx": "Carstensen.",
                "year": 2005
            },
            {
                "title": "CC-CEDICT",
                "author": [
                    "CEDict."
                ],
                "venue": "https: //www.mdbg.net/chindict/chindict. php?page=cc-cedict.",
                "citeRegEx": "CEDict.,? 2016",
                "shortCiteRegEx": "CEDict.",
                "year": 2016
            },
            {
                "title": "Automatic english-chinese name translation by using webmining and phonetic similarity",
                "author": [
                    "Jen-Ming Chung",
                    "Fu-Yuan Hsu",
                    "Cheng-Yu Lu",
                    "HahnMing Lee",
                    "Jan-Ming Ho."
                ],
                "venue": "IEEE International Conference on Information Reuse and Inte-",
                "citeRegEx": "Chung et al\\.,? 2011",
                "shortCiteRegEx": "Chung et al\\.",
                "year": 2011
            },
            {
                "title": "A joint learning model of word segmentation, lexical acquisition, and phonetic variability",
                "author": [
                    "Micha Elsner",
                    "Sharon Goldwater",
                    "Naomi Feldman",
                    "Frank Wood."
                ],
                "venue": "Proc. EMNLP.",
                "citeRegEx": "Elsner et al\\.,? 2013",
                "shortCiteRegEx": "Elsner et al\\.",
                "year": 2013
            },
            {
                "title": "Automatically constructing a normalisation dictionary for microblogs",
                "author": [
                    "Bo Han",
                    "Paul Cook",
                    "Timothy Baldwin."
                ],
                "venue": "Proc. of the joint conference on EMNLP and CoNLL, pages 421\u2013432. ACL.",
                "citeRegEx": "Han et al\\.,? 2012",
                "shortCiteRegEx": "Han et al\\.",
                "year": 2012
            },
            {
                "title": "Answering the call for a standard reliability measure for coding data",
                "author": [
                    "Andrew F. Hayes",
                    "Klaus Krippendorff."
                ],
                "venue": "Communication Methods and Measures, 1(1):77\u201389.",
                "citeRegEx": "Hayes and Krippendorff.,? 2007",
                "shortCiteRegEx": "Hayes and Krippendorff.",
                "year": 2007
            },
            {
                "title": "ISO 7098: Romanization of Chinese",
                "author": [
                    "ISO."
                ],
                "venue": "https://www.iso.org/standard/ 61420.html.",
                "citeRegEx": "ISO.,? 2015",
                "shortCiteRegEx": "ISO.",
                "year": 2015
            },
            {
                "title": "Phonetic comparison algorithms",
                "author": [
                    "Brett Kessler."
                ],
                "venue": "Transactions of the Philological Society, 103(2):243\u2013260.",
                "citeRegEx": "Kessler.,? 2005",
                "shortCiteRegEx": "Kessler.",
                "year": 2005
            },
            {
                "title": "Phonetic alignment and similarity",
                "author": [
                    "Grzegorz Kondrak."
                ],
                "venue": "Computers and the Humanities, 37(3):273\u2013 291.",
                "citeRegEx": "Kondrak.,? 2003",
                "shortCiteRegEx": "Kondrak.",
                "year": 2003
            },
            {
                "title": "A phonetic similarity model for automatic extraction of transliteration pairs",
                "author": [
                    "Jin-Shea Kuo",
                    "Haizhou Li",
                    "Ying-Kuei Yang."
                ],
                "venue": "ACM Transactions on Asian Language Information Processing (TALIP), 6(2):6.",
                "citeRegEx": "Kuo et al\\.,? 2007",
                "shortCiteRegEx": "Kuo et al\\.",
                "year": 2007
            },
            {
                "title": "The measurement of phonetic similarity",
                "author": [
                    "Peter Ladefoged."
                ],
                "venue": "Proceedings of the 1969 conference on Computational linguistics, pages 1\u201314. Association for Computational Linguistics.",
                "citeRegEx": "Ladefoged.,? 1969",
                "shortCiteRegEx": "Ladefoged.",
                "year": 1969
            },
            {
                "title": "Learning phonetic similarity for matching named entity translations and mining new translations",
                "author": [
                    "Wai Lam",
                    "Ruizhang Huang",
                    "Pik-Shan Cheung."
                ],
                "venue": "Proc. ACM SIGIR, pages 289\u2013296.",
                "citeRegEx": "Lam et al\\.,? 2004",
                "shortCiteRegEx": "Lam et al\\.",
                "year": 2004
            },
            {
                "title": "Joint learning of phonetic units and word pronunciations for asr",
                "author": [
                    "Chia-ying Lee",
                    "Yu Zhang",
                    "James R Glass."
                ],
                "venue": "EMNLp, pages 182\u2013192.",
                "citeRegEx": "Lee et al\\.,? 2013",
                "shortCiteRegEx": "Lee et al\\.",
                "year": 2013
            },
            {
                "title": "An kind of chinese text strings\u2019 similarity and its application in speech recognition",
                "author": [
                    "Hong-lian Li",
                    "Wei He",
                    "Bao-zong Yuan."
                ],
                "venue": "Journal of Chinese Information Processing, 17(1):60\u201364.",
                "citeRegEx": "Li et al\\.,? 2003",
                "shortCiteRegEx": "Li et al\\.",
                "year": 2003
            },
            {
                "title": "Backward machine transliteration by learning phonetic similarity",
                "author": [
                    "Wei-Hao Lin",
                    "Hsin-Hsi Chen."
                ],
                "venue": "Proc. CoNLL, pages 1\u20137. ACL.",
                "citeRegEx": "Lin and Chen.,? 2002",
                "shortCiteRegEx": "Lin and Chen.",
                "year": 2002
            },
            {
                "title": "Phone clustering using the bhattacharyya distance",
                "author": [
                    "Brian Mak",
                    "Etienne Barnard."
                ],
                "venue": "Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on, volume 4, pages 2005\u2013 2008. IEEE.",
                "citeRegEx": "Mak and Barnard.,? 1996",
                "shortCiteRegEx": "Mak and Barnard.",
                "year": 1996
            },
            {
                "title": "Soundexing and genealogy",
                "author": [
                    "Gary Mokotoff."
                ],
                "venue": "Avotaynu.",
                "citeRegEx": "Mokotoff.,? 1997",
                "shortCiteRegEx": "Mokotoff.",
                "year": 1997
            },
            {
                "title": "A guided tour to approximate string matching",
                "author": [
                    "Gonzalo Navarro."
                ],
                "venue": "ACM computing surveys (CSUR), 33(1):31\u201388.",
                "citeRegEx": "Navarro.,? 2001",
                "shortCiteRegEx": "Navarro.",
                "year": 2001
            },
            {
                "title": "Measuring dialect distance phonetically",
                "author": [
                    "John Nerbonne",
                    "Wilbert Heeringa."
                ],
                "venue": "Proceedings of the Third Meeting of the ACL Special Interest Group in Computational Phonology (SIGPHON-97).",
                "citeRegEx": "Nerbonne and Heeringa.,? 1997",
                "shortCiteRegEx": "Nerbonne and Heeringa.",
                "year": 1997
            },
            {
                "title": "Hanging on the metaphone",
                "author": [
                    "Lawrence Philips."
                ],
                "venue": "Computer Language, 7(2):6.",
                "citeRegEx": "Philips.,? 1990",
                "shortCiteRegEx": "Philips.",
                "year": 1990
            },
            {
                "title": "The double metaphone search algorithm",
                "author": [
                    "Lawrence Philips."
                ],
                "venue": "C/C++ users journal, 18(6):38\u201343.",
                "citeRegEx": "Philips.,? 2000",
                "shortCiteRegEx": "Philips.",
                "year": 2000
            },
            {
                "title": "A transition-based model for joint segmentation, pos-tagging and normalization",
                "author": [
                    "Tao Qian",
                    "Yue Zhang",
                    "Meishan Zhang",
                    "Yafeng Ren",
                    "Dong-Hong Ji."
                ],
                "venue": "EMNLP, pages 1837\u20131846.",
                "citeRegEx": "Qian et al\\.,? 2015",
                "shortCiteRegEx": "Qian et al\\.",
                "year": 2015
            },
            {
                "title": "The Phonology of Standard Chinese",
                "author": [
                    "Duanmu San."
                ],
                "venue": "Oxford University Press.",
                "citeRegEx": "San.,? 2007",
                "shortCiteRegEx": "San.",
                "year": 2007
            },
            {
                "title": "A graphbased approach for contextual text normalization",
                "author": [
                    "Cagil Sonmez",
                    "Arzucan Ozgur."
                ],
                "venue": "EMNLP, pages 313\u2013324.",
                "citeRegEx": "Sonmez and Ozgur.,? 2014",
                "shortCiteRegEx": "Sonmez and Ozgur.",
                "year": 2014
            },
            {
                "title": "A sub-character architecture for korean language processing",
                "author": [
                    "Karl Stratos."
                ],
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 721\u2013726.",
                "citeRegEx": "Stratos.,? 2017",
                "shortCiteRegEx": "Stratos.",
                "year": 2017
            },
            {
                "title": "Name search techniques",
                "author": [
                    "Robert L Taft."
                ],
                "venue": "1. Bureau of Systems Development, New York State Identification and Intelligence System.",
                "citeRegEx": "Taft.,? 1970",
                "shortCiteRegEx": "Taft.",
                "year": 1970
            },
            {
                "title": "Pronunciation modeling for improved spelling correction",
                "author": [
                    "Kristina Toutanova",
                    "Robert C. Moore."
                ],
                "venue": "Proc. ACL, ACL \u201902, pages 144\u2013151, Stroudsburg, PA, USA.",
                "citeRegEx": "Toutanova and Moore.,? 2002",
                "shortCiteRegEx": "Toutanova and Moore.",
                "year": 2002
            },
            {
                "title": "Improving domainindependent cloud-based speech recognition with domain-dependent phonetic post-processing",
                "author": [
                    "Johannes Twiefel",
                    "Timo Baumann",
                    "Stefan Heinrich",
                    "Stefan Wermter."
                ],
                "venue": "AAAI, pages 1529\u20131536.",
                "citeRegEx": "Twiefel et al\\.,? 2014",
                "shortCiteRegEx": "Twiefel et al\\.",
                "year": 2014
            },
            {
                "title": "The trec-8 question answering track report",
                "author": [
                    "Ellen M Voorhees"
                ],
                "venue": "Trec, volume 99, pages 77\u201382.",
                "citeRegEx": "Voorhees,? 1999",
                "shortCiteRegEx": "Voorhees",
                "year": 1999
            },
            {
                "title": "Commonly used phonetic vocabulary",
                "author": [
                    "You Wu."
                ],
                "venue": "http://www.51wendang.com/doc/ 97585e99067d692a1bbaec92.",
                "citeRegEx": "Wu.,? 2016",
                "shortCiteRegEx": "Wu.",
                "year": 2016
            },
            {
                "title": "A phonetic-based approach to chinese chat text normalization",
                "author": [
                    "Yunqing Xia",
                    "Kam-Fai Wong",
                    "Wenjie Li."
                ],
                "venue": "Proc. ACL, pages 993\u20131000.",
                "citeRegEx": "Xia et al\\.,? 2006",
                "shortCiteRegEx": "Xia et al\\.",
                "year": 2006
            },
            {
                "title": "An algorithm for Chinese Similarity based on phonetic grapheme coding",
                "author": [
                    "Mabus Yao."
                ],
                "venue": "http: //mabusyao.iteye.com/blog/2267661.",
                "citeRegEx": "Yao.,? 2015",
                "shortCiteRegEx": "Yao.",
                "year": 2015
            },
            {
                "title": "Differentiating f and h",
                "author": [
                    "Zhishihao."
                ],
                "venue": "http:// www.zhishihao.com/xue/show/51763.",
                "citeRegEx": "Zhishihao.,? 2017",
                "shortCiteRegEx": "Zhishihao.",
                "year": 2017
            }
        ],
        "abstractText": "Phonetic similarity algorithms identify words and phrases with similar pronunciation which are used in many natural language processing tasks. However, existing approaches are designed mainly for Indo-European languages and fail to capture the unique properties of Chinese pronunciation. In this paper, we propose a high dimensional encoded phonetic similarity algorithm for Chinese, DIMSIM. The encodings are learned from annotated data to separately map initial and final phonemes into n-dimensional coordinates. Pinyin phonetic similarities are then calculated by aggregating the similarities of initial, final and tone. DIMSIM demonstrates a 7.5X improvement on mean reciprocal rank over the state-of-theart phonetic similarity approaches."
    },
    {
        "title": "Improved Neural Relation Detection for Knowledge Base Question Answering",
        "sections": [
            {
                "heading": null,
                "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 571\u2013581 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1053"
            },
            {
                "heading": "1 Introduction",
                "text": "Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al.,"
            },
            {
                "heading": "2014; Bordes et al., 2015; Bast and Haussmann,",
                "text": ""
            },
            {
                "heading": "2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a",
                "text": "KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single <head-entity, relation, tail-entity> KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled\nfor multiple entities in the question. The KBQA system in the figure performs two key tasks: (1) entity linking, which links n-grams in questions to KB entities, and (2) relation detection, which identifies the KB relation(s) a question refers to.\nThe main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the KBQA system. Although general relation detection1 methods are well studied in the NLP community, such studies usually do not take the end task of KBQA into consideration. As a result, there is a significant gap between general relation detection studies and KB-specific relation detection. First, in most general relation detection tasks, the number of target relations is limited, normally smaller than 100. In contrast, in KBQA even a small KB, like Freebase2M (Bordes et al., 2015), contains more than 6,000 relation types. Second, relation detection for KBQA often becomes a zero-shot learning task, since some test instances may have unseen relations in the training data. For example, the SimpleQuestions (Bordes et al., 2015) data set has 14% of the golden test relations not observed in golden training tuples. Third, as shown in Figure 1(b), for some KBQA tasks like WebQuestions (Berant et al., 2013), we need to predict a chain of relations instead of a single relation. This increases the number of target relation types and the sizes of candidate relation pools, further increasing the difficulty of KB relation detection. Owing to these reasons, KB relation detection is significantly more challenging compared to general relation detection tasks.\nThis paper improves KB relation detection to cope with the problems mentioned above. First, in order to deal with the unseen relations, we propose to break the relation names into word sequences for question-relation matching. Second, noticing\n1In the information extraction field such tasks are usually called relation extraction or relation classification.\n571"
            },
            {
                "heading": "Mike Kelley",
                "text": ""
            },
            {
                "heading": "Mike Kelley",
                "text": "that original relation names can sometimes help to match longer question contexts, we propose to build both relation-level and word-level relation representations. Third, we use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Finally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improves hierarchical matching.\nIn order to assess how the proposed improved relation detection could benefit the KBQA end task, we also propose a simple KBQA implementation composed of two-step relation detection. Given an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the KBQA process: (1) Re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model. This step is important to deal with the ambiguities normally present in entity linking results. (2) Finding the core relation (chains) for each topic entity2 selection from a much smaller candidate entity set after re-ranking. The above steps are followed by an optional constraint detection step, when the question cannot be answered by single relations (e.g., multiple entities in the question). Finally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node.\nsteps is used to query the KB for answers. Our main contributions include: (i) An improved relation detection model by hierarchical matching between questions and relations with residual learning; (ii) We demonstrate that the improved relation detector enables our simple KBQA system to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks."
            },
            {
                "heading": "2 Related Work",
                "text": "Relation Extraction Relation extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016).\nThe above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC-\nKBP2015 has 74 relations although it considers open-domain Wikipedia relations. All are much fewer than thousands of relations in KBQA. As a result, few work in this field focuses on dealing with large number of relations or unseen relations. Yu et al. (2016) proposed to use relation embeddings in a low-rank tensor method. However their relation embeddings are still trained in supervised way and the number of relations is not large in the experiments.\nRelation Detection in KBQA Systems Relation detection for KBQA also starts with featurerich approaches (Yao and Van Durme, 2014; Bast and Haussmann, 2015) towards usages of deep networks (Yih et al., 2015; Xu et al., 2016; Dai et al., 2016) and attention models (Yin et al., 2016; Golub and He, 2016). Many of the above relation detection research could naturally support large relation vocabulary and open relation sets (especially for QA with OpenIE KB like ParaLex (Fader et al., 2013)), in order to fit the goal of open-domain question answering.\nDifferent KBQA data sets have different levels of requirement about the above open-domain capacity. For example, most of the gold test relations in WebQuestions can be observed during training, thus some prior work on this task adopted the close domain assumption like in the general RE research. While for data sets like SimpleQuestions and ParaLex, the capacity to support large relation sets and unseen relations becomes more necessary. To the end, there are two main solutions: (1) use pre-trained relation embeddings (e.g. from TransE (Bordes et al., 2013)), like (Dai et al., 2016); (2) factorize the relation names to sequences and formulate relation detection as a sequence matching and ranking task. Such factorization works because that the relation names usually comprise meaningful word sequences. For example, Yin et al. (2016) split relations to word sequences for single-relation detection. Liang et al. (2016) also achieve good performance on WebQSP with wordlevel relation representation in an end-to-end neural programmer model. Yih et al. (2015) use character tri-grams as inputs on both question and relation sides. Golub and He (2016) propose a generative framework for single-relation KBQA which predicts relation with a character-level sequenceto-sequence model.\nAnother difference between relation detection in KBQA and general RE is that general RE re-\nsearch assumes that the two argument entities are both available. Thus it usually benefits from features (Nguyen and Grishman, 2014; Gormley et al., 2015) or attention mechanisms (Wang et al., 2016) based on the entity information (e.g. entity types or entity embeddings). For relation detection in KBQA, such information is mostly missing because: (1) one question usually contains single argument (the topic entity) and (2) one KB entity could have multiple types (type vocabulary size larger than 1,500). This makes KB entity typing itself a difficult problem so no previous used entity information in the relation detection model.3"
            },
            {
                "heading": "3 Background: Different Granularity in",
                "text": "KB Relations\nPrevious research (Yih et al., 2015; Yin et al., 2016) formulates KB relation detection as a sequence matching problem. However, while the questions are natural word sequences, how to represent relations as sequences remains a challenging problem. Here we give an overview of two types of relation sequence representations commonly used in previous work.\n(1) Relation Name as a Single Token (relationlevel). In this case, each relation name is treated as a unique token. The problem with this approach is that it suffers from the low relation coverage due to limited amount of training data, thus cannot generalize well to large number of opendomain relations. For example, in Figure 1, when treating relation names as single tokens, it will be difficult to match the questions to relation names \u201cepisodes written\u201d and \u201cstarring roles\u201d if these names do not appear in training data \u2013 their relation embeddings hrs will be random vectors thus are not comparable to question embeddings hqs.\n(2) Relation as Word Sequence (word-level). In this case, the relation is treated as a sequence of words from the tokenized relation name. It has better generalization, but suffers from the lack of global information from the original relation names. For example in Figure 1(b), when doing only word-level matching, it is difficult to rank the target relation \u201cstarring roles\u201d higher compared to the incorrect relation \u201cplays produced\u201d. This is because the incorrect relation contains word \u201cplays\u201d, which is more similar to the question\n3Such entity information has been used in KBQA systems as features for the final answer re-rankers.\n(containing word \u201cplay\u201d) in the embedding space. On the other hand, if the target relation co-occurs with questions related to \u201ctv appearance\u201d in training, by treating the whole relation as a token (i.e. relation id), we could better learn the correspondence between this token and phrases like \u201ctv show\u201d and \u201cplay on\u201d.\nThe two types of relation representation contain different levels of abstraction. As shown in Table 1, the word-level focuses more on local information (words and short phrases), and the relation-level focus more on global information (long phrases and skip-grams) but suffer from data sparsity. Since both these levels of granularity have their own pros and cons, we propose a hierarchical matching approach for KB relation detection: for a candidate relation, our approach matches the input question to both word-level and relation-level representations to get the final ranking score. Section 4 gives the details of our proposed approach."
            },
            {
                "heading": "4 Improved KB Relation Detection",
                "text": "This section describes our hierarchical sequence matching with residual learning approach for relation detection. In order to match the question to different aspects of a relation (with different abstraction levels), we deal with three problems as follows on learning question/relation representations."
            },
            {
                "heading": "4.1 Relation Representations from Different Granularity",
                "text": "We provide our model with both types of relation representation: word-level and relationlevel. Therefore, the input relation becomes r = {rword1 , \u00b7 \u00b7 \u00b7 , rwordM1 } [ {rrel1 , \u00b7 \u00b7 \u00b7 , rrelM2}, where the first M1 tokens are words (e.g. {episode, written}), and the last M2 tokens are relation names, e.g., {episode written} or {starring roles, series} (when the target is a chain like in Figure 1(b)). We transform each token above to its word embed-\nding then use two BiLSTMs (with shared parameters) to get their hidden representations [Bword1:M1 : Brel1:M2 ] (each row vector i is the concatenation between forward/backward representations at i). We initialize the relation sequence LSTMs with the final state representations of the word sequence, as a back-off for unseen relations. We apply one max-pooling on these two sets of vectors and get the final relation representation hr."
            },
            {
                "heading": "4.2 Different Abstractions of Questions Representations",
                "text": "From Table 1, we can see that different parts of a relation could match different contexts of question texts. Usually relation names could match longer phrases in the question and relation words could match short phrases. Yet different words might match phrases of different lengths.\nAs a result, we hope the question representations could also comprise vectors that summarize various lengths of phrase information (different levels of abstraction), in order to match relation representations of different granularity. We deal with this problem by applying deep BiLSTMs on questions. The first-layer of BiLSTM works on the word embeddings of question words q = {q1, \u00b7 \u00b7 \u00b7 , qN} and gets hidden representations\n(1) 1:N = [ (1) 1 ; \u00b7 \u00b7 \u00b7 ; (1) N ]. The second-layer BiLSTM works on (1)1:N to get the second set of hidden representations (2)1:N . Since the second BiLSTM starts with the hidden vectors from the first layer, intuitively it could learn more general and abstract information compared to the first layer.\nNote that the first(second)-layer of question representations does not necessarily correspond to the word(relation)-level relation representations, instead either layer of question representations could potentially match to either level of relation representations. This raises the difficulty of matching between different levels of relation/question representations; the following section gives our proposal to deal with such problem."
            },
            {
                "heading": "4.3 Hierarchical Matching between Relation and Question",
                "text": "Now we have question contexts of different lengths encoded in (1)1:N and (2) 1:N . Unlike the standard usage of deep BiLSTMs that employs the representations in the final layer for prediction, here we expect that two layers of question representations can be complementary to each other and both should be compared to the relation representation space (Hierarchical Matching). This is important for our task since each relation token can correspond to phrases of different lengths, mainly because of syntactic variations. For example in Table 1, the relation word written could be matched to either the same single word in the question or a much longer phrase be the writer of.\nWe could perform the above hierarchical matching by computing the similarity between each layer of and hr separately and doing the (weighted) sum between the two scores. However this does not give significant improvement (see Table 2). Our analysis in Section 6.2 shows that this naive method suffers from the training difficulty, evidenced by that the converged training loss of this model is much higher than that of a single-layer baseline model. This is mainly because (1) Deep BiLSTMs do not guarantee that the two-levels of question hidden representations are comparable, the training usually falls to local optima where one layer has good matching scores and the other always has weight close to 0. (2)\nThe training of deeper architectures itself is more difficult.\nTo overcome the above difficulties, we adopt the idea from Residual Networks (He et al., 2016) for hierarchical matching by adding shortcut connections between two BiLSTM layers. We proposed two ways of such Hierarchical Residual Matching: (1) Connecting each (1)i and (2) i , resulting in a 0 i = (1) i + (2) i for each position i. Then the final question representation hq becomes a maxpooling over all\n0 is, 1iN . (2) Applying max-\npooling on (1)1:N and (2) 1:N to get h (1) max and h (2) max, respectively, then setting hq = h(1)max + h (2) max. Finally we compute the matching score of r given q as srel(r;q) = cos(hr,hq).\nIntuitively, the proposed method should benefit from hierarchical training since the second layer is fitting the residues from the first layer of matching, so the two layers of representations are more likely to be complementary to each other. This also ensures the vector spaces of two layers are comparable and makes the second-layer training easier.\nDuring training we adopt a ranking loss to maximizing the margin between the gold relation r+ and other relations r in the candidate pool R.\nlrel = max{0, srel(r+;q) + srel(r ;q)} where is a constant parameter. Fig 2 summarizes the above Hierarchical Residual BiLSTM (HR-BiLSTM) model.\nRemark: Another way of hierarchical matching consists in relying on attention mechanism, e.g. (Parikh et al., 2016), to find the correspondence between different levels of representations. This performs below the HR-BiLSTM (see Table 2)."
            },
            {
                "heading": "5 KBQA Enhanced by Relation Detection",
                "text": "This section describes our KBQA pipeline system. We make minimal efforts beyond the training of the relation detection model, making the whole system easy to build.\nFollowing previous work (Yih et al., 2015; Xu et al., 2016), our KBQA system takes an existing entity linker to produce the top-K linked entities, ELK(q), for a question q (\u201cinitial entity linking\u201d). Then we generate the KB queries for q following the four steps illustrated in Algorithm 1.\nAlgorithm 1: KBQA with two-step relation detection Input : Question q, Knowledge Base KB, the initial\ntop-K entity candidates ELK(q) Output: Top query tuple (e\u0302, r\u0302, {(c, rc)})\n1 Entity Re-Ranking (first-step relation detection): Use the raw question text as input for a relation detector to score all relations in the KB that are associated to the entities in ELK(q); use the relation scores to re-rank ELK(q) and generate a shorter list EL0K0(q) containing the top-K0 entity candidates (Section 5.1) 2 Relation Detection: Detect relation(s) using the reformatted question text in which the topic entity is replaced by a special token <e> (Section 5.2) 3 Query Generation: Combine the scores from step 1 and 2, and select the top pair (e\u0302, r\u0302) (Section 5.3) 4 Constraint Detection (optional): Compute similarity between q and any neighbor entity c of the entities along r\u0302 (connecting by a relation rc) , add the high scoring c and rc to the query (Section 5.4).\nCompared to previous approaches, the main difference is that we have an additional entity reranking step after the initial entity linking. We have this step because we have observed that entity linking sometimes becomes a bottleneck in KBQA systems. For example, on SimpleQuestions the best reported linker could only get 72.7% top-1 accuracy on identifying topic entities. This is usually due to the ambiguities of entity names, e.g. in Fig 1(a), there are TV writer and baseball player \u201cMike Kelley\u201d, which is impossible to distinguish with only entity name matching.\nHaving observed that different entity candidates usually connect to different relations, here we propose to help entity disambiguation in the initial entity linking with relations detected in questions.\nSections 5.1 and 5.2 elaborate how our relation detection help to re-rank entities in the initial entity linking, and then those re-ranked entities enable more accurate relation detection. The KBQA end task, as a result, benefits from this process."
            },
            {
                "heading": "5.1 Entity Re-Ranking",
                "text": "In this step, we use the raw question text as input for a relation detector to score all relations in the KB with connections to at least one of the entity candidates in ELK(q). We call this step relation detection on entity set since it does not work on a single topic entity as the usual settings. We use the HR-BiLSTM as described in Sec. 4. For each question q, after generating a score srel(r; q) for each relation using HR-BiLSTM, we use the top l best scoring relations (Rlq) to re-rank the original entity candidates. Concretely, for each entity e and its associated relations Re, given the original entity linker score slinker, and the score of the most confident relation r 2 Rlq\\Re, we sum these two scores to re-rank the entities:\nsrerank(e; q) =\u21b5 \u00b7 slinker(e; q) +(1 \u21b5) \u00b7 max\nr2Rlq\\Re srel(r; q).\nFinally, we select top K 0 < K entities according to score srerank to form the re-ranked list EL 0 K0(q).\nWe use the same example in Fig 1(a) to illustrate the idea. Given the input question in the example, a relation detector is very likely to assign high scores to relations such as \u201cepisodes written\u201d, \u201cauthor of \u201d and \u201cprofession\u201d. Then, according to the connections of entity candidates in KB, we find that the TV writer \u201cMike Kelley\u201d will be scored higher than the baseball player \u201cMike Kelley\u201d, because the former has the relations \u201cepisodes written\u201d and \u201cprofession\u201d. This method can be viewed as exploiting entity-relation collocation for entity linking."
            },
            {
                "heading": "5.2 Relation Detection",
                "text": "In this step, for each candidate entity e 2 EL0K(q), we use the question text as the input to a relation detector to score all the relations r 2 Re that are associated to the entity e in the KB.4 Because we have a single topic entity input in this step, we do the following question reformatting: we replace the the candidate e\u2019s entity mention in\n4Note that the number of entities and the number of relation candidates will be much smaller than those in the previous step.\nq with a token \u201c<e>\u201d. This helps the model better distinguish the relative position of each word compared to the entity. We use the HR-BiLSTM model to predict the score of each relation r 2 Re: srel(r; e, q)."
            },
            {
                "heading": "5.3 Query Generation",
                "text": "Finally, the system outputs the <entity, relation (or core-chain)> pair (e\u0302, r\u0302) according to:\ns(e\u0302, r\u0302; q) = max e2EL0 K0 (q),r2Re ( \u00b7 srerank(e; q)\n+(1 ) \u00b7 srel(r; e, q)) , where is a hyperparameter to be tuned."
            },
            {
                "heading": "5.4 Constraint Detection",
                "text": "Similar to (Yih et al., 2015), we adopt an additional constraint detection step based on text matching. Our method can be viewed as entitylinking on a KB sub-graph. It contains two steps: (1) Sub-graph generation: given the top scored query generated by the previous 3 steps5, for each node v (answer node or the CVT node like in Figure 1(b)), we collect all the nodes c connecting to v (with relation rc) with any relation, and generate a sub-graph associated to the original query. (2) Entity-linking on sub-graph nodes: we compute a matching score between each n-gram in the input question (without overlapping the topic entity) and entity name of c (except for the node in the original query) by taking into account the maximum overlapping sequence of characters between them (see Appendix A for details and B for special rules dealing with date/answer type constraints). If the matching score is larger than a threshold \u2713 (tuned on training set), we will add the constraint entity c (and rc) to the query by attaching it to the corresponding node v on the core-chain."
            },
            {
                "heading": "6 Experiments",
                "text": ""
            },
            {
                "heading": "6.1 Task Introduction & Settings",
                "text": "We use the SimpleQuestions (Bordes et al., 2015) and WebQSP (Yih et al., 2016) datasets. Each question in these datasets is labeled with the gold semantic parse. Hence we can directly evaluate relation detection performance independently as well as evaluate on the KBQA end task.\n5Starting with the top-1 query suffers more from error propagation. However we still achieve state-of-the-art on WebQSP in Sec.6, showing the advantage of our relation detection model. We leave in future work beam-search and feature extraction on beam for final answer re-ranking like in previous research.\nSimpleQuestions (SQ): It is a single-relation KBQA task. The KB we use consists of a Freebase subset with 2M entities (FB2M) (Bordes et al., 2015), in order to compare with previous research. Yin et al. (2016) also evaluated their relation extractor on this data set and released their proposed question-relation pairs, so we run our relation detection model on their data set. For the KBQA evaluation, we also start with their entity linking results6. Therefore, our results can be compared with their reported results on both tasks. WebQSP (WQ): A multi-relation KBQA task. We use the entire Freebase KB for evaluation purposes. Following Yih et al. (2016), we use S-MART (Yang and Chang, 2015) entity-linking outputs.7 In order to evaluate the relation detection models, we create a new relation detection task from the WebQSP data set.8 For each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length  2) connected to the topic entity, and set the corechain labeled in the parse as the positive label and all the others as the negative examples.\nWe tune the following hyper-parameters on development sets: (1) the size of hidden states for LSTMs ({50, 100, 200, 400})9; (2) learning rate ({0.1, 0.5, 1.0, 2.0}); (3) whether the shortcut connections are between hidden states or between max-pooling results (see Section 4.3); and (4) the number of training epochs.\nFor both the relation detection experiments and the second-step relation detection in KBQA, we have entity replacement first (see Section 5.2 and Figure 1). All word vectors are initialized with 300-d pretrained word embeddings (Mikolov et al., 2013). The embeddings of relation names are randomly initialized, since existing pre-trained relation embeddings (e.g. TransE) usually support limited sets of relation names. We leave the usage of pre-trained relation embeddings to future work."
            },
            {
                "heading": "6.2 Relation Detection Results",
                "text": "Table 2 shows the results on two relation detection tasks. The AMPCNN result is from (Yin et al., 2016), which yielded state-of-the-art scores by outperforming several attention-based meth-\n6The two resources have been downloaded from https: //github.com/Gorov/SimpleQuestions-EntityLinking\n7https://github.com/scottyih/STAGG 8The dataset is available at https://github.com/Gorov/\nSimpleQuestions-EntityLinking. 9For CNNs we double the size for fair comparison.\nods. We re-implemented the BiCNN model from (Yih et al., 2015), where both questions and relations are represented with the word hash trick on character tri-grams. The baseline BiLSTM with relation word sequence appears to be the best baseline on WebQSP and is close to the previous best result of AMPCNN on SimpleQuestions. Our proposed HR-BiLSTM outperformed the best baselines on both tasks by margins of 2-3% (p < 0.001 and 0.01 compared to the best baseline BiLSTM w/ words on SQ and WQ respectively).\nNote that using only relation names instead of words results in a weaker baseline BiLSTM model. The model yields a significant performance drop on SimpleQuestions (91.2% to 88.9%). However, the drop is much smaller on WebQSP, and it suggests that unseen relations have a much bigger impact on SimpleQuestions.\nAblation Test: The bottom of Table 2 shows ablation results of the proposed HR-BiLSTM. First, hierarchical matching between questions and both relation names and relation words yields improvement on both datasets, especially for SimpleQuestions (93.3% vs. 91.2/88.8%). Second, residual learning helps hierarchical matching compared to weighted-sum and attention-based baselines (see Section 4.3). For the attention-based baseline, we tried the model from (Parikh et al., 2016) and its one-way variations, where the one-way model gives better results10. Note that residual learning significantly helps on WebQSP (80.65% to\n10We also tried to apply the same attention method on deep BiLSTM with residual connections, but it does not lead to better results compared to HR-BiLSTM. We hypothesize that the idea of hierarchical matching with attention mechanism may work better for long sequences, and the new advanced attention mechanisms (Wang and Jiang, 2016; Wang et al., 2017) might help hierarchical matching. We leave the above directions to future work.\n82.53%), while it does not help as much on SimpleQuestions. On SimpleQuestions, even removing the deep layers only causes a small drop in performance. WebQSP benefits more from residual and deeper architecture, possibly because in this dataset it is more important to handle larger scope of context matching.\nFinally, on WebQSP, replacing BiLSTM with CNN in our hierarchical matching framework results in a large performance drop. Yet on SimpleQuestions the gap is much smaller. We believe this is because the LSTM relation encoder can better learn the composition of chains of relations in WebQSP, as it is better at dealing with longer dependencies.\nAnalysis Next, we present empirical evidences, which show why our HR-BiLSTM model achieves the best scores. We use WebQSP for the analysis purposes. First, we have the hypothesis that training of the weighted-sum model usually falls to local optima, since deep BiLSTMs do not guarantee that the two-levels of question hidden representations are comparable. This is evidenced by that during training one layer usually gets a weight close to 0 thus is ignored. For example, one run gives us weights of -75.39/0.14 for the two layers (we take exponential for the final weighted sum). It also gives much lower training accuracy (91.94%) compared to HR-BiLSTM (95.67%), suffering from training difficulty.\nSecond, compared to our deep BiLSTM with shortcut connections, we have the hypothesis that for KB relation detection, training deep BiLSTMs is more difficult without shortcut connections. Our experiments suggest that deeper BiLSTM does not always result in lower training accuracy. In the experiments a two-layer BiLSTM converges to 94.99%, even lower than the 95.25% achieved by a\nsingle-layer BiLSTM. Under our setting the twolayer model captures the single-layer model as a special case (so it could potentially better fit the training data), this result suggests that the deep BiLSTM without shortcut connections might suffers more from training difficulty.\nFinally, we hypothesize that HR-BiLSTM is more than combination of two BiLSTMs with residual connections, because it encourages the hierarchical architecture to learn different levels of abstraction. To verify this, we replace the deep BiLSTM question encoder with two single-layer BiLSTMs (both on words) with shortcut connections between their hidden states. This decreases test accuracy to 76.11%. It gives similar training accuracy compared to HR-BiLSTM, indicating a more serious over-fitting problem. This proves that the residual and deep structures both contribute to the good performance of HR-BiLSTM."
            },
            {
                "heading": "6.3 KBQA End-Task Results",
                "text": "Table 3 compares our system with two published baselines (1) STAGG (Yih et al., 2015), the stateof-the-art on WebQSP11 and (2) AMPCNN (Yin et al., 2016), the state-of-the-art on SimpleQuestions. Since these two baselines are specially designed/tuned for one particular dataset, they do not generalize well when applied to the other dataset. In order to highlight the effect of different relation detection models on the KBQA end-task, we also implemented another baseline that uses our KBQA system but replaces HR-BiLSTM with our implementation of AMPCNN (for SimpleQuestions) or the char-3-gram BiCNN (for WebQSP) relation detectors (second block in Table 3).\nCompared to the baseline relation detector (3rd row of results), our method, which includes an improved relation detector (HR-BiLSTM), improves the KBQA end task by 2-3% (4th row). Note that in contrast to previous KBQA systems, our system does not use joint-inference or feature-based re-ranking step, nevertheless it still achieves better or comparable results to the state-of-the-art.\nThe third block of the table details two ablation tests for the proposed components in our KBQA systems: (1) Removing the entity re-ranking step significantly decreases the scores. Since the reranking step relies on the relation detection models, this shows that our HR-BiLSTM model contributes to the good performance in multiple ways.\n11The STAGG score on SQ is from (Bao et al., 2016).\nAppendix C gives the detailed performance of the re-ranking step. (2) In contrast to the conclusion in (Yih et al., 2015), constraint detection is crucial for our system12. This is probably because our joint performance on topic entity and core-chain detection is more accurate (77.5% top-1 accuracy), leaving a huge potential (77.5% vs. 58.0%) for the constraint detection module to improve.\nFinally, like STAGG, which uses multiple relation detectors (see Yih et al. (2015) for the three models used), we also try to use the top-3 relation detectors from Section 6.2. As shown on the last row of Table 3, this gives a significant performance boost, resulting in a new state-of-the-art result on SimpleQuestions and a result comparable to the state-of-the-art on WebQSP."
            },
            {
                "heading": "7 Conclusion",
                "text": "KB relation detection is a key step in KBQA and is significantly different from general relation extraction tasks. We propose a novel KB relation detection model, HR-BiLSTM, that performs hierarchical matching between questions and KB relations. Our model outperforms the previous methods on KB relation detection tasks and allows our KBQA system to achieve state-of-the-arts. For future work, we will investigate the integration of our HR-BiLSTM into end-to-end systems. For example, our model could be integrated into the decoder in (Liang et al., 2016), to provide better sequence prediction. We will also investigate new emerging datasets like GraphQuestions (Su et al.,"
            },
            {
                "heading": "2016) and ComplexQuestions (Bao et al., 2016) to handle more characteristics of general QA.",
                "text": "12Note that another reason is that we are evaluating on accuracy here. When evaluating on F1 the gap will be smaller."
            },
            {
                "heading": "Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and",
                "text": "Tiejun Zhao. 2016. Constraint-based question answering with knowledge graph. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee, Osaka, Japan, pages 2503\u20132514.\nHannah Bast and Elmar Haussmann. 2015. More accurate question answering on freebase. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, pages 1431\u20131440."
            },
            {
                "heading": "Jonathan Berant, Andrew Chou, Roy Frostig, and Percy",
                "text": "Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle, Washington, USA, pages 1533\u2013 1544."
            },
            {
                "heading": "Antoine Bordes, Nicolas Usunier, Sumit Chopra, and",
                "text": "Jason Weston. 2015. Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075 ."
            },
            {
                "heading": "Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko.",
                "text": "2013. Translating embeddings for modeling multirelational data. In Advances in Neural Information Processing Systems. pages 2787\u20132795.\nZihang Dai, Lei Li, and Wei Xu. 2016. Cfo: Conditional focused neural question answering with largescale knowledge bases. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pages 800\u2013810."
            },
            {
                "heading": "Cicero dos Santos, Bing Xiang, and Bowen Zhou.",
                "text": "2015. Classifying relations by ranking with convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, Beijing, China, pages 626\u2013634."
            },
            {
                "heading": "Anthony Fader, Luke S Zettlemoyer, and Oren Etzioni.",
                "text": ""
            },
            {
                "heading": "2013. Paraphrase-driven learning for open question",
                "text": "answering. In ACL (1). Citeseer, pages 1608\u20131618."
            },
            {
                "heading": "David Golub and Xiaodong He. 2016. Character-level",
                "text": "question answering with attention. arXiv preprint arXiv:1604.00727 ."
            },
            {
                "heading": "Matthew R. Gormley, Mo Yu, and Mark Dredze. 2015.",
                "text": "Improved relation extraction with feature-rich compositional embedding models. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon, Portugal, pages 1774\u2013 1784."
            },
            {
                "heading": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian",
                "text": "Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 770\u2013778."
            },
            {
                "heading": "Chen Liang, Jonathan Berant, Quoc Le, Kenneth D",
                "text": "Forbus, and Ni Lao. 2016. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. arXiv preprint arXiv:1611.00020 .\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. pages 3111\u20133119.\nThien Huu Nguyen and Ralph Grishman. 2014. Employing word representations and regularization for domain adaptation of relation extraction. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Baltimore, Maryland, pages 68\u201374."
            },
            {
                "heading": "Ankur Parikh, Oscar Ta\u0308ckstro\u0308m, Dipanjan Das, and",
                "text": "Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 2249\u20132255.\nBryan Rink and Sanda Harabagiu. 2010. Utd: Classifying semantic relations by combining lexical and semantic resources. In Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics, Uppsala, Sweden, pages 256\u2013259."
            },
            {
                "heading": "Yu Su, Huan Sun, Brian Sadler, Mudhakar Srivatsa, Izzeddin Gur, Zenghui Yan, and Xifeng Yan.",
                "text": "2016. On generating characteristic-rich question sets for qa evaluation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 562\u2013572. https://aclweb.org/anthology/D16-1054."
            },
            {
                "heading": "Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.",
                "text": "Semi-supervised relation extraction with large-scale word clustering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Portland, Oregon, USA, pages 521\u2013529.\nNgoc Thang Vu, Heike Adel, Pankaj Gupta, and Hinrich Schu\u0308tze. 2016. Combining recurrent and convolutional neural networks for relation classification. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, San Diego, California, pages 534\u2013539."
            },
            {
                "heading": "Linlin Wang, Zhu Cao, Gerard de Melo, and Zhiyuan",
                "text": "Liu. 2016. Relation classification via multi-level attention cnns. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pages 1298\u20131307."
            },
            {
                "heading": "Shuohang Wang and Jing Jiang. 2016. Learning",
                "text": "natural language inference with lstm. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, San Diego, California, pages 1442\u20131451. http://www.aclweb.org/anthology/N16-1170."
            },
            {
                "heading": "Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.",
                "text": "Bilateral multi-perspective matching for natural language sentences. arXiv preprint arXiv:1702.03814 .\nKun Xu, Siva Reddy, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2016. Question answering on freebase via relation extraction and textual evidence. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pages 2326\u20132336."
            },
            {
                "heading": "Yi Yang and Ming-Wei Chang. 2015. S-mart: Novel tree-based structured learning algorithms applied to",
                "text": "tweet entity linking. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, Beijing, China, pages 504\u2013513."
            },
            {
                "heading": "Xuchen Yao, Jonathan Berant, and Benjamin",
                "text": "Van Durme. 2014. Freebase qa: Information extraction or semantic parsing? ACL 2014 page 82.\nXuchen Yao and Benjamin Van Durme. 2014. Information extraction over structured data: Question answering with freebase. In ACL (1). Citeseer, pages 956\u2013966."
            },
            {
                "heading": "Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and",
                "text": "Jianfeng Gao. 2015. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Association for Computational Linguistics (ACL)."
            },
            {
                "heading": "Wen-tau Yih, Xiaodong He, and Christopher Meek.",
                "text": "2014. Semantic parsing for single-relation question answering. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Baltimore, Maryland, pages 643\u2013648.\nWen-tau Yih, Matthew Richardson, Chris Meek, MingWei Chang, and Jina Suh. 2016. The value of semantic parse labeling for knowledge base question\nanswering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Berlin, Germany, pages 201\u2013206."
            },
            {
                "heading": "Wenpeng Yin, Mo Yu, Bing Xiang, Bowen Zhou, and",
                "text": "Hinrich Schu\u0308tze. 2016. Simple question answering by attentive convolutional neural network. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee, Osaka, Japan, pages 1746\u20131756."
            },
            {
                "heading": "Mo Yu, Mark Dredze, Raman Arora, and Matthew R.",
                "text": "Gormley. 2016. Embedding lexical features via lowrank tensors. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, San Diego, California, pages 1019\u20131029. http://www.aclweb.org/anthology/N16-1117."
            },
            {
                "heading": "Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,",
                "text": "and Jun Zhao. 2014. Relation classification via convolutional deep neural network. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics, Dublin, Ireland, pages 2335\u2013 2344."
            },
            {
                "heading": "GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.",
                "text": "2005. Exploring various knowledge in relation extraction. In Association for Computational Linguistics. pages 427\u2013434."
            },
            {
                "heading": "Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen",
                "text": "Li, Hongwei Hao, and Bo Xu. 2016. Attentionbased bidirectional long short-term memory networks for relation classification. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Berlin, Germany, pages 207\u2013212."
            }
        ],
        "references": [
            {
                "title": "Constraint-based question answering with knowledge graph",
                "author": [
                    "Junwei Bao",
                    "Nan Duan",
                    "Zhao Yan",
                    "Ming Zhou",
                    "Tiejun Zhao."
                ],
                "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers.",
                "citeRegEx": "Bao et al\\.,? 2016",
                "shortCiteRegEx": "Bao et al\\.",
                "year": 2016
            },
            {
                "title": "More accurate question answering on freebase",
                "author": [
                    "Hannah Bast",
                    "Elmar Haussmann."
                ],
                "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, pages 1431\u20131440.",
                "citeRegEx": "Bast and Haussmann.,? 2015",
                "shortCiteRegEx": "Bast and Haussmann.",
                "year": 2015
            },
            {
                "title": "Semantic parsing on Freebase from question-answer pairs",
                "author": [
                    "Jonathan Berant",
                    "Andrew Chou",
                    "Roy Frostig",
                    "Percy Liang."
                ],
                "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational",
                "citeRegEx": "Berant et al\\.,? 2013",
                "shortCiteRegEx": "Berant et al\\.",
                "year": 2013
            },
            {
                "title": "Large-scale simple question answering with memory networks",
                "author": [
                    "Antoine Bordes",
                    "Nicolas Usunier",
                    "Sumit Chopra",
                    "Jason Weston."
                ],
                "venue": "arXiv preprint arXiv:1506.02075 .",
                "citeRegEx": "Bordes et al\\.,? 2015",
                "shortCiteRegEx": "Bordes et al\\.",
                "year": 2015
            },
            {
                "title": "Translating embeddings for modeling multirelational data",
                "author": [
                    "Antoine Bordes",
                    "Nicolas Usunier",
                    "Alberto GarciaDuran",
                    "Jason Weston",
                    "Oksana Yakhnenko."
                ],
                "venue": "Advances in Neural Information Processing Systems. pages 2787\u20132795.",
                "citeRegEx": "Bordes et al\\.,? 2013",
                "shortCiteRegEx": "Bordes et al\\.",
                "year": 2013
            },
            {
                "title": "Cfo: Conditional focused neural question answering with largescale knowledge bases",
                "author": [
                    "Zihang Dai",
                    "Lei Li",
                    "Wei Xu."
                ],
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Asso-",
                "citeRegEx": "Dai et al\\.,? 2016",
                "shortCiteRegEx": "Dai et al\\.",
                "year": 2016
            },
            {
                "title": "Classifying relations by ranking with convolutional neural networks",
                "author": [
                    "Cicero dos Santos",
                    "Bing Xiang",
                    "Bowen Zhou."
                ],
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
                "citeRegEx": "Santos et al\\.,? 2015",
                "shortCiteRegEx": "Santos et al\\.",
                "year": 2015
            },
            {
                "title": "Paraphrase-driven learning for open question answering",
                "author": [
                    "Anthony Fader",
                    "Luke S Zettlemoyer",
                    "Oren Etzioni."
                ],
                "venue": "ACL (1). Citeseer, pages 1608\u20131618.",
                "citeRegEx": "Fader et al\\.,? 2013",
                "shortCiteRegEx": "Fader et al\\.",
                "year": 2013
            },
            {
                "title": "Character-level question answering with attention",
                "author": [
                    "David Golub",
                    "Xiaodong He."
                ],
                "venue": "arXiv preprint arXiv:1604.00727 .",
                "citeRegEx": "Golub and He.,? 2016",
                "shortCiteRegEx": "Golub and He.",
                "year": 2016
            },
            {
                "title": "Improved relation extraction with feature-rich compositional embedding models",
                "author": [
                    "Matthew R. Gormley",
                    "Mo Yu",
                    "Mark Dredze."
                ],
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Compu-",
                "citeRegEx": "Gormley et al\\.,? 2015",
                "shortCiteRegEx": "Gormley et al\\.",
                "year": 2015
            },
            {
                "title": "Deep residual learning for image recognition",
                "author": [
                    "Kaiming He",
                    "Xiangyu Zhang",
                    "Shaoqing Ren",
                    "Jian Sun."
                ],
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 770\u2013778.",
                "citeRegEx": "He et al\\.,? 2016",
                "shortCiteRegEx": "He et al\\.",
                "year": 2016
            },
            {
                "title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision",
                "author": [
                    "Chen Liang",
                    "Jonathan Berant",
                    "Quoc Le",
                    "Kenneth D Forbus",
                    "Ni Lao."
                ],
                "venue": "arXiv preprint arXiv:1611.00020 .",
                "citeRegEx": "Liang et al\\.,? 2016",
                "shortCiteRegEx": "Liang et al\\.",
                "year": 2016
            },
            {
                "title": "Distributed representations of words and phrases and their compositionality",
                "author": [
                    "Tomas Mikolov",
                    "Ilya Sutskever",
                    "Kai Chen",
                    "Greg S Corrado",
                    "Jeff Dean."
                ],
                "venue": "Advances in neural information processing systems. pages 3111\u20133119.",
                "citeRegEx": "Mikolov et al\\.,? 2013",
                "shortCiteRegEx": "Mikolov et al\\.",
                "year": 2013
            },
            {
                "title": "Employing word representations and regularization for domain adaptation of relation extraction",
                "author": [
                    "Thien Huu Nguyen",
                    "Ralph Grishman."
                ],
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short",
                "citeRegEx": "Nguyen and Grishman.,? 2014",
                "shortCiteRegEx": "Nguyen and Grishman.",
                "year": 2014
            },
            {
                "title": "A decomposable attention model for natural language inference",
                "author": [
                    "Ankur Parikh",
                    "Oscar T\u00e4ckstr\u00f6m",
                    "Dipanjan Das",
                    "Jakob Uszkoreit."
                ],
                "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association",
                "citeRegEx": "Parikh et al\\.,? 2016",
                "shortCiteRegEx": "Parikh et al\\.",
                "year": 2016
            },
            {
                "title": "Utd: Classifying semantic relations by combining lexical and semantic resources",
                "author": [
                    "Bryan Rink",
                    "Sanda Harabagiu."
                ],
                "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics, Uppsala, Swe-",
                "citeRegEx": "Rink and Harabagiu.,? 2010",
                "shortCiteRegEx": "Rink and Harabagiu.",
                "year": 2010
            },
            {
                "title": "On generating characteristic-rich question sets for qa evaluation",
                "author": [
                    "Yu Su",
                    "Huan Sun",
                    "Brian Sadler",
                    "Mudhakar Srivatsa",
                    "Izzeddin Gur",
                    "Zenghui Yan",
                    "Xifeng Yan."
                ],
                "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natu-",
                "citeRegEx": "Su et al\\.,? 2016",
                "shortCiteRegEx": "Su et al\\.",
                "year": 2016
            },
            {
                "title": "Semi-supervised relation extraction with large-scale word clustering",
                "author": [
                    "Ang Sun",
                    "Ralph Grishman",
                    "Satoshi Sekine."
                ],
                "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Associa-",
                "citeRegEx": "Sun et al\\.,? 2011",
                "shortCiteRegEx": "Sun et al\\.",
                "year": 2011
            },
            {
                "title": "Combining recurrent and convolutional neural networks for relation classification",
                "author": [
                    "Ngoc Thang Vu",
                    "Heike Adel",
                    "Pankaj Gupta",
                    "Hinrich Sch\u00fctze."
                ],
                "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for",
                "citeRegEx": "Vu et al\\.,? 2016",
                "shortCiteRegEx": "Vu et al\\.",
                "year": 2016
            },
            {
                "title": "Relation classification via multi-level attention cnns",
                "author": [
                    "Linlin Wang",
                    "Zhu Cao",
                    "Gerard de Melo",
                    "Zhiyuan Liu."
                ],
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for",
                "citeRegEx": "Wang et al\\.,? 2016",
                "shortCiteRegEx": "Wang et al\\.",
                "year": 2016
            },
            {
                "title": "Learning natural language inference with lstm",
                "author": [
                    "Shuohang Wang",
                    "Jing Jiang."
                ],
                "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
                "citeRegEx": "Wang and Jiang.,? 2016",
                "shortCiteRegEx": "Wang and Jiang.",
                "year": 2016
            },
            {
                "title": "Bilateral multi-perspective matching for natural language sentences",
                "author": [
                    "Zhiguo Wang",
                    "Wael Hamza",
                    "Radu Florian."
                ],
                "venue": "arXiv preprint arXiv:1702.03814 .",
                "citeRegEx": "Wang et al\\.,? 2017",
                "shortCiteRegEx": "Wang et al\\.",
                "year": 2017
            },
            {
                "title": "Question answering on freebase via relation extraction and textual evidence",
                "author": [
                    "Kun Xu",
                    "Siva Reddy",
                    "Yansong Feng",
                    "Songfang Huang",
                    "Dongyan Zhao."
                ],
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
                "citeRegEx": "Xu et al\\.,? 2016",
                "shortCiteRegEx": "Xu et al\\.",
                "year": 2016
            },
            {
                "title": "S-mart: Novel tree-based structured learning algorithms applied to tweet entity linking",
                "author": [
                    "Yi Yang",
                    "Ming-Wei Chang."
                ],
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer-",
                "citeRegEx": "Yang and Chang.,? 2015",
                "shortCiteRegEx": "Yang and Chang.",
                "year": 2015
            },
            {
                "title": "Freebase qa: Information extraction or semantic parsing",
                "author": [
                    "Xuchen Yao",
                    "Jonathan Berant",
                    "Benjamin Van Durme"
                ],
                "venue": "ACL",
                "citeRegEx": "Yao et al\\.,? \\Q2014\\E",
                "shortCiteRegEx": "Yao et al\\.",
                "year": 2014
            },
            {
                "title": "Information extraction over structured data: Question answering with freebase",
                "author": [
                    "Xuchen Yao",
                    "Benjamin Van Durme."
                ],
                "venue": "ACL (1). Citeseer, pages 956\u2013966.",
                "citeRegEx": "Yao and Durme.,? 2014",
                "shortCiteRegEx": "Yao and Durme.",
                "year": 2014
            },
            {
                "title": "Semantic parsing via staged query graph generation: Question answering with knowledge base",
                "author": [
                    "Wen-tau Yih",
                    "Ming-Wei Chang",
                    "Xiaodong He",
                    "Jianfeng Gao."
                ],
                "venue": "Association for Computational Linguistics (ACL).",
                "citeRegEx": "Yih et al\\.,? 2015",
                "shortCiteRegEx": "Yih et al\\.",
                "year": 2015
            },
            {
                "title": "Semantic parsing for single-relation question answering",
                "author": [
                    "Wen-tau Yih",
                    "Xiaodong He",
                    "Christopher Meek."
                ],
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association",
                "citeRegEx": "Yih et al\\.,? 2014",
                "shortCiteRegEx": "Yih et al\\.",
                "year": 2014
            },
            {
                "title": "The value of semantic parse labeling for knowledge base question",
                "author": [
                    "Wen-tau Yih",
                    "Matthew Richardson",
                    "Chris Meek",
                    "MingWei Chang",
                    "Jina Suh"
                ],
                "venue": null,
                "citeRegEx": "Yih et al\\.,? \\Q2016\\E",
                "shortCiteRegEx": "Yih et al\\.",
                "year": 2016
            },
            {
                "title": "Simple question answering by attentive convolutional neural network",
                "author": [
                    "Wenpeng Yin",
                    "Mo Yu",
                    "Bing Xiang",
                    "Bowen Zhou",
                    "Hinrich Sch\u00fctze."
                ],
                "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Techni-",
                "citeRegEx": "Yin et al\\.,? 2016",
                "shortCiteRegEx": "Yin et al\\.",
                "year": 2016
            },
            {
                "title": "Embedding lexical features via lowrank tensors",
                "author": [
                    "Mo Yu",
                    "Mark Dredze",
                    "Raman Arora",
                    "Matthew R. Gormley."
                ],
                "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
                "citeRegEx": "Yu et al\\.,? 2016",
                "shortCiteRegEx": "Yu et al\\.",
                "year": 2016
            },
            {
                "title": "Relation classification via convolutional deep neural network",
                "author": [
                    "Daojian Zeng",
                    "Kang Liu",
                    "Siwei Lai",
                    "Guangyou Zhou",
                    "Jun Zhao."
                ],
                "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers.",
                "citeRegEx": "Zeng et al\\.,? 2014",
                "shortCiteRegEx": "Zeng et al\\.",
                "year": 2014
            },
            {
                "title": "Exploring various knowledge in relation extraction",
                "author": [
                    "GuoDong Zhou",
                    "Jian Su",
                    "Jie Zhang",
                    "Min Zhang."
                ],
                "venue": "Association for Computational Linguistics. pages 427\u2013434.",
                "citeRegEx": "Zhou et al\\.,? 2005",
                "shortCiteRegEx": "Zhou et al\\.",
                "year": 2005
            },
            {
                "title": "Attentionbased bidirectional long short-term memory networks for relation classification",
                "author": [
                    "Peng Zhou",
                    "Wei Shi",
                    "Jun Tian",
                    "Zhenyu Qi",
                    "Bingchen Li",
                    "Hongwei Hao",
                    "Bo Xu."
                ],
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Com-",
                "citeRegEx": "Zhou et al\\.,? 2016",
                "shortCiteRegEx": "Zhou et al\\.",
                "year": 2016
            }
        ],
        "abstractText": "Relation detection is a core component of many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning which detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different levels of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks."
    },
    {
        "title": "Understanding Convolutional Neural Networks for Text Classification",
        "sections": [
            {
                "heading": null,
                "text": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 56\u201365 Brussels, Belgium, November 1, 2018. c\u00a92018 Association for Computational Linguistics\n56"
            },
            {
                "heading": "1 Introduction",
                "text": "Convolutional Neural Networks (CNNs), originally invented for computer vision, have been shown to achieve strong performance on text classification tasks (Bai et al., 2018; Kalchbrenner et al., 2014; Wang et al., 2015; Zhang et al., 2015; Johnson and Zhang, 2015; Iyyer et al., 2015) as well as other traditional Natural Language Processing (NLP) tasks (Collobert et al., 2011), even when considering relatively simple one-layer models (Kim, 2014).\nAs with other architectures of neural networks, explaining the learned functionality of CNNs is still an active research area. The ability to interpret neural models can be used to increase trust in model predictions, analyze errors or improve the model (Ribeiro et al., 2016). The problem of interpretability in machine learning can be divided into\ntwo concrete tasks: Given a trained model, model interpretability aims to supply a structured explanation which captures what the model has learned. Given a trained model and a single example, prediction interpretability aims to explain how the model arrived at its prediction. These can be further divided into white-box and black-box techniques. While recent works have begun to supply the means of interpreting predictions (AlvarezMelis and Jaakkola, 2017; Lei et al., 2016; Guo et al., 2018), interpreting neural NLP models remains an under-explored area.\nAccompanying their rising popularity, CNNs have seen multiple advances in interpretability when used for computer vision tasks (Zeiler and Fergus, 2014). These techniques unfortunately do not trivially apply to discrete sequences, as they assume a continuous input space used to represent images. Intuitions about how CNNs work on an abstract level also may not carry over from image inputs to text\u2014for example, pooling in CNNs has been used to induce deformation invariance (LeCun et al., 1998, 2015), which is likely different than the role it has when processing text.\nIn this work, we examine and attempt to understand how CNNs process text, and then use this information for the more practical goals of improving model-level and prediction-level explanations.\nWe identify and refine current intuitions as to how CNNs work. Specifically, current common wisdom suggests that CNNs classify text by working through the following steps (Goldberg, 2016):\n1) 1-dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams.\n2) Max-pooling over time extracts the relevant ngrams for making a decision.\n3) The rest of the network classifies the text based on this information.\nWe refine items 1 and 2 and show that:\n\u2022 Max-pooling induces a thresholding behavior, and values below a given threshold are ignored when (i.e. irrelevant to) making a prediction. Specifically, we show an experiment for which 40% of the pooled ngrams on average can be dropped with no loss of performance (Section 4).\n\u2022 Filters are not homogeneous, i.e. a single filter can, and often does, detect multiple distinctly different families of ngrams (Section 5.3).\n\u2022 Filters also detect negative items in ngrams\u2014 they not only select for a family of ngrams but often actively suppress a related family of negated ngrams (Section 5.4).\nWe also show that the filters are trained to work with naturally-occurring ngrams, and can be easily misled (made to produce values substantially larger than their expected range) by selected nonnatural ngrams.\nThese findings can be used for improving model-level and prediction-level interpretability (Section 6). Concretely: 1) We improve model interpretability by deriving a useful summary for each filter, highlighting the kinds of structures it is sensitive to. 2) We improve prediction interpretability by focusing on informative ngrams and taking into account also the negative cues."
            },
            {
                "heading": "2 Background: 1D Text Convolutions",
                "text": "We focus on the task of text classification. We consider the common architecture in which each word in a document is represented as an embedding vector, a single convolutional layer with m filters is applied, producing an m-dimensional vector for each document ngram. The vectors are combined using max-pooling followed by a ReLU activation. The result is then passed to a linear layer for the final classification.\nFor an n-words input text w1, ..., wn we embed each symbol as d dimensional vector, resulting in word vectors w1, ...,wn \u2208 Rd. The resulting d\u00d7n matrix is then fed into a convolutional layer where we pass a sliding window over the text. For each l-words ngram:\nui = [wi, ...,wi+`\u22121] \u2208 Rd\u00d7` ; 0 \u2264 i \u2264 n\u2212 `\nAnd for each filter fj \u2208 Rd\u00d7` we calculate \u3008ui, fj\u3009. The convolution results in matrix\nF \u2208 Rn\u00d7m. Applying max-pooling across the ngram dimension results in p \u2208 Rm which is fed into ReLU non-linearity. Finally, a linear fullyconnected layer W \u2208 Rc\u00d7m produces the distribution over classification classes from which the strongest class is outputted. Formally:\nui = [wi; ...;wi+`\u22121] Fij = \u3008ui, fj\u3009 pj = ReLU(max\ni Fij)\no = softmax(Wp)\nIn practice, we use multiple window sizes ` \u2208 L, L ( N by using multiple convolution layers in parallel and concatenating the resulting p` vectors. We note that the methods in this work are applicable for dilated convolutions as well."
            },
            {
                "heading": "3 Datasets and Hyperparameters",
                "text": "For our empirical experiments and results presented in this work we use three text classification datasets for Sentiment Analysis, which involves classifying the input text (user reviews in all cases) between positive and negative. The specific datasets were chosen for their relative variety in size and domain as well as for the relative simplicity and interpretability of the binary sentiment analysis task.\nThe three datasets are: a) MR: sentence polarity dataset v1.0 introduced by Pang and Lee (2005), containing 10k evenly split short (sentences or snippets) movie reviews. b) Elec: electronic product reviews for sentiment classification introduced by Johnson and Zhang (2015), assembled from the Amazon review dataset (McAuley and Leskovec, 2013; McAuley et al., 2015), containing 200k train and 25k test evenly split reviews. c) Yelp Review Polarity: introduced by Zhang et al. (2015) from the Yelp Dataset Challenge 2015, containing 560k train and 38k test evenly split business reviews.\nFor word embeddings, we use the pre-trained GloVe Wikipedia 2014\u2014Gigaword 5 embeddings (Pennington et al., 2014), which we fine-tune with the model.\nWe use embedding dimension of 50, filter sizes of ` \u2208 {2, 3, 4} words, and m \u2208 {10, 50} filters. Models are implemented in PyTorch and trained with the Adam optimizer."
            },
            {
                "heading": "4 Identifying Important Features",
                "text": "Current common wisdom posits that filters serve as ngram detectors: each filter searches for a specific class of ngrams, which it marks by assigning them high scores. These highest-scoring detected ngrams survive the max-pooling operation. The final decision is then based on the set of ngrams in the max-pooled vector (represented by the set of corresponding filters). Intuitively, ngrams which any filter scores highly (relative to how it scores other ngrams) are ngrams which are highly relevant for the classification of the text.\nIn this section we refine this view by attempting to answer the questions: what information about ngrams is captured in the max-pooled vector, and how is it used for the final classification?1"
            },
            {
                "heading": "4.1 Informative vs. Uninformative Ngrams",
                "text": "Consider the pooled vector p \u2208 Rm on which the classification is based. Each value pj = ReLU(maxi\u3008ui, fj\u3009) stems from a filter-ngram interaction, and can be traced back to the ngram ui = [wi, ...,wi+`\u22121] that triggered it. Denote the set of ngrams contributing to p as Sp. Ngrams not in Sp do not influence the decision of the classifier. But what about the ngrams that are in Sp? Previous attempts in prediction-based interpretation of CNNs for text highlight the ngrams in Sp and their scores as means of explaining the prediction. We take here a more refined view. Note that the final classification does not observe the ngram identities directly, but only through the scores assigned to them by the filters. Hence, the information in p must rely on the assigned scores.\nConceptually, we separate ngrams in Sp into two classes, deliberate and accidental. Deliberate ngrams end up in Sp because they were scored high by their filter, likely because they are informative regarding the final decision. In contrast, accidental ngrams end up in Sp despite having a low score, because no other ngram scored higher than them. These ngrams are likely not informative for the classification decision. Can we tease apart the deliberate and accidental ngrams?\n1Although this work focuses on text classification, the findings in this section apply to any neural architecture which utilizes global max pooling, for both discrete and continuous domains. To our knowledge this is the first work that examines the assumption that max-pooling induces classifying behavior. Previously, Ruderman et al. (2018) showed that other assumptions to the functionality of max-pooling as deformation stabilizers (relevant only in continuous domains) do not necessarily hold true.\nWe assume that there is threshold for each filter, where values above the threshold signal informative information regarding the classification, while values below the threshold are uninformative and can be ignored for the purpose of classification. We thus search for the threshold that separate the two classes. However, as we cannot measure directly which values pj influence the final decision, we opt instead for measuring correlation between pj values and the predicted label for the vector p.\nThe linearity of the decision function Wp allows to measure exactly how much pj is weighted for the logit of label class k. The class which filter fj contributes to is cj = argmaxk Wkj\n2. We refer to class cj as the class identity of filter fj .\nBy assigning each filter a class identity cj and comparing it to the predicted label we arrive at a correlation label\u2014whether the filter\u2019s identity class matches the final decision by the network. Concretely, we run the classifier over a set of texts, resulting in pooled vectors pi and network predictions ci. For each filter j we then consider the values pij and whether c\ni = cj . For each filter, we obtain a dataset (p1j , c 1 = cj), ..., (p D j , c\nD = cj), and we look for a threshold tj that separates pij for which ci = cj from those where ci 6= cj .\n(X,Y )j = {(pij , ci = cj) | j < m & i < D}\nIn an ideal case, the set is linearly separable and we can easily separate informative from uninformative values: if pij > tj then the classifier\u2019s prediction agrees with the filter\u2019s label, and otherwise they disagree. In practice, the set is not separable. We instead work with the purity of a filter-threshold combination, defined as the percentage of informative (correlative) ngrams which were scored above the threshold3. Formally, given threshold dataset (X,Y ):\npurity(f, t) =\n|{(x, y) \u2208 (X,Y )f | x \u2265 t & y = true}| |{(x, y) \u2208 (X,Y )f | x \u2265 t}|\nWe heuristically set the threshold of a filter to the lowest value that achieves a sufficiently high\n2In the case of non-linear fully-connected layers, the question of how each feature contributes to each class is significantly harder to answer. Possible methods include saliency map methods or gradient-based methods. Recently, Guo et al. (2018) has attributed labels to filters using Bayesian inference and other image annotations.\n3The purity metric can be considered as the precision metric for this task.\npurity (we experimentally find that a purity value of 0.75 works well).\nIn Figure 2b,c we show examples for threshold datasets for a model trained on the MR sentiment analysis task.\nThreshold Effectiveness We described a method for obtaining per-filter threshold values. But is the threshold assumption\u2014that items below a given threshold do not participate in the decision\u2014even correct? To assess the quality of threshold obtained by our proposal and validate the thresholding assumption, we discard values that do not pass the threshold for each filter and observe the performance of the model. Practically, we replace the ReLU non-linearity with a threshold function:\nthreshold(x, t) = { x, if x \u2265 t 0, otherwise\nFigure 1 presents the results on the MR dataset (we observed similar results on the Elec dataset). where the threshold is set for each filter separately, based on a shared purity value. If the thresholding assumption is correct and our way of deriving the threshold is effective, we expect to not see a drop in accuracy. Indeed, for purity value of 0.75, we observe that the model performance improves slightly when replacing the ReLU with a per-filter threshold, indicating that the thresholding model is indeed a good approximation for the feature behavior. The percentage of informative (non-accidental) values in p is roughly a linear function of the purity (Figure 1c). With a purity value of 0.754, we discard roughly 44% of the values in p\u2014and hence 44% of the ngrams in Sp.\nNot all filters behave in a similar way, however. In Figure 2 we show an example for a filter\u2014#6 in the figure\u2014which is especially uninformative: by applying the lowest threshold which satisfies a purity of 0.75, we discard 99.99% of activations. Therefore in the experiments in Figure 1, this filter is effectively unused, yet it does not cause loss in performance. In essence, the threshold classifier\n4We note that empirically and intuitively, the more filters we utilize in the network, the less correlation there is between each filter\u2019s class and the final classification, as the decision is being made by a greater consensus. This means that demanding a higher purity will be accompanied by lower coverage, relative to other experiments, and more ngrams will be discarded. The \u201ccorrect\u201d purity level for a filter then is a function of the model and dataset used, and should be investigated using the train or validation datasets.\nidentified and effectively discarded a filter which is not useful to the model.\nTo summarize, we validated our assumptions and shown empirically that global max-pooling indeed induces a functionality of separating important and not important activation signals using a latent (presumably soft) threshold. For the rest of this work we will assume a known threshold value for every filter in the model which we can use to identify important ngrams."
            },
            {
                "heading": "5 What is captured by a filter?",
                "text": "Previous work looked at the top-k scoring ngrams for each filter. However, focusing on the top-k does not tell a complete story. We insead look at the set of deliberate ngrams: those that pass the filter\u2019s threshold value. Common intuition suggests that each filter is homogeneous and specializes in detecting a specific classes of ngrams. For example, a filter may specializing in detecting ngrams such as \u201chad no issues\u201d, \u201chad zero issues\u201d, and \u201chad no problems\u201d. We challenge this view and show that filters often specialize in multiple distinctly different semantic classes by utilizing activation patterns which are not necessarily maximized. We also show that filters may not only identify good ngrams, but may also actively supress bad ones."
            },
            {
                "heading": "5.1 Slot Activation Vectors",
                "text": "As discussed in Section 2, for each ngram u = [w1, ...,w`] and for each filter f we calculate the score \u3008u, f\u3009. The ngram score can be decomposed as a sum of individual word scores by considering the inner products between every word embedding wi in u and every parallel slice in f :\n\u3008u, f\u3009 = `\u22121\u2211 i=0 \u3008wi, fid:i(d+1)\u3009\nWe refer to slice fid:i(d+1) as slot i of the filter weights, denoted as f(i). Instead of taking the sum of these inner products, we can instead interpret them directly\u2014saying that \u3008wi, f(i)\u3009 captures how much slot i in f is activated by the ith word in the ngram5.\nWe can now move from examining the activation of an ngram-filter pair \u3008u := [w1; ...;w`], f\u3009 to examining its slot activation vector: (\u3008w1, f(1)\u3009, ..., \u3008w`, f(`)\u3009). The slot ac-\ntivation vector captures how much each word in the ngram contributes to its activation."
            },
            {
                "heading": "5.2 Naturally occurring vs. possible ngrams",
                "text": "We distinguish naturally occurring or observed ngrams, which are ngrams that are observed in a large corpus, from possible ngrams which are any combination of ` words from the vocabulary. The possible ngrams are a superset of the naturally occurring ones. Given a filter, we can find its topscoring naturally occurring ngram by searching over all ngrams in a corpus. We can find its topscoring possible ngram by maximizing each slot value individually. We observe there is a big and consistent gap in scores between the top-scoring natural ngrams and top-scoring possible ngrams. In our Elec model, when averaging over all filters, the top naturally-occurring ngrams score 30% less than the top possible ngrams. Interestingly, the\n5 We note that this breakdown does not consider the filter\u2019s bias, if one is used. This bias is a single number (per filter) which is added to the sum of slot activations to arrive at the ngram activation which is passed to the max-pooling layer. Bias can be accommodated by appending an additional \u201cbias word\u201d with an embedding vector of [1, ..., 1] to every ngram. Regardless, as this bias is identical for all ngrams for the filter in question, it has no role in identifying which ngrams the filter is most similar to, and we can ignore it in this context.\ntop-scoring natural ngrams almost never fully activate all slots in a filter.\nTable 1 shows the top-scoring naturally occurring and possible ngrams for nine filters in the Elec model. In each of the top scoring natural ngrams, at least one slot receives a low activation. Table 2 zooms in on one of the filters and shows its top7 naturally occurring ngrams and top-7 most activated words in each slot. Here, most top-scoring ngrams maximize slot #3 with words such as invaluable and perfect, however some ngrams such as \u201cworks as good\u201d and \u201cstill holding strong\u201d maximize slots #1 and #2 respectively, instead.\nAdditionally, most top-scoring words do not appear to be utilized in high-scoring ngrams at all. This can be explained with the following: if a word such as crt rarely or never appears in slot #1 alongside other high-scoring words in other slots, then crt can score highly with no consequence. Since an ngram containing crt at slot #1 will rarely pass the max-pooling layer, its score at that slot is essentially random.\nOn naturally occurring ngrams, the filters do not achieve maximum values in all slots but only on some of them. Why? We consider two hypotheses to explain this behavior:\n(i) Each filter captures multiple semantic classes of ngrams, and each class has some dominating slots and some non-dominating slots (which we define as a slot activation pattern).\n(ii) A slot may not be maximized because it\u2019s not used to detect word existence, but rather lack of existence\u2014ensuring that specific words do not occur.\nWe investigate both hypotheses in Sections 5.3 and 5.4 respectively.\nAdversarial potential We note in passing that this discrepancy in scores between naturally occurring and possible ngrams can be used to derive adversarial examples that cause a trained model to misclassify. By inserting a few seemingly random ngrams, we can cause filters to activate beyond their expected range, potentially driving the model to misclassification. We reserve this area of exploration for future work."
            },
            {
                "heading": "5.3 Clustering (Hypothesis (i))",
                "text": "We explore hypothesis (i) by clustering thresholdpassing (naturally occurring) ngrams in each filter according to their activation vectors. We use Mean Shift Clustering (Fukunaga and Hostetler,\n1975; Cheng, 1995), an algorithm that does not require specifying an a-priori number of clusters, and does not make assumptions about their shapes. Mean Shift considers the feature vectors as sampled from an underlying probability density function6. Each cluster captures a different slot activation pattern. We use the cluster\u2019s centroid as the prototypical slot activation for that cluster.\nTable 3 shows a sample clustering output. The clustering algorithm identified two clusters: one primarily containing ngrams of the pattern DET INTENSITY-ADVERB POSITIVE-WORD, while the second contains ngrams that begin with phrases like go wrong.7\nThe centroids for these clusters capture the activation patterns well: low-medium-high and highhigh-low for clusters 1 and 2 respectively.\nTo summarize, by discarding noisy ngrams which do not pass the filter\u2019s threshold and then clustering those that remain according to their slot activation patterns, we arrived at a clearer image\n6Intuitively, we can think of the sampling noise as the ngram embeddings, and the probability distribution as defined by a function of the filter weights.\n7In the Yelp dataset, go wrong overwhelmingly occurs in a negated context such as \u201ccan\u2019t go wrong\u201d and \u201cwon\u2019t go wrong\u201d, which explains why it is detected by a positive filter.\nof the semantic classes of ngrams that a given filter specializes in capturing. In particular, we reveal that filters are not necessarily homogeneous: a single filter may detect several different semantic patterns, each one of them relying on a different slot activation pattern."
            },
            {
                "heading": "5.4 Negative Ngrams (Hypothesis (ii))",
                "text": "Our second theory to explain the discrepancy between the activations of naturally occurring and possible ngrams is that certain filter slots are not used to detect a class of highly activating words, but rather to rule out a class of highly negative words. We refer to these as negative ngrams.\nFor example, Table 3 shows an ngram pattern for which slot #1 contains determiners and other \u201cfiller\u201d tokens such as hyphens, periods and commas with relatively weak slot activations. Hypothesis (ii) suggests that this slot may receive a strong negative score for words such as not and n\u2019t, causing such negated patterns to drop below the threshold. Indeed, ngrams containing not or n\u2019t in slot #1 do not pass the threshold for this filter.\nWe are interested in a more systematic method of identifying these cases. Identifying negative slot activations would be very useful for understanding the semantics captured by a filter and the reasoning behind the dismissal of an ngram, as we discuss in Sections 6.1 and 6.2 respectively.\nWe achieve this by searching the belowthreshold ngram space for ngrams which are \u201cflipped versions\u201d of above-threshold ngrams. Concretely: Given ngram u which was scored highly by filter f , we search for low-scoring\nngrams u\u2032 such that the hamming distance between u and u\u2032 is low. By doing this for the topk scoring ngrams per cluster, we arrive at a comprehensive set of negative ngrams. In Table 4 we show a sample output of this algorithm.\nFurthermore, we can divide negative ngrams into two cases: 1) Lowering the ngram score below the threshold by replacing high-scoring words with low-scoring words. 2) Lowering the ngram score below the threshold by replacing words with a low positive score with words with a highly-negative score. Case 2 is more interesting because it embodies cases where hypothesis (ii) is relevant. Additionally, it highlights ngrams where a strongly positive word in one slot was negated with another strongly negative word in another slot. Table 4 shows examples in bold.\nIn order to identify \u201cCase 2\u201d negative ngrams, we heuristically test whether the \u201cchanged\u201d words\u2019 scores directly influence the status of the activation relative to the threshold: given an already identified negative ngram, if the ngram score\u2014sans the bottom-k negative slot activations (considering a hamming distance of k and given that there are k negative slot activations)\u2014passes the threshold, yet it does not pass the threshold by including the negative slot activations, then the ngram is considered a \u201cCase 2\u201d negative ngram."
            },
            {
                "heading": "6 Interpretability",
                "text": "In this section we show two practical implications of the findings above: improvements in both model-level and prediction-level interpretability of 1D CNNs for text classification."
            },
            {
                "heading": "6.1 Model Interpretability",
                "text": "As in computer vision, we can now interpret a trained CNN model by \u201cvisualizing\u201d its filters and interpreting the visible shapes\u2014in other words, defining a high-level description of what the filter detects. We propose to associate each filter with the following items: 1) The class which this filter\u2019s strong signals contribute to (in the sentiment task: positive or negative); 2) The threshold value for the filter, together with its purity and coverages percentages (which essentially capture how informative this filter is); 3) A list of semantic patterns identified by this filter. Each list item corresponds to a slot-activations cluster. For each cluster we present the top-k ngrams activating it, and for each ngram we specify its total activation, its\nslot-activation vector, and its list of bottom-k negative ngrams with their activations and slot activations. In particular, by clustering the activated ngrams according to their slot activation patterns and showing the top-k in each clusters, we get a much more refined coverage of the linguistic patterns that are captured by the filter."
            },
            {
                "heading": "6.2 Prediction Interpretability",
                "text": "Previous prediction-based interpretation attempts traced back the ngrams from the max-pooling layer. Here we improve these previous attempts by considering only ngrams that pass the threshold for their filter. This results in a more concise and relevant explanation (Figure 1). Figure 3 shows two examples. Note that in example #1, many negative-class filters were \u201cforced\u201d to choose an ngram in max-pooling despite there not being strongly negative phrases\u2014but those ngrams do not pass the threshold and are thus cleaned from the explanation.\nAdditionally we can use the individual slot activations to tease-apart the contribution of each word in the ngram. Finally, we can also mark cases of negative-ngrams (Section 5.4), where an ngram has high slot activations for some words, but these are negated by a highly-negative slot and\nas a consequence are not selected by max-pooling, or are selected but do not pass the filter\u2019s threshold."
            },
            {
                "heading": "7 Conclusion",
                "text": "We have refined several common wisdom assumptions regarding the way in which CNNs process and classify text. First, we have shown that maxpooling over time induces a thresholding behavior on the convolution layer\u2019s output, essentially separating between features that are relevant to the final classification and features that are not. We used this information to identify which ngrams are important to the classification. We also associate each filter with the class it contributes to. We decompose the ngram score into word-level scores by treating the convolution of a filter as a sum of word-level convolutions, allowing us to examine the word-level composition of the activation. Specifically, by maximizing the word-level activations by iterating over the vocabulary, we observed that filters do not maximize activations at\nthe word-level, but instead form slot activation patterns that give different types of ngrams similar activation strengths. This provides empirical evidence that filters are not homogeneous. By clustering high-scoring ngrams according to their slotactivation patterns we can identify the groups of linguistic patterns captured by a filter. We also show that filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words. Finally, we use these findings to suggest improvements to model-based and predictionbased interpretability of CNNs for text."
            }
        ],
        "references": [
            {
                "title": "A causal framework for explaining the predictions of black-box sequence-to-sequence models",
                "author": [
                    "David Alvarez-Melis",
                    "Tommi S. Jaakkola."
                ],
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
                "citeRegEx": "Alvarez.Melis and Jaakkola.,? 2017",
                "shortCiteRegEx": "Alvarez.Melis and Jaakkola.",
                "year": 2017
            },
            {
                "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
                "author": [
                    "Shaojie Bai",
                    "J. Zico Kolter",
                    "Vladlen Koltun."
                ],
                "venue": "CoRR, abs/1803.01271.",
                "citeRegEx": "Bai et al\\.,? 2018",
                "shortCiteRegEx": "Bai et al\\.",
                "year": 2018
            },
            {
                "title": "Mean shift, mode seeking, and clustering",
                "author": [
                    "Yizong Cheng."
                ],
                "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 17(8):790\u2013799.",
                "citeRegEx": "Cheng.,? 1995",
                "shortCiteRegEx": "Cheng.",
                "year": 1995
            },
            {
                "title": "Natural language processing (almost) from scratch",
                "author": [
                    "Ronan Collobert",
                    "Jason Weston",
                    "L\u00e9on Bottou",
                    "Michael Karlen",
                    "Koray Kavukcuoglu",
                    "Pavel P. Kuksa."
                ],
                "venue": "Journal of Machine Learning Research, 12:2493\u20132537.",
                "citeRegEx": "Collobert et al\\.,? 2011",
                "shortCiteRegEx": "Collobert et al\\.",
                "year": 2011
            },
            {
                "title": "The estimation of the gradient of a density function, with applications in pattern recognition",
                "author": [
                    "Keinosuke Fukunaga",
                    "Larry D. Hostetler."
                ],
                "venue": "IEEE Trans. Information Theory, 21(1):32\u201340.",
                "citeRegEx": "Fukunaga and Hostetler.,? 1975",
                "shortCiteRegEx": "Fukunaga and Hostetler.",
                "year": 1975
            },
            {
                "title": "A primer on neural network models for natural language processing",
                "author": [
                    "Yoav Goldberg."
                ],
                "venue": "J. Artif. Intell. Res., 57:345\u2013420.",
                "citeRegEx": "Goldberg.,? 2016",
                "shortCiteRegEx": "Goldberg.",
                "year": 2016
            },
            {
                "title": "Neural network interpretation via fine grained textual summarization",
                "author": [
                    "Pei Guo",
                    "Connor Anderson",
                    "Kolten Pearson",
                    "Ryan Farrell."
                ],
                "venue": "CoRR, abs/1805.08969.",
                "citeRegEx": "Guo et al\\.,? 2018",
                "shortCiteRegEx": "Guo et al\\.",
                "year": 2018
            },
            {
                "title": "Deep unordered composition rivals syntactic methods for text classification",
                "author": [
                    "Mohit Iyyer",
                    "Varun Manjunatha",
                    "Jordan L. BoydGraber",
                    "Hal Daum\u00e9 III."
                ],
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
                "citeRegEx": "Iyyer et al\\.,? 2015",
                "shortCiteRegEx": "Iyyer et al\\.",
                "year": 2015
            },
            {
                "title": "Effective use of word order for text categorization with convolutional neural networks",
                "author": [
                    "Rie Johnson",
                    "Tong Zhang."
                ],
                "venue": "NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association for Computational Linguistics:",
                "citeRegEx": "Johnson and Zhang.,? 2015",
                "shortCiteRegEx": "Johnson and Zhang.",
                "year": 2015
            },
            {
                "title": "A convolutional neural network for modelling sentences",
                "author": [
                    "Nal Kalchbrenner",
                    "Edward Grefenstette",
                    "Phil Blunsom."
                ],
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014,",
                "citeRegEx": "Kalchbrenner et al\\.,? 2014",
                "shortCiteRegEx": "Kalchbrenner et al\\.",
                "year": 2014
            },
            {
                "title": "Convolutional neural networks for sentence classification",
                "author": [
                    "Yoon Kim."
                ],
                "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special",
                "citeRegEx": "Kim.,? 2014",
                "shortCiteRegEx": "Kim.",
                "year": 2014
            },
            {
                "title": "Deep learning",
                "author": [
                    "Yann LeCun",
                    "Y Bengio",
                    "Geoffrey Hinton."
                ],
                "venue": "521:436\u201344.",
                "citeRegEx": "LeCun et al\\.,? 2015",
                "shortCiteRegEx": "LeCun et al\\.",
                "year": 2015
            },
            {
                "title": "Gradient-based learning applied to document recognition",
                "author": [
                    "Yann LeCun",
                    "Leon Bottou",
                    "Y Bengio",
                    "Patrick Haffner."
                ],
                "venue": "86:2278 \u2013 2324.",
                "citeRegEx": "LeCun et al\\.,? 1998",
                "shortCiteRegEx": "LeCun et al\\.",
                "year": 1998
            },
            {
                "title": "Rationalizing neural predictions",
                "author": [
                    "Tao Lei",
                    "Regina Barzilay",
                    "Tommi S. Jaakkola."
                ],
                "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages",
                "citeRegEx": "Lei et al\\.,? 2016",
                "shortCiteRegEx": "Lei et al\\.",
                "year": 2016
            },
            {
                "title": "Hidden factors and hidden topics: understanding rating dimensions with review text",
                "author": [
                    "Julian J. McAuley",
                    "Jure Leskovec."
                ],
                "venue": "Seventh ACM Conference on Recommender Systems, RecSys \u201913, Hong Kong, China, October 12-16, 2013, pages 165\u2013172.",
                "citeRegEx": "McAuley and Leskovec.,? 2013",
                "shortCiteRegEx": "McAuley and Leskovec.",
                "year": 2013
            },
            {
                "title": "Image-based recommendations on styles and substitutes",
                "author": [
                    "Julian J. McAuley",
                    "Christopher Targett",
                    "Qinfeng Shi",
                    "Anton van den Hengel."
                ],
                "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Informa-",
                "citeRegEx": "McAuley et al\\.,? 2015",
                "shortCiteRegEx": "McAuley et al\\.",
                "year": 2015
            },
            {
                "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
                "author": [
                    "Bo Pang",
                    "Lillian Lee."
                ],
                "venue": "CoRR, abs/cs/0506075.",
                "citeRegEx": "Pang and Lee.,? 2005",
                "shortCiteRegEx": "Pang and Lee.",
                "year": 2005
            },
            {
                "title": "Glove: Global vectors for word representation",
                "author": [
                    "Jeffrey Pennington",
                    "Richard Socher",
                    "Christopher D. Manning"
                ],
                "venue": "In Empirical Methods in Natural Language Processing (EMNLP),",
                "citeRegEx": "Pennington et al\\.,? \\Q2014\\E",
                "shortCiteRegEx": "Pennington et al\\.",
                "year": 2014
            },
            {
                "title": "why should I trust you?\u201d: Explaining the predictions of any classifier",
                "author": [
                    "Marco T\u00falio Ribeiro",
                    "Sameer Singh",
                    "Carlos Guestrin."
                ],
                "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
                "citeRegEx": "Ribeiro et al\\.,? 2016",
                "shortCiteRegEx": "Ribeiro et al\\.",
                "year": 2016
            },
            {
                "title": "Learned deformation stability in convolutional neural networks",
                "author": [
                    "Avraham Ruderman",
                    "Neil C. Rabinowitz",
                    "Ari S. Morcos",
                    "Daniel Zoran."
                ],
                "venue": "CoRR, abs/1804.04438.",
                "citeRegEx": "Ruderman et al\\.,? 2018",
                "shortCiteRegEx": "Ruderman et al\\.",
                "year": 2018
            },
            {
                "title": "Semantic clustering and convolutional neural network for short text categorization",
                "author": [
                    "Peng Wang",
                    "Jiaming Xu",
                    "Bo Xu",
                    "Cheng-Lin Liu",
                    "Heng Zhang",
                    "Fangyuan Wang",
                    "Hongwei Hao."
                ],
                "venue": "Proceedings of the 53rd Annual Meeting of the Association",
                "citeRegEx": "Wang et al\\.,? 2015",
                "shortCiteRegEx": "Wang et al\\.",
                "year": 2015
            },
            {
                "title": "Visualizing and understanding convolutional networks",
                "author": [
                    "Matthew D. Zeiler",
                    "Rob Fergus."
                ],
                "venue": "Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I, volume 8689 of Lecture Notes",
                "citeRegEx": "Zeiler and Fergus.,? 2014",
                "shortCiteRegEx": "Zeiler and Fergus.",
                "year": 2014
            },
            {
                "title": "Character-level convolutional networks for text classification",
                "author": [
                    "Xiang Zhang",
                    "Junbo Jake Zhao",
                    "Yann LeCun."
                ],
                "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015,",
                "citeRegEx": "Zhang et al\\.,? 2015",
                "shortCiteRegEx": "Zhang et al\\.",
                "year": 2015
            }
        ],
        "abstractText": "We present an analysis into the inner workings of Convolutional Neural Networks (CNNs) for processing text. CNNs used for computer vision can be interpreted by projecting filters into image space, but for discrete sequence inputs CNNs remain a mystery. We aim to understand the method by which the networks process and classify text. We examine common hypotheses to this problem: that filters, accompanied by global max-pooling, serve as ngram detectors. We show that filters may capture several different semantic classes of ngrams by using different activation patterns, and that global max-pooling induces behavior which separates important ngrams from the rest. Finally, we show practical use cases derived from our findings in the form of model interpretability (explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and NLP) and prediction interpretability (explaining predictions)."
    },
    {
        "title": "Interactive Dictionary Expansion using Neural Language Models",
        "sections": [
            {
                "heading": "1 Introduction",
                "text": "Dictionary expansion [12] is one area where close integration of humans into the discovery loop has been shown to enhance task performance substantially over more traditional post-adjudication. This is not surprising, as dictionary membership is often a fairly subjective judgment (e.g., should a fruit dictionary include tomatoes?) [13]. Thus even with a system which finds \u201csimilar\u201d terms (e.g., word2vec) guidance is important to keep the system focused on the subject matter expert\u2019s notion of lexicon.\nIn this work we propose a feature agnostic approach for dictionary expansion based on lightweight neural language models, such as word2vec [9]. To prevent semantic drift during the dictionary expansion, we effectively include humanin-the-loop (HumL). Given an input text corpus and a set of seed examples, the proposed approach runs in two phases, explore and exploit, to identify new potential dictionary entries. The explore phase tries to identify similar instances to the dictionary entries that are present in the input text corpus, using term vectors from the neural language model to calculate a similarity score. The exploit phase tries to construct more complex multi-term phrases based on the instances\nalready in the input dictionary. Multi-term phrases are a challenge for word2vec style systems as they need to be \u201cknown\u201d prior to model creation. To identify multi-term phrases, most commonly a simple phrase detection model is used, which is based on a term\u2019s co-occurrence score, i.e., terms that often appear together probably are part of the same phrase [9]. The phrase detection must be done before the model is built, and they remain unchanged after the model is built. However, depending on the domain and the task, the instances of interest evolve, or the example corpus may not be complete. For example, valid phrase combinations may simply not occur (e.g., acute joint pain may appear in the sample corpus, but for some reason chronic hip pain may not). However, these phrases are likely to occur in future texts from the same source, and thus are important to include in any entity extraction lexicon.\nIn the exploit phase, the approach generates new phrases by analyzing the single terms of the instances in the input dictionary. We use two phrase generation algorithms: (i) modify the phrases by replacing single terms with similar terms from the text corpus, e.g., \u201cabnormal behavior\u201d can be modified to \u201dstrange behavior\u201d; (ii) extend the instances with terms from the text corpus that are related to the terms in the instance, e.g., abnormal blood clotting problems is a an adverse drug reaction, which doesn\u2019t appear as such in a large text corpus, however the instances \u201cabnormal blood count\u201d, \u201cblood clotting\u201d and \u201cclotting problems\u201d appear several times in the corpus, which can be used to build the more complex instance. The approach allows us to construct new multi-term instances that don\u2019t appear as such in the text corpus, but there is enough statistical evidence in the corpus that such instances might be of interest for the user.\nCombining the explore and exploit approaches in an unsupervised fashion (or an infrequently supervised fashion) is not particularly effective. It tends to generate many spurious results that the human subject matter expert needs to wade through. Close supervision, however, results in a much more performant system. The evaluation shows that high promptness of the HumL (tighter computer/human partnership) results in nearly perfect performance of the system, i.e., nearly all the candidates identified by the system are valid entries in the dictionary. More precisely, the experiments show that the system is 216% more effective when receiving HumL feedback after 10 identified candidates, compared to receiving HumL feedback after 500 identified candidates, while both cases require equal amount of human effort.\nThe rest of this paper is structured as follows. In Section 2, we give an overview of related work. In Section 3, we present our interactive dictionary expansion approach, followed by an evaluation in Section 4; We conclude with a summary and an outlook on future work."
            },
            {
                "heading": "2 Related Work",
                "text": "Dictionaries and ontologies are the backbone of many NLP and information retrieval systems. Hence, a lot of work in the literature focuses on identifying\nnew approaches for more efficient and more effective dictionary extraction from unstructured text.\nRiloff and Jones [12], is one of the first works to propose an automatic iterattive approach for dictionary extraction from unstructured text. The approach uses mutual bootstrapping technique that learns extraction patterns from the seed terms and then exploits the learned extraction patterns to identify more terms that belong to the semantic category. In the following years, many similar approaches have been developed [13,3,7,2]. However, all these approaches require NLP parsing for feature extraction, and have a reliance on syntactic information for identifying quality patterns. Hence, such approaches underperform on not-so-well structured texts, like user-generated text. Furthermore, without human-in-the-loop, iterative methods can easily generate semantic drift.\nOne of the major challenges with concept extraction involves dealing with not-so-well structured text, given the importance of user generated content, which can prove to be extremely valuable source of information for many domains, pharmacovigilance being one of those.1 To this end, Lee et al. [8] propose a semi-supervised model which uses a random Twitter stream as unlabeled training data and prove it successful for the recognition of Adverse Drug Reaction. Another hurdle is the fact that the dictionary to be created can be highly dependent on the task at hand, especially when dealing with positive/negative words which are highly domain-dependent [6,11]. While completely automatic techniques are highly appealing they need to be fine-tuned for every new task. We propose a human-in-the-loop approach where the \u201ctuning\u201d is an integral part of the process, i.e. the human works in partnership with the statistical method to drive the semantic of the task effectively and efficaciously.\nMany works rely on machine learning techniques and tailor the algorithms to certain specific domains (e.g. drugs): these methods are in general expensive, requiring an annotated corpus and/or domain specific feature extraction (a comprehensive overview can be found in [10]).\nOur work is closely related to glimpse [5] and glimpseLD [1]. Glimpse is a statistical algorithm for dictionary extraction based on SPOT [5] with a faster underlying matching engine. The input is a large text corpus and a set of seed examples. Starting from these it evaluates the contexts (the set of words surrounding an item) in which the seeds occur and identifies \u201cgood\u201d contexts. Contexts are scored retrospectively in terms of how many \u201cgood\u201d results they generate. All contexts are kept which have a score over a given threshold and the candidates that appear in the most \u201cgood\u201d contexts are provided first to the HumL. The approach has been extended to glimpseLD [1], which is language agnostic and uses Linked Data to as a bootstrapping source. While both approaches have been proven to achieve high effectiveness for dictionary extension, both of the approaches can only identify new dictionary entries that are only present in the input text corpus. In this work, we adopt the glimpse computer/human part-\n1 PSB2016 is a recent benchmarking initiative on the problem http://diego.asu.edu/psb2016/ sharedtaskeval.html.\nnership architecture and extend it with the explore/exploit algorithm for more effective dictionary expansion."
            },
            {
                "heading": "3 Approach",
                "text": "The input of the algorithm is a text corpus TC and a set of dictionary seed example terms S. In the preprocessing step, we build a word2vec model [9], with the skip-gram implementation using TC as an input. Word2vec is a particularly computationally-efficient two-layer neural net model for learning term embeddings from raw text. The output of the model is an embedding matrix W, where each term (word or phrase) from the corpus vocabulary VTC is represented as an n-dimensional vector. Projecting such latent representations of words into a lower dimensional feature space shows that semantically similar words appear closer to each other.\nOur approach is based on the explore/exploit paradigm to effectively discover new instances (explore) from the text corpus and generate new \u201cunseen\u201d instances based on user feedback (exploit). The approach runs in iterations, where each iteration runs first the explore phase then the exploit phase. The explore phase uses the instances available in the input dictionary to identify similar candidates that are already present in the corpus vocabulary VTC , which are then accepted or rejected by the HumL. The accepted candidates are then added to the input dictionary and are used in the exploit phase as well as the next explore iteration. During the exploit phase, we use the instances in the input dictionary to construct more complex phrases that might be of interest for the user."
            },
            {
                "heading": "3.1 Explore",
                "text": "As previously mentioned, in the word2vec feature embedding space, semantically similar words appear close to each other in the feature space. Therefore, the problem of calculating the similarity between two instances is a matter of calculating the distance between two instances in the given feature space. To do so we use the standard cosine similarity measure which is applied on the vectors of the instances. Formally, the similarity between two terms w1 and w2, with vectors V1 and V2, is calculated as the cosine similarity between the vectors V1 and V2:\nsim(w1, w2) = V1 \u00b7 V2\n||V1|| \u00b7 ||V2|| (1)\nWe calculate the similarity between the instances in the input dictionary and all the words in the corpus vocabulary VTC . We sort the vocabulary in descending order using the cumulative similarity score, and choose the top-N candidates to present to the HumL. The accepted candidates are added in the input dictionary, which are then used in the exploit phase and the next iteration."
            },
            {
                "heading": "3.2 Exploit",
                "text": "In the exploit phase we try to identify more complex phrases that don\u2019t exist in the corpus vocabulary by analyzing the structure of the instances in the input dictionary.\nThis is critical to help \u201cfuture proof\u201d a lexicon against new text. For a surveillance application (e.g., drug side effects mentioned on twitter) it reduces how frequently a human needs to \u201ctune up\u201d the lexicon to make sure it is catching all relevant entity instances.\nWe use two phrase generation algorithms.\nIn the first approach, we first break each instance in to a set of single terms T = {t1, t2, ..., tn}, then for each term ti in T we identify a set of similar terms TSti = {ts1, ts2, ..., tss} in the vocabulary VTC using Equation 1. In the next step, we build new phrases by replacing ti with a term tsi from TSti . The new phrases are sorted based on the similarity score and the top-N are selected as candidates. For example, given the entry \u201cabnormal behavior\u201d the approach will identify \u201cstrange behavior\u201d, \u201cabnormal attitude\u201d and \u201cstrange attitude\u201d.\nIn the second approach, we generate new phrases by extending the instances with terms from the text corpus that are related to the terms in the instance. Related terms are terms that often share the same context, which means they often are surrounded by similar words. Given a word2vec model, we calculate the relatedness between two terms w1 and w2, as the probability p(w1|w2) calculated using the softmax function,\np(w1|w2) = exp(v\u2032Tw1vw2)\u2211V w=1 exp(v \u2032T w vw2) , (2)\nwhere vw and v \u2032 w are the input and the output vector of the word w, and V is the complete vocabulary of words.\nAs before, we first break each instance in to a set of single terms T = {t1, t2, ..., tn}, then for each term ti in T we identify a set of similar terms TRti = {tr1, tr2, ..., trr} in the vocabulary VTC using Equation 2. In the next step, we build new phrases by appending a term tri from TRti to each term ti from T . The new phrases are sorted based on the relatedness score and the top-N are selected as candidates. For example, given the instance \u201cclotting problems\u201d in the input dictionary the approach first tries to identify related terms in the text corpus for \u201cclotting\u201d. For which the top word is \u201cblood\u201d, because in many sentences \u201cblood clotting\u201d appears as a phrase, which can be used to generate new instances \u201cblood clotting problems\u201d. In the next iteration the phrase can be further extended, by identifying new related words. For example, in the top-N related words for \u201cblood\u201d we will find \u201cabnormal\u201d, which can be used to generate the instance \u201cabnormal blood clotting problems\u201d."
            },
            {
                "heading": "4 Evaluation",
                "text": "To evaluate our approach we conduct two experiments, i.e., (i) count the number of newly discovered dictionary entries per iteration; (ii) the impact of the promptness of the HumL on the system performance.\nFor the experiments we use data from the healtcare domain, specifically tackling the problem of identifying Adverse Drug Reactions in user generated data. As an input text corpus we use user blogs extracted from http://www. askapatient.com (a forum where patients report their experience with medication drugs). As an input set of seed examples we use a set of 203 instances referring to adverse drug events, which were labeled by a medical doctor [4]."
            },
            {
                "heading": "4.1 Dictionary Growth",
                "text": "In this experiment we compare the performance of the explore, exploit and the explore/exploit approaches for discovering new dictionary instances. We run the evaluation in 10 iterations, where after each iteration we count how many new instances are discovered in the top 50 proposed candidates by the algorithm. The accepted instances are then added in the dictionary and used for the next iteration. For the explore/exploit approach we run explore to identify 25 candidates, and exploit to identify another 25 candidates. The results are shown in Fig. 1.\nThe results show that using the explore/exploit approach we are able to discover significantly more instances in each iteration compared to the other approaches. We can observe that when using the explore approach the number of newly discovered instances quickly decreases as the number of available instances in the whole corpus is decreasing in each iteration. When using the exploit approach the number of newly discovered instances sharply decreases as no new base terms are introduced, thus the exploit cannot generate new instances that can be added in the dictionary.\nThe results show that using explore and exploit alternately leads to the best performances."
            },
            {
                "heading": "4.2 Impact of the HumL on the Dictionary Growth",
                "text": "In this experiment we show the importance of the promptness of the HumL on the number of newly discovered instances, i.e., we evaluate if the user gives their feedback to the system sooner it will improve the performance of the system. To do so, we run the explore/exploit approach with different feedback intervals. The feedback interval indicates how many candidates the system needs to identify before the user gives their feedback to the system. For example, when using feedback interval of 10, the user gives their feedback after 10 candidates are identified by the system. We evaluate feedback intervals of 10, 50, 100, 250 and 500. After each iteration we count the number of accepted candidates, and\ninclude them in the dictionary to be used for the next iteration. The results are shown in Fig 2.\nThe results show that the tighter the HumL integration is, the more quickly new instances are discovered. We see that with a large 500 examples feedback interval the HumL system discovers 212 new instances, but requires the human to consider 500 candidates.\nA more tightly integrated system with a 10 examples feedback interval finds 212 new instances in just 23 iterations, requiring the human to consider only 230 candidates. After 50 iterations the system discovered 460 new dictionary entries, compared to only 212 new entries when using 500 examples feedback interval. That yields 216% improvement in effectivness of the system."
            },
            {
                "heading": "5 Conclusions and future work",
                "text": "This paper proposes an interactive dictionary expansion tool using a lightweight neural language model. Our algorithm is iterative and purely statistical, hence does not require any feature extraction beyond tokenization. It incorporates human feedback to improve performance and control semantic drift at every iteration cycle. The experiments showed high importance of tight HumL integration on discovery efficiency.\nIn this work, we have considered only lightweight language models, which can be efficiently built and updated on large text corpora. In future work, we will analyze more complex language neural network models, such as Recurrent Neural Networks (RNN), Long Short Term Memory Networks (LSTM), and bidirectional LSTM, which might improve the search for similar and related terms, at the expense of higher training time. Furthermore, future work will include an evaluation of the approach on multiple datasets covering different domains."
            }
        ],
        "references": [
            {
                "title": "Multi-lingual concept extraction with linked data and human-in-the-loop",
                "author": [
                    "A. Alba",
                    "A. Coden",
                    "A.L. Gentile",
                    "D. Gruhl",
                    "P. Ristoski",
                    "S. Welch"
                ],
                "venue": "Proceedings of the Knowledge Capture Conference",
                "citeRegEx": "1",
                "shortCiteRegEx": "1",
                "year": 2017
            },
            {
                "title": "Semantic lexicon construction: Learning from unlabeled data via spectral analysis",
                "author": [
                    "R.K. Ando"
                ],
                "venue": "Tech. rep., IBM THOMAS J WATSON RESEARCH CENTER YORKTOWN HEIGHTS NY",
                "citeRegEx": "2",
                "shortCiteRegEx": "2",
                "year": 2004
            },
            {
                "title": "Using the web to reduce data sparseness in pattern-based information extraction",
                "author": [
                    "S. Blohm",
                    "P. Cimiano"
                ],
                "venue": "PKDD",
                "citeRegEx": "3",
                "shortCiteRegEx": "3",
                "year": 2007
            },
            {
                "title": "SPOT the drug! An unsupervised pattern matching method to extract drug names from very large clin- ical corpora",
                "author": [
                    "A. Coden",
                    "D. Gruhl",
                    "N. Lewis",
                    "M. Tanenblatt",
                    "J. Terdiman"
                ],
                "venue": "Proceedings - 2012 IEEE 2nd Conference on Healthcare Informatics, Imaging and Systems Biology,",
                "citeRegEx": "5",
                "shortCiteRegEx": "5",
                "year": 2012
            },
            {
                "title": "Inducing domain-specific sentiment lexicons from unlabeled corpora",
                "author": [
                    "W.L. Hamilton",
                    "K. Clark",
                    "J. Leskovec",
                    "D. Jurafsky"
                ],
                "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 595\u2013605",
                "citeRegEx": "6",
                "shortCiteRegEx": "6",
                "year": 2016
            },
            {
                "title": "Corpus-based semantic lexicon induction with web-based cor- roboration",
                "author": [
                    "S.P. Igo",
                    "E. Riloff"
                ],
                "venue": "Proceedings of the Workshop on Unsupervised and Minimally Su- pervised Learning of Lexical Semantics. pp. 18\u201326",
                "citeRegEx": "7",
                "shortCiteRegEx": "7",
                "year": 2009
            },
            {
                "title": "Ad- verse Drug Event Detection in Tweets with Semi-Supervised Convolutional Neural Networks",
                "author": [
                    "K. Lee",
                    "A. Qadir",
                    "S.A. Hasan",
                    "V. Datla",
                    "A. Prakash",
                    "J. Liu",
                    "O. Farri"
                ],
                "venue": null,
                "citeRegEx": "8",
                "shortCiteRegEx": "8",
                "year": 2017
            },
            {
                "title": "Distributed repre- sentations of words and phrases and their compositionality",
                "author": [
                    "T. Mikolov",
                    "I. Sutskever",
                    "K. Chen",
                    "G.S. Corrado",
                    "J. Dean"
                ],
                "venue": "Advances in neural information processing systems. pp",
                "citeRegEx": "9",
                "shortCiteRegEx": "9",
                "year": 2013
            },
            {
                "title": "Terminology Extrac- tion: an analysis of linguistic and statistical approaches",
                "author": [
                    "M.T. Pazienza",
                    "M. Pennacchiotti",
                    "F.M. Zanzotto"
                ],
                "venue": "Knowledge Mining SFSC185(2005),",
                "citeRegEx": "10",
                "shortCiteRegEx": "10",
                "year": 2005
            },
            {
                "title": "Generating Domain-Specific Dictio- naries using Bayesian Learning",
                "author": [
                    "N. Pr\u00f6llochs",
                    "S. Feuerriegel",
                    "D. Neumann"
                ],
                "venue": "Ecis (2015),",
                "citeRegEx": "11",
                "shortCiteRegEx": "11",
                "year": 2015
            },
            {
                "title": "Learning dictionaries for information extraction by multi-level bootstrapping",
                "author": [
                    "E. Riloff",
                    "R Jones"
                ],
                "venue": "In: AAAI/IAAI. pp",
                "citeRegEx": "12",
                "shortCiteRegEx": "12",
                "year": 1999
            },
            {
                "title": "Learning subjective nouns using extraction pat- tern bootstrapping",
                "author": [
                    "E. Riloff",
                    "J. Wiebe",
                    "T. Wilson"
                ],
                "venue": "Proceedings of the Seventh Conference on Natural Lan- guage Learning at HLT-NAACL 2003 - Volume",
                "citeRegEx": "13",
                "shortCiteRegEx": "13",
                "year": 2003
            }
        ],
        "abstractText": "Dictionaries and ontologies are foundational elements of systems extracting knowledge from unstructured text. However, as new content arrives keeping dictionaries up-to-date is a crucial operation. In this paper, we propose a human-in-the-loop (HumL) dictionary expansion approach that employs a lightweight neural language model coupled with tight HumL supervision to assist the user in building and maintaining a domain-specific dictionary from an input text corpus. The approach is based on the explore/exploit paradigm to effectively discover new instances (explore) from the text corpus as well as predict new \u201cunseen\u201d terms not currently in the corpus using the accepted dictionary entries (exploit). We evaluate our approach on a real-world scenario in the healthcare domain, in which we construct a dictionary of adverse drug reactions from user blogs as input text corpus. The evaluation shows that using our approach the user can easily extend the input dictionary, where tight human-in-the-loop integration results in a 216% improvement in effectiveness."
    },
    {
        "title": "Scalable Demand-Aware Recommendation",
        "sections": [
            {
                "heading": "1 Introduction",
                "text": "E-commerce recommender systems aim to present items with high utility to the consumers [18]. Utility may be decomposed into form utility: the item is desired as it is manifested, and time utility: the item is desired at the given point in time [28]; recommender systems should take both types of utility into account. Economists define items to be either durable goods or nondurable goods based on how long they are intended to last before being replaced [27]. A key characteristic of durable goods is the long duration of time between successive purchases within item categories whereas this duration for nondurable goods is much shorter, or even negligible. Thus, durable and nondurable goods have differing time utility characteristics which lead to differing demand characteristics.\nAlthough we have witnessed great success of collaborative filtering in media recommendation, we should be careful when expanding its application to general e-commerce recommendation involving both durable and nondurable goods due to the following reasons:\n1. Since media such as movies and music are nondurable goods, most users are quite receptive to buying or renting them in rapid succession. However, users only purchase durable goods when the time is right. For instance, most users will not buy televisions the day after they have already bought one. Therefore, recommending an item for which a user has no immediate demand can hurt user experience and waste an opportunity to drive sales.\n\u2217Now at Tencent AI Lab, Bellevue, WA, USA\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n2. A key assumption made by matrix factorization- and completion-based collaborative filtering algorithms is that the underlying rating matrix is of low-rank since only a few factors typically contribute to an individual\u2019s form utility [5]. However, a user\u2019s demand is not only driven by form utility, but is the combined effect of both form utility and time utility. Hence, even if the underlying form utility matrix is of low-rank, the overall purchase intention matrix is likely to be of high-rank,2 and thus cannot be directly recovered by existing approaches.\nAn additional challenge faced by many real-world recommender systems is the one-sided sampling of implicit feedback [15, 23]. Unlike the Netflix-like setting that provides both positive and negative feedback (high and low ratings), no negative feedback is available in many e-commerce systems. For example, a user might not purchase an item because she does not derive utility from it, or just because she was simply unaware of it or plans to buy it in the future. In this sense, the labeled training data only draws from the positive class, and the unlabeled data is a mixture of positive and negative samples, a problem usually referred to as positive-unlabeled (PU) learning [13]. To address these issues, we study the problem of demand-aware recommendation. Given purchase triplets (user, item, time) and item categories, the objective is to make recommendations based on users\u2019 overall predicted combination of form utility and time utility.\nWe denote purchases by the sparse binary tensor P . To model implicit feedback, we assume that P is obtained by thresholding an underlying real-valued utility tensor to a binary tensor Y and then revealing a subset of Y\u2019s positive entries. The key to demand-aware recommendation is defining an appropriate utility measure for all (user, item, time) triplets. To this end, we quantify purchase intention as a combined effect of form utility and time utility. Specifically, we model a user\u2019s time utility for an item by comparing the time t since her most recent purchase within the item\u2019s category and the item category\u2019s underlying inter-purchase duration d; the larger the value of d\u2212 t, the less likely she needs this item. In contrast, d \u2264 t may indicate that the item needs to be replaced, and she may be open to related recommendations. Therefore, the function h = max(0, d\u2212 t) may be employed to measure the time utility factor for a (user, item) pair. Then the purchase intention for a (user, item, time) triplet is given by x\u2212 h, where x denotes the user\u2019s form utility. This observation allows us to cast demand-aware recommendation as the problem of learning users\u2019 form utility tensor X and items\u2019 inter-purchase durations vector d given the binary tensor P . Although the learning problem can be naturally formulated as a tensor nuclear norm minimization problem, the high computational cost significantly limits its application to large-scale recommendation problems. To address this limitation, we first relax the problem to a matrix optimization problem with a label-dependent loss. We note that the problem after relaxation is still non-trivial to solve since it is a highly non-smooth problem with nested hinge losses. More severely, the optimization problem involves mnl entries, where m, n, and l are the number of users, items, and time slots, respectively. Thus a naive optimization algorithm will take at least O(mnl) time, and is intractable for largescale recommendation problems. To overcome this limitation, we develop an efficient alternating minimization algorithm and show that its time complexity is only approximately proportional to the number of nonzero elements in the purchase records tensor P . Since P is usually very sparse, our algorithm is extremely efficient and can solve problems with millions of users and items.\nCompared to existing recommender systems, our work has the following contributions and advantages: (i) to the best of our knowledge, this is the first work that makes demand-aware recommendation by considering inter-purchase durations for durable and nondurable goods; (ii) the proposed algorithm is able to simultaneously infer items\u2019 inter-purchase durations and users\u2019 real-time purchase intentions, which can help e-retailers make more informed decisions on inventory planning and marketing strategy; (iii) by effectively exploiting sparsity, the proposed algorithm is extremely efficient and able to handle large-scale recommendation problems."
            },
            {
                "heading": "2 Related Work",
                "text": "Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective [6], time-aware recommender systems [4, 29, 8, 19], and PU learning [20, 9, 13, 14, 23, 2]. The extensive consumer modeling literature is concerned with descriptive and analytical models of choice rather than prediction or recommendation, but nonetheless\n2A detailed illustration can be found in the supplementary material\nforms the basis for our modeling approach. A variety of time-aware recommender systems have been proposed to exploit time information, but none of them explicitly consider the notion of time utility derived from inter-purchase durations in item categories. Much of the PU learning literature is focused on the binary classification problem, e.g. [20, 9], whereas we are in the collaborative filtering setting. For the papers that do examine collaborative filtering with PU learning or learning with implicit feedback [14, 23, 2, 32], they mainly focus on media recommendation and overlook users\u2019 demands, thus are not suitable for durable goods recommendation.\nTemporal aspects of the recommendation problem have been examined in a few ways: as part of the cold-start problem [3], to capture dynamics in interests or ratings over time [17], and as part of the context in context-aware recommenders [1]. However, the problem we address in this paper is different from all of those aspects, and in fact could be combined with the other aspects in future solutions. To the best of our knowledge, there is no existing work that tries to take inter-purchase durations into account to better time recommendations as we do herein."
            },
            {
                "heading": "3 Positive-Unlabeled Demand-Aware Recommendation",
                "text": "Throughout the paper, we use boldface Euler script letters, boldface capital letters, and boldface lower-case letters to denote tensors (e.g., A), matrices (e.g., A) and vectors (e.g., a), respectively. Scalars such as entries of tensors, matrices, and vectors are denoted by lowercase letters, e.g., a. In particular, the (i, j, k) entry of a third-order tensor A is denoted by aijk. Given a set of m users, n items, and l time slots, we construct a third-order binary tensor P \u2208 {0, 1}m\u00d7n\u00d7l to represent the purchase history. Specifically, entry pijk = 1 indicates that user i has purchased item j in time slot k. We denote \u2016P\u20160 as the number of nonzero entries in tensor P . Since P is usually very sparse, we have \u2016P\u20160 mnl. Also, we assume that the n items belong to r item categories, with items in each category sharing similar inter-purchase durations.3 We use an n-dimensional vector c \u2208 {1, 2, . . . , r}n to represent the category membership of each item. Given P and c, we further generate a tensor T \u2208 Rm\u00d7r\u00d7l where ticjk denotes the number of time slots between user i\u2019s most recent purchase within item category cj until time k. If user i has not purchased within item category cj until time k, ticjk is set to +\u221e."
            },
            {
                "heading": "3.1 Inferring Purchase Intentions from Users\u2019 Purchase Histories",
                "text": "In this work, we formulate users\u2019 utility as a combined effect of form utility and time utility. To this end, we use an underlying third-order tensor X \u2208 Rm\u00d7n\u00d7l to quantify form utility. In addition, we employ a non-negative vector d \u2208 Rr+ to measure the underlying inter-purchase duration times of the r item categories. It is understood that the inter-purchase durations for durable good categories are large, while for nondurable good categories are small, or even zero. In this study, we focus on items\u2019 inherent properties and assume that the inter-purchase durations are user-independent. The problem of learning personalized durations will be studied in our future work.\nAs discussed above, the demand is mediated by the time elapsed since the last purchase of an item in the same category. Let dcj be the inter-purchase duration time of item j\u2019s category cj , and let ticjk be the time gap of user i\u2019s most recent purchase within item category cj until time k. Then if dcj > ticjk, a previously purchased item in category cj continues to be useful, and thus user i\u2019s utility from item j is weak. Intuitively, the greater the value dcj \u2212 ticjk, the weaker the utility. On the other hand, dcj < ticjk indicates that the item is nearing the end of its lifetime and the user may be open to recommendations in category cj . We use a hinge loss max(0, dcj \u2212 ticjk) to model such time utility. The overall utility can be obtained by comparing form utility and time utility. In more detail, we model a binary utility indicator tensor Y \u2208 {0, 1}m\u00d7n\u00d7l as being generated by the following thresholding process:\nyijk = 1[xijk \u2212max(0, dcj \u2212 ticjk) > \u03c4 ], (1)\nwhere 1(\u00b7) : R\u2192 {0, 1} is the indicator function, and \u03c4 > 0 is a predefined threshold.\n3To meet this requirement, the granularity of categories should be properly selected. For instance, the category \u201cSmart TV\u201d is a better choice than the category \u201cElectrical Equipment\u201d, since the latter category covers a broad range of goods with different durations.\nNote that the positive entries of Y denote high purchase intentions, while the positive entries of P denote actual purchases. Generally speaking, a purchase only happens when the utility is high, but a high utility does not necessarily lead to a purchase. This observation allows us to link the binary tensors P and Y: P is generated by a one-sided sampling process that only reveals a subset of Y\u2019s positive entries. Given this observation, we follow [13] and include a label-dependent loss [26] trading the relative cost of positive and unlabeled samples:\nL(X ,P)= \u03b7 \u2211\nijk: pijk=1\nmax[1\u2212 (xijk \u2212max(0, dcj \u2212 ticjk)), 0]2 + (1\u2212 \u03b7) \u2211\nijk: pijk=0\nl(xijk, 0),\nwhere l(x, c) = (x\u2212 c)2 denotes the squared loss. In addition, the form utility tensor X should be of low-rank to capture temporal dynamics of users\u2019 interests, which are generally believed to be dictated by a small number of latent factors [22].\nBy combining asymmetric sampling and the low-rank property together, we jointly recover the tensor X and the inter-purchase duration vector d by solving the following tensor nuclear norm minimization (TNNM) problem:\nmin X\u2208Rm\u00d7n\u00d7l, d\u2208Rr+\n\u03b7 \u2211\nijk: pijk=1\nmax[1\u2212 (xijk \u2212max(0, dcj \u2212 ticjk)), 0]2\n+ (1\u2212 \u03b7) \u2211\nijk: pijk=0\nx2ijk + \u03bb \u2016X\u2016\u2217, (2)\nwhere \u2016X\u2016\u2217 denotes the tensor nuclear norm, a convex combination of nuclear norms of X \u2019s unfolded matrices [21]. Given the learned X\u0302 and d\u0302, the underlying binary tensor Y can be recovered by (1). We note that although the TNNM problem (2) can be solved by optimization techniques such as block coordinate descent [21] and ADMM [10], they suffer from high computational cost since they need to be solved iteratively with multiple SVDs at each iteration. An alternative way to solve the problem is tensor factorization [16]. However, this also involves iterative singular vector estimation and thus not scalable enough. As a typical example, recovering a rank 20 tensor of size 500\u00d7 500\u00d7 500 takes the state-of-the-art tensor factorization algorithm TenALS 4 more than 20, 000 seconds on an Intel Xeon 2.40 GHz processor with 32 GB main memory."
            },
            {
                "heading": "3.2 A Scalable Relaxation",
                "text": "In this subsection, we discuss how to significantly improve the scalability of the proposed demandaware recommendation model. To this end, we assume that an individual\u2019s form utility does not change over time, an assumption widely-used in many collaborative filtering methods [25, 32]. Under this assumption, the tensor X is a repeated copy of its frontal slice x::1, i.e.,\nX = x::1 \u25e6 e, (3)\nwhere e is an l-dimensional all-one vector and the symbol \u25e6 represents the outer product operation. In this way, we can relax the problem of learning a third-order tensor X to the problem of learning its frontal slice, which is a second-order tensor (matrix). For notational simplicity, we use a matrix X to denote the frontal slice x::1, and use xij to denote the entry (i, j) of the matrix X.\nSince X is a low-rank tensor, its frontal slice X should be of low-rank as well. Hence, the minimization problem (2) simplifies to:\nmin X\u2208Rm\u00d7n\nd\u2208Rr\n\u03b7 \u2211\nijk: pijk=1\nmax[1\u2212 (xij \u2212max(0, dcj \u2212 ticjk)), 0]2\n+ (1\u2212 \u03b7) \u2211\nijk: pijk=0\nx2ij + \u03bb \u2016X\u2016\u2217 := f(X,d), (4)\nwhere \u2016X\u2016\u2217 stands for the matrix nuclear norm, the convex surrogate of the matrix rank function. By relaxing the optimization problem (2) to the problem (4), we recover a matrix instead of a tensor to infer users\u2019 purchase intentions.\n4http://web.engr.illinois.edu/~swoh/software/optspace/code.html"
            },
            {
                "heading": "4 Optimization",
                "text": "Although the learning problem has been relaxed, optimizing (4) is still very challenging for two main reasons: (i) the objective is highly non-smooth with nested hinge losses, and (ii) it contains mnl terms, and a naive optimization algorithm will take at least O(mnl) time.\nTo address these challenges, we adopt an alternating minimization scheme that iteratively fixes one of d and X and minimizes with respect to the other. Specifically, we propose an extremely efficient optimization algorithm by effectively exploring the sparse structure of the tensor P and low-rank structure of the matrix X. We show that (i) the problem (4) can be solved within O(\u2016P\u20160(k + log(\u2016P\u20160)) + (n+m)k2) time, where k is the rank of X, and (ii) the algorithm converges to the critical points of f(X,d). In the following, we provide a sketch of the algorithm. The detailed description can be found in the supplementary material."
            },
            {
                "heading": "4.1 Update d",
                "text": "When X is fixed, the optimization problem with respect to d can be written as:\nmin d \u2211 ijk: pijk=1\n{ max ( 1\u2212 (xij \u2212max(0, dcj \u2212 ticjk)), 0 )2} := g(d) := \u2211 ijk: pijk=1 gijk(dcj ). (5)\nProblem (5) is non-trivial to solve since it involves nested hinge losses. Fortunately, by carefully analyzing the value of each term gijk(dcj ), we can show that\ngijk(dcj ) = { max(1\u2212 xij , 0)2, if dcj \u2264 ticjk + max(xij \u2212 1, 0) (1\u2212 (xij \u2212 dcj + ticjk))2, if dcj > ticjk + max(xij \u2212 1, 0).\nFor notational simplicity, we let sijk = ticjk + max(xij \u2212 1, 0) for all triplets (i, j, k) satisfying pijk = 1. Now we can focus on each category \u03ba: for each \u03ba, we collect the set Q = {(i, j, k) | pijk = 1 and cj = \u03ba} and calculate the corresponding sijks. We then sort sijks such that s(i1j1k1) \u2264 \u00b7 \u00b7 \u00b7 \u2264 s(i|Q|j|Q|k|Q|). For each interval [s(iqjqkq), s(iq+1jq+1kq+1)], the function is quadratic, thus can be solved in a closed form. Therefore, by scanning the solution regions from left to right according to the sorted s values, and maintaining some intermediate computed variables, we are able to find the optimal solution, as summarized by the following lemma: Lemma 1. The subproblem (5) is convex with respect to d and can be solved exactly in O(\u2016P\u20160 log(\u2016P\u20160)), where \u2016P\u20160 is the number of nonzero elements in tensor P .\nTherefore, we can efficiently update d since P is a very sparse tensor with only a small number of nonzero elements."
            },
            {
                "heading": "4.2 Update X",
                "text": "By defining\naijk = { 1 + max(0, dcj \u2212 ticjk), if pijk = 1 0, otherwise\nthe subproblem with respect to X can be written as\nmin X\u2208Rm\u00d7n\nh(X)+\u03bb\u2016X\u2016\u2217 where h(X) := { \u03b7 \u2211 ijk: pijk=1 max(aijk\u2212xij , 0)2+(1\u2212\u03b7) \u2211 ijk: pijk=0 x2ij } .\n(6) Since there are O(mnl) terms in the objective function, a naive implementation will take at least O(mnl) time, which is computationally infeasible when the data is large. To address this issue, We use proximal gradient descent to solve the problem. At each iteration, X is updated by\nX\u2190 S\u03bb(X\u2212 \u03b1\u2207h(X)), (7) where S\u03bb(\u00b7) is the soft-thresholding operator for singular values.5\n5If X has the singular value decomposition X = U\u03a3VT , then S\u03bb(X) = U(\u03a3 \u2212 \u03bbI)+VT where a+ = max(0, a).\nIn order to efficiently compute the top singular vectors of X\u2212 \u03b1\u2207h(X), we rewrite it as\nX\u2212 \u03b1\u2207h(X) = [1\u2212 2(1\u2212 \u03b7)l] X + 2(1\u2212 \u03b7) \u2211 ijk: pijk=1 xij \u2212 2\u03b7 \u2211 ijk: pijk=1 max(aijk \u2212 xij , 0)  . = fa(X) + fb(X).\nSince fa(X) is of low-rank and fb(X) is sparse, multiplying (X \u2212 \u03b1\u2207h(X)) with a skinny m by k matrix can be computed in O(nk2 + mk2 + \u2016P\u20160k) time. As shown in [12], each iteration of proximal gradient descent for nuclear norm minimization only requires a fixed number of iterations of randomized SVD (or equivalently, power iterations) using the warm start strategy, thus we have the following lemma.\nLemma 2. A proximal gradient descent algorithm can be applied to solve problem (6) within O(nk2T +mk2T + \u2016P\u20160kT ) time, where T is the number of iterations.\nWe note that the algorithm is guaranteed to converge to the true solution. This is because when we apply a fixed number of iterations to update X via problem (7), it is equivalent to the \u201cinexact gradient descent update\u201d where each gradient is approximately computed and the approximation error is upper bounded by a constant between zero and one. Intuitively speaking, when the gradient converges to 0, the error will also converge to 0 at an even faster rate. See [12] for the detailed explanations."
            },
            {
                "heading": "4.3 Overall Algorithm",
                "text": "Combining the two subproblems together, the time complexity of each iteration of the proposed algorithm is:\nO(\u2016P\u20160 log(\u2016P\u20160) + nk2T +mk2T + \u2016P\u20160kT ).\nRemark: Since each user should make at least one purchase and each item should be purchased at least once to be included in P , n and m are smaller than \u2016P\u20160. Also, since k and T are usually very small, the time complexity to solve problem (4) is dominated by the term \u2016P\u20160, which is a significant improvement over the naive approach with at least O(mnl) complexity.\nSince our problem has only two blocks d, X and each subproblem is convex, our optimization algorithm is guaranteed to converge to a stationary point [11]. Indeed, it converges very fast in practice. As a concrete example, our experiment shows that it takes only 9 iterations to optimize a problem with 1 million users, 1 million items, and more than 166 million purchase records."
            },
            {
                "heading": "5 Experiments",
                "text": ""
            },
            {
                "heading": "5.1 Experiment with Synthesized Data",
                "text": "We first conduct experiments with simulated data to verify that the proposed demand-aware recommendation algorithm is computationally efficient and robust to noise. To this end, we first construct a low-rank matrix X = WHT , where W \u2208 Rm\u00d710 and H \u2208 Rn\u00d710 are random Gaussian matrices with entries drawn from N (1, 0.5), and then normalize X to the range of [0, 1]. We randomly assign all the n items to r categories, with their inter-purchase durations d equaling [10, 20, . . . , 10r]. We then construct the high purchase intension set \u2126 = {(i, j, k) | ticjk \u2265 dcj and xij \u2265 0.5}, and sample a subset of its entries as the observed purchase records. We let n = m and vary them in the range {10, 000, 20, 000, 30, 000, 40, 000}. We also vary r in the range {10, 20, \u00b7 \u00b7 \u00b7 , 100}. Given the learned durations d\u2217, we use \u2016d\u2212 d\u2217\u20162/\u2016d\u20162 to measure the prediction errors.\nAccuracy Figure 1(a) and 1(b) clearly show that the proposed algorithm can perfectly recover the underlying inter-purchase durations with varied numbers of users, items, and categories. To further evaluate the robustness of the proposed algorithm, we randomly flip some entries in tensor P from 0 to 1 to simulate the rare cases of purchasing two items in the same category in close temporal succession. Figure 1(c) shows that when the ratios of noisy entries are not large, the predicted durations d\u0302 are close enough to the true durations, thus verifying the robustness of the proposed algorithm.\nScalability To verify the scalability of the proposed algorithm, we fix the numbers of users and items to be 1 million, the number of time slots to be 1, 000, and vary the number of purchase records (i.e., \u2016P\u20160). Table 1 summarizes the CPU time of solving problem (4) on an Intel Xeon 2.40 GHz server with 32 GB main memory. We observe that the proposed algorithm is extremely efficient, e.g., even with 1 million users, 1 million items, and more than 166 million purchase records, the running time of the proposed algorithm is less than 2 hours."
            },
            {
                "heading": "5.2 Experiment with Real-World Data",
                "text": "In the real-world experiments, we evaluate the proposed demand-aware recommendation algorithm by comparing it with the six state-of the-art recommendation methods: (a) M3F, maximum-margin matrix factorization [24], (b) PMF, probabilistic matrix factorization [25], (c) WR-MF, weighted regularized matrix factorization [14], (d) CP-APR, Candecomp-Parafac alternating Poisson regression [7], (e) Rubik, knowledge-guided tensor factorization and completion method [30], and (f) BPTF, Bayesian probabilistic tensor factorization [31]. Among them, M3F and PMF are widely-used static collaborative filtering algorithms. We include these two algorithms as baselines to justify whether traditional collaborative filtering algorithms are suitable for general e-commerce recommendation involving both durable and nondurable goods. Since they require explicit ratings as inputs, we follow [2] to generate numerical ratings based on the frequencies of (user, item) consumption pairs. WR-MF is essentially the positive-unlabeled version of PMF and has shown to be very effective in modeling implicit feedback data. All the other three baselines, i.e., CP-APR, Rubik, and BPTF, are tensor-based methods that can consider time utility when making recommendations. We refer to the proposed recommendation algorithm as Demand-Aware Recommender for One-Sided Sampling, or DAROSS for short.\nOur testbeds are two real-world datasets Tmall6 and Amazon Review7. Since some of the baseline algorithms are not scalable enough, we first conduct experiments on their subsets and then on the full set of Amazon Review. In order to generate the subsets, we randomly sample 80 item categories for Tmall dataset and select the users who have purchased at least 3 items within these categories, leading to the purchase records of 377 users and 572 items. For Amazon Review dataset, we randomly select 300 users who have provided reviews to at least 5 item categories on Amazon.com. This leads to a total of 5, 111 items belonging to 11 categories. Time information for both datasets is provided in days, and we have 177 and 749 time slots for Tmall and Amazon Review subsets, respectively. The full Amazon Review dataset is significantly larger than its subset. After removing duplicate items, it contains more than 72 million product reviews from 19.8 million users and 7.7 million items that\n6http://ijcai-15.org/index.php/repeat-buyers-prediction-competition 7http://jmcauley.ucsd.edu/data/amazon/\nbelong to 24 item categories. The collected reviews span a long range of time: from May 1996 to July 2014, which leads to 6, 639 time slots in total. Comparing to its subset, the full set is a much more challenging dataset both due to its much larger size and much higher sparsity, i.e., many reviewers only provided a few reviews, and many items were only reviewed a small number of times.\nFor each user, we randomly sample 90% of her purchase records as the training data, and use the remaining 10% as the test data. For each purchase record (u, i, t) in the test set, we evaluate all the algorithms on two tasks: (i) category prediction, and (ii) purchase time prediction. In the first task, we record the highest ranking of items that are within item i\u2019s category among all items at time t. Since a purchase record (u, i, t) may suggest that in time slot t, user u needed an item that share similar functionalities with item i, category prediction essentially checks whether the recommendation algorithms recognize this need. In the second task, we record the number of slots between the true purchase time t and its nearest predicted purchase time within item i\u2019s category. Ideally, good recommendations should have both small category rankings and small time errors. Thus we adopt the average top percentages, i.e., (average category ranking) / n \u00d7 100% and (average time error) / l \u00d7 100%, as the evaluation metrics of category and purchase time prediction tasks, respectively. The algorithms M3F, PMF, and WR-MF are excluded from the purchase time prediction task since they are static models that do not consider time information.\nFigure 2 displays the predictive performance of the seven recommendation algorithms on Tmall and Amazon Review subsets. As expected, M3F and PMF fail to deliver strong performance since they neither take into account users\u2019 demands, nor consider the positive-unlabeled nature of the data. This is verified by the performance of WR-MF: it significantly outperforms M3F and PMF by considering the PU issue and obtains the second-best item prediction accuracy on both datasets (while being unable to provide a purchase time prediction). By taking into account both issues, our proposed algorithm DAROSS yields the best performance for both datasets and both tasks. Table 2 reports the inter-review durations of Amazon Review subset estimated by our algorithm. Although they may not perfectly reflect the true inter-purchase durations, the estimated durations clearly distinguish between durable good categories, e.g., automotive, musical instruments, and non-durable good categories, e.g., instant video, apps, and food. Indeed, the learned inter-purchase durations can also play an important role in applications more advanced than recommender systems, such as inventory management, operations management, and sales/marketing mechanisms. We do not report the estimated durations of Tmall herein since the item categories are anonymized in the dataset.\nFinally, we conduct experiments on the full Amazon Review dataset. In this study, we replace category prediction with a more strict evaluation metric item prediction [8], which indicates the predicted ranking of item i among all items at time t for each purchase record (u, i, t) in the test set. Since most of our baseline algorithms fail to handle such a large dataset, we only obtain the predictive performance of three algorithms: DAROSS, WR-MF, and PMF. Note that for such a large dataset, prediction time instead of training time becomes the bottleneck: to evaluate average item rankings, we\nneed to compute the scores of all the 7.7 million items, thus is computationally inefficient. Therefore, we only sample a subset of items for each user and estimate the rankings of her purchased items. Using this evaluation method, the average item ranking percentages for DAROSS, WR-MF, and PMF are 16.7%, 27.3%, and 38.4%, respectively. In addition to superior performance, it only takes our algorithm 10 iterations and 1 hour to converge to a good solution. Since WR-MF and PMF are both static models, our algorithm is the only approach evaluated here that considers time utility while being scalable enough to handle the full Amazon Review dataset. Note that this dataset has more users, items, and time slots but fewer purchase records than our largest synthesized dataset, and the running time of the former dataset is lower than the latter one. This clearly verifies that the time complexity of our algorithm is dominated by the number of purchase records instead of the tensor size. Interestingly, we found that some inter-review durations estimated from the full Amazon Review dataset are much smaller than the durations estimated from its subset. This is because the durations may be underestimated when many users reviewed items within a same durable goods category in close temporal succession. On the other hand, this result verifies the effectiveness of the PU formulation \u2013 even if the durations are underestimated, our algorithm still outperforms the competitors by a considerable margin. As a final note, we want to point out that Tmall and Amazon Review may not take full advantage of the proposed algorithm, since (i) their categories are relatively coarse and may contain multiple sub-categories with different durations, and (ii) the time stamps of Amazon Review reflect the review time instead of purchase time, and inter-review durations could be different from inter-purchase durations. By choosing a purchase history dataset with a more appropriate category granularity, we may obtain more accurate duration estimations and also a better recommendation performance."
            },
            {
                "heading": "6 Conclusion",
                "text": "In this paper, we examine the problem of demand-aware recommendation in settings when interpurchase duration within item categories affects users\u2019 purchase intention in combination with intrinsic properties of the items themselves. We formulate it as a tensor nuclear norm minimization problem that seeks to jointly learn the form utility tensor and a vector of inter-purchase durations, and propose a scalable optimization algorithm with a tractable time complexity. Our empirical studies show that the proposed approach can yield perfect recovery of duration vectors in noiseless settings; it is robust to noise and scalable as analyzed theoretically. On two real-world datasets, Tmall and Amazon Review, we show that our algorithm outperforms six state-of-the-art recommendation algorithms on the tasks of category, item, and purchase time predictions."
            },
            {
                "heading": "Acknowledgements",
                "text": "Cho-Jui Hsieh and Yao Li acknowledge the support of NSF IIS-1719097, TACC and Nvidia."
            }
        ],
        "references": [
            {
                "title": "Context-aware recommender systems",
                "author": [
                    "Gediminas Adomavicius",
                    "Alexander Tuzhilin"
                ],
                "venue": "In Recommender Systems Handbook,",
                "citeRegEx": "1",
                "shortCiteRegEx": "1",
                "year": 2011
            },
            {
                "title": "Towards time-dependant recommendation based on implicit feedback",
                "author": [
                    "Linas Baltrunas",
                    "Xavier Amatriain"
                ],
                "venue": "In Workshop on context-aware recommender systems,",
                "citeRegEx": "2",
                "shortCiteRegEx": "2",
                "year": 2009
            },
            {
                "title": "A collaborative filtering approach to mitigate the new user cold start problem",
                "author": [
                    "Jes\u00fas Bobadilla",
                    "Fernando Ortega",
                    "Antonio Hernando",
                    "Jes\u00fas Bernal"
                ],
                "venue": "Knowl.-Based Syst.,",
                "citeRegEx": "3",
                "shortCiteRegEx": "3",
                "year": 2012
            },
            {
                "title": "Time-aware recommender systems: a comprehensive survey and analysis of existing evaluation protocols",
                "author": [
                    "Pedro G. Campos",
                    "Fernando D\u00edez",
                    "Iv\u00e1n Cantador"
                ],
                "venue": "User Model. User-Adapt. Interact.,",
                "citeRegEx": "4",
                "shortCiteRegEx": "4",
                "year": 2014
            },
            {
                "title": "Exact matrix completion via convex optimization",
                "author": [
                    "Emmanuel J. Cand\u00e8s",
                    "Benjamin Recht"
                ],
                "venue": "Foundations of Computational Mathematics,",
                "citeRegEx": "5",
                "shortCiteRegEx": "5",
                "year": 2009
            },
            {
                "title": "A consumer purchasing model with erlang inter-purchase times",
                "author": [
                    "Christopher Chatfield",
                    "Gerald J Goodhardt"
                ],
                "venue": "Journal of the American Statistical Association,",
                "citeRegEx": "6",
                "shortCiteRegEx": "6",
                "year": 1973
            },
            {
                "title": "On tensors, sparsity, and nonnegative factorizations",
                "author": [
                    "Eric C. Chi",
                    "Tamara G. Kolda"
                ],
                "venue": "SIAM Journal on Matrix Analysis and Applications,",
                "citeRegEx": "7",
                "shortCiteRegEx": "7",
                "year": 2012
            },
            {
                "title": "Time-sensitive recommendation from recurrent user activities",
                "author": [
                    "Nan Du",
                    "Yichen Wang",
                    "Niao He",
                    "Jimeng Sun",
                    "Le Song"
                ],
                "venue": "In NIPS,",
                "citeRegEx": "8",
                "shortCiteRegEx": "8",
                "year": 2015
            },
            {
                "title": "Analysis of learning from positive and unlabeled data",
                "author": [
                    "Marthinus Christoffel du Plessis",
                    "Gang Niu",
                    "Masashi Sugiyama"
                ],
                "venue": "In NIPS,",
                "citeRegEx": "9",
                "shortCiteRegEx": "9",
                "year": 2014
            },
            {
                "title": "Tensor completion and low-n-rank tensor recovery via convex optimization",
                "author": [
                    "Silvia Gandy",
                    "Benjamin Recht",
                    "Isao Yamada"
                ],
                "venue": "Inverse Problems,",
                "citeRegEx": "10",
                "shortCiteRegEx": "10",
                "year": 2011
            },
            {
                "title": "On the convergence of the block nonlinear Gauss-Seidel method under convex constraints",
                "author": [
                    "L. Grippo",
                    "M. Sciandrone"
                ],
                "venue": "Operations Research Letters,",
                "citeRegEx": "11",
                "shortCiteRegEx": "11",
                "year": 2000
            },
            {
                "title": "Nuclear norm minimization via active subspace selection",
                "author": [
                    "C.-J. Hsieh",
                    "P.A. Olsen"
                ],
                "venue": "In ICML,",
                "citeRegEx": "12",
                "shortCiteRegEx": "12",
                "year": 2014
            },
            {
                "title": "PU learning for matrix completion",
                "author": [
                    "Cho-Jui Hsieh",
                    "Nagarajan Natarajan",
                    "Inderjit S. Dhillon"
                ],
                "venue": "In ICML,",
                "citeRegEx": "13",
                "shortCiteRegEx": "13",
                "year": 2015
            },
            {
                "title": "Collaborative filtering for implicit feedback datasets",
                "author": [
                    "Y. Hu",
                    "Y. Koren",
                    "C. Volinsky"
                ],
                "venue": "In ICDM,",
                "citeRegEx": "14",
                "shortCiteRegEx": "14",
                "year": 2008
            },
            {
                "title": "Collaborative filtering for implicit feedback datasets",
                "author": [
                    "Yifan Hu",
                    "Yehuda Koren",
                    "Chris Volinsky"
                ],
                "venue": "In ICDM,",
                "citeRegEx": "15",
                "shortCiteRegEx": "15",
                "year": 2008
            },
            {
                "title": "Provable tensor factorization with missing data",
                "author": [
                    "P. Jain",
                    "S. Oh"
                ],
                "venue": "In NIPS,",
                "citeRegEx": "16",
                "shortCiteRegEx": "16",
                "year": 2014
            },
            {
                "title": "Collaborative filtering with temporal dynamics",
                "author": [
                    "Yehuda Koren"
                ],
                "venue": "Commun. ACM,",
                "citeRegEx": "17",
                "shortCiteRegEx": "17",
                "year": 2010
            },
            {
                "title": "Impact of recommender systems on sales volume and diversity",
                "author": [
                    "Dokyun Lee",
                    "Kartik Hosanagar"
                ],
                "venue": "In Proc. Int. Conf. Inf. Syst.,",
                "citeRegEx": "18",
                "shortCiteRegEx": "18",
                "year": 2014
            },
            {
                "title": "Cross-domain collaborative filtering over time",
                "author": [
                    "Bin Li",
                    "Xingquan Zhu",
                    "Ruijiang Li",
                    "Chengqi Zhang",
                    "Xiangyang Xue",
                    "Xindong Wu"
                ],
                "venue": "In IJCAI,",
                "citeRegEx": "19",
                "shortCiteRegEx": "19",
                "year": 2011
            },
            {
                "title": "Building text classifiers using positive and unlabeled examples",
                "author": [
                    "Bing Liu",
                    "Yang Dai",
                    "Xiaoli Li",
                    "Wee Sun Lee",
                    "Philip S. Yu"
                ],
                "venue": "In ICML,",
                "citeRegEx": "20",
                "shortCiteRegEx": "20",
                "year": 2003
            },
            {
                "title": "Tensor completion for estimating missing values in visual data",
                "author": [
                    "Ji Liu",
                    "Przemyslaw Musialski",
                    "Peter Wonka",
                    "Jieping Ye"
                ],
                "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
                "citeRegEx": "21",
                "shortCiteRegEx": "21",
                "year": 2013
            },
            {
                "title": "Tensor factorization using auxiliary information",
                "author": [
                    "Atsuhiro Narita",
                    "Kohei Hayashi",
                    "Ryota Tomioka",
                    "Hisashi Kashima"
                ],
                "venue": "In ECML/PKDD,",
                "citeRegEx": "22",
                "shortCiteRegEx": "22",
                "year": 2011
            },
            {
                "title": "BPR: bayesian personalized ranking from implicit feedback",
                "author": [
                    "Steffen Rendle",
                    "Christoph Freudenthaler",
                    "Zeno Gantner",
                    "Lars Schmidt-Thieme"
                ],
                "venue": "In UAI,",
                "citeRegEx": "23",
                "shortCiteRegEx": "23",
                "year": 2009
            },
            {
                "title": "Fast maximum margin matrix factorization for collaborative prediction",
                "author": [
                    "Jason D.M. Rennie",
                    "Nathan Srebro"
                ],
                "venue": "In ICML,",
                "citeRegEx": "24",
                "shortCiteRegEx": "24",
                "year": 2005
            },
            {
                "title": "Bayesian probabilistic matrix factorization using markov chain monte carlo",
                "author": [
                    "Ruslan Salakhutdinov",
                    "Andriy Mnih"
                ],
                "venue": "In ICML,",
                "citeRegEx": "25",
                "shortCiteRegEx": "25",
                "year": 2008
            },
            {
                "title": "Calibrated asymmetric surrogate losses",
                "author": [
                    "Clayton Scott"
                ],
                "venue": "Electronic Journal of Statistics,",
                "citeRegEx": "26",
                "shortCiteRegEx": "26",
                "year": 2012
            },
            {
                "title": "Exploring Economics",
                "author": [
                    "Robert L. Sexton"
                ],
                "venue": "Cengage Learning,",
                "citeRegEx": "27",
                "shortCiteRegEx": "27",
                "year": 2013
            },
            {
                "title": "The prejudice against marketing",
                "author": [
                    "Robert L. Steiner"
                ],
                "venue": "J. Marketing,",
                "citeRegEx": "28",
                "shortCiteRegEx": "28",
                "year": 1976
            },
            {
                "title": "Collaborative Kalman filtering for dynamic matrix factorization",
                "author": [
                    "John Z. Sun",
                    "Dhruv Parthasarathy",
                    "Kush R. Varshney"
                ],
                "venue": "IEEE Trans. Signal Process.,",
                "citeRegEx": "29",
                "shortCiteRegEx": "29",
                "year": 2014
            },
            {
                "title": "Rubik: Knowledge guided tensor factorization and completion for health data analytics",
                "author": [
                    "Yichen Wang",
                    "Robert Chen",
                    "Joydeep Ghosh",
                    "Joshua C. Denny",
                    "Abel N. Kho",
                    "You Chen",
                    "Bradley A. Malin",
                    "Jimeng Sun"
                ],
                "venue": "In SIGKDD,",
                "citeRegEx": "30",
                "shortCiteRegEx": "30",
                "year": 2015
            },
            {
                "title": "Temporal collaborative filtering with bayesian probabilistic tensor factorization",
                "author": [
                    "X. Liang",
                    "C. Xi",
                    "H. Tzu-Kuo",
                    "S. Jeff G",
                    "C. Jaime G"
                ],
                "venue": "In SDM,",
                "citeRegEx": "31",
                "shortCiteRegEx": "31",
                "year": 2010
            },
            {
                "title": "Inferring users\u2019 preferences from crowdsourced pairwise comparisons: A matrix completion approach",
                "author": [
                    "Jinfeng Yi",
                    "Rong Jin",
                    "Shaili Jain",
                    "Anil K. Jain"
                ],
                "venue": "In First AAAI Conference on Human Computation and Crowdsourcing (HCOMP),",
                "citeRegEx": "32",
                "shortCiteRegEx": "32",
                "year": 2013
            }
        ],
        "abstractText": "Recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. The demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. In particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. Moreover, purchase data, in contrast to rating data, is implicit with non-purchases not necessarily indicating dislike. Together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation. We further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and millions of items in a single thread. We also show superior prediction accuracies on multiple real-world datasets."
    },
    {
        "title": "An Editorial Network for Enhanced Document Summarization",
        "sections": [
            {
                "heading": null,
                "text": "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 57\u201363 Hong Kong, China, November 4, 2019. c\u00a92019 Association for Computational Linguistics\n57"
            },
            {
                "heading": "1 Introduction",
                "text": "Automatic text summarizers condense a given piece of text into a shorter version (the summary). This is done while trying to preserve the main essence of the original text and keeping the generated summary as readable as possible.\nExisting summarization methods can be classified into two main types, either extractive or abstractive. Extractive methods select and order text fragments (e.g., sentences) from the original text source. Such methods are relatively simpler to develop and keep the extracted fragments untouched, allowing to preserve important parts, e.g., keyphrases, facts, opinions, etc. Yet, extractive summaries tend to be less fluent, coherent and readable and may include superfluous text.\nAbstractive methods apply natural language paraphrasing and/or compression on a given text. A common approach is based on the encoder-decoder (seq-to-seq) paradigm (Sutskever et al., 2014), with the original text sequence being encoded while the summary is the decoded sequence.\n\u2217Work was done during a summer internship in IBM Research AI\nWhile such methods usually generate summaries with better readability, their quality declines over longer textual inputs, which may lead to a higher redundancy (Paulus et al., 2017). Moreover, such methods are sensitive to vocabulary size, making them more difficult to train and generalize (See et al., 2017).\nA common approach for handling long text sequences in abstractive settings is through attention mechanisms, which aim to imitate the attentive reading behaviour of humans (Chopra et al., 2016). Two main types of attention methods may be utilized, either soft or hard. Soft attention methods first locate salient text regions within the input text and then bias the abstraction process to prefer such regions during decoding (Cohan et al., 2018; Gehrmann et al., 2018; Hsu et al., 2018; Nallapati et al., 2016; Li et al., 2018; Pasunuru and Bansal, 2018; Tan et al., 2017). On the other hand, hard attention methods perform abstraction only on text regions that were initially selected by some extraction process (Chen and Bansal, 2018; Nallapati et al., 2017; Liu et al., 2018).\nCompared to previous works, whose final summary is either entirely extracted or generated using an abstractive process, in this work, we suggest a new idea of \u201cEditorial Network\" (EditNet) \u2013 a mixed extractive-abstractive summarization approach. A summary generated by EditNet may include sentences that were either extracted, abstracted or of both types. Moreover, per considered sentence, EditNet may decide not to take either of these decisions and completely reject the sentence.\nUsing the CNN/DailyMail dataset we demonstrate that, EditNet\u2019s summarization quality is highly competitive to that obtained\nby both state-of-the-art abstractive-only and extractive-only baselines."
            },
            {
                "heading": "2 Editorial Network",
                "text": "Figure 1 depicts the architecture of EditNet. EditNet is applied as a post-processing step over a given input summary whose sentences were initially selected by some extractor. The key idea behind EditNet is to create an automatic editing process to enhance summary quality.\nLet S denote a summary which was extracted from a given text (document) D. The editorial process is implemented by iterating over sentences in S according to the selection order of the extractor. For each sentence in S, the \u201ceditor\" may make three possible decisions. The first decision is to keep the extracted sentence untouched (represented by label E in Figure 1). The second alternative is to rephrase the sentence (represented by label A in Figure 1). Such a decision, for example, may represent the editor\u2019s wish to simplify or compress the original source sentence. The last possible decision is to completely reject the sentence (represented by label R in Figure 1). For example, the editor may wish to ignore a superfluous or duplicate information expressed in the current sentence. An example mixed summary generated by our approach is depicted in Figure 2 in the appendix, further emphasizing the various editor\u2019s decisions."
            },
            {
                "heading": "2.1 Implementing the editor\u2019s decisions",
                "text": "For a given sentence s \u2208 D, we now denote by se and sa its original (extracted) and paraphrased (abstracted) versions. To obtain sa we use an abstractor, whose details will be shortly explained (see Section 2.2). Let es \u2208 Rn and as \u2208 Rn further denote the corresponding sentence representations of se and sa, respectively. Such representations allow to compare both sentence versions on the same grounds.\nRecall that, for each sentence si \u2208 S (in order) the editor makes one of the three possible decisions: extract, abstract or reject si. Therefore, the editor may modify summary S by paraphrasing or rejecting some of its sentences, resulting in a mixed extractive-abstractive summary S\u2032.\nLet l be the number of sentences in S. In each step i \u2208 {1, 2, . . . , l}, in order to make an educated decision, the editor considers both sentence representations esi and asi as its input, together with two additional auxiliary representations. The first auxiliary representation is that of the whole document D itself, hereinafter denoted d \u2208 Rn. Such a representation provides a global context for decision making. Assuming document D has\nN sentences, let e\u0304 = 1N N\u2211 s\u2208D es. Following (Chen and Bansal, 2018; Wu and Hu, 2018a), d is then calculated as follows: d = tanh (Wde\u0304+ bd) , where Wd \u2208 Rn\u00d7n and bd \u2208 Rn are learnable parameters.\nThe second auxiliary representation is that of\nthe summary that was generated by the editor so far, denoted at step i as gi\u22121 \u2208 Rn, with g0 = ~0. Such a representation provides a local context for decision making. Given the four representations as an input, the editor\u2019s decision for sentence si \u2208 S is implemented using two fully-connected layers, as follows:\nsoftmax (V tanh (Wc[esi , asi , gi\u22121, d] + bc) + b) , (1)\nwhere [\u00b7] denotes the vectors concatenation, V \u2208 R3\u00d7m, Wc \u2208 Rm\u00d74n, bc \u2208 Rm and b \u2208 R3 are learnable parameters.\nIn each step i, therefore, the editor chooses the action \u03c0i \u2208 {E,A,R} with the highest likelihood (according to Eq. 1), further denoted p(\u03c0i). Upon decision, in case it is either E or A, the editor appends the corresponding sentence version (i.e., either sei or s a i ) to S\n\u2032; otherwise, the decision is R and sentence si is discarded. Depending on its decision, the current summary representation is further updated as follows:\ngi = gi\u22121 + tanh (Wghi) , (2)\nwhere Wg \u2208 Rn\u00d7n are learnable parameters, gi\u22121 is the summary representation from the previous decision step; and hi \u2208 {esi , asi ,~0}, depending on which decision is made.\nSuch a network architecture allows to capture various complex interactions between the different inputs. For example, the network may learn that given the global context, one of the sentence versions may allow to produce a summary with a better coverage. As another example, based on the interaction between both sentence versions with either of the local or global contexts (and possibly among the last two), the network may learn that both sentence versions may only add superfluous or redundant information to the summary, and therefore, decide to reject both."
            },
            {
                "heading": "2.2 Extractor and Abstractor",
                "text": "As a proof of concept, in this work, we utilize the extractor and abstractor that were previously used in (Chen and Bansal, 2018), with a slight modification to the latter, motivated by its specific usage within our approach. We now only highlight important aspects of these two sub-components and kindly refer the reader to (Chen and Bansal, 2018) for the full implementation details.\nThe extractor of (Chen and Bansal, 2018) consists of two main sub-components. The first\nis an encoder which encodes each sentence s \u2208 D into es using an hierarchical representation1. The second is a sentence selector using a PointerNetwork (Vinyals et al., 2015). For the latter, let P (s) be the selection likelihood of sentence s.\nThe abstractor of (Chen and Bansal, 2018) is basically a standard encoder-aligner-decoder with a copy mechanism (See et al., 2017). Yet, instead of applying it directly only on a single given extracted sentence sei \u2208 S, we apply it on a \u201cchunk\" of three consecutive sentences2 (se\u2212, s e i , s e +), where s e \u2212 and s e + denote the sentence that precedes and succeeds sei in D, respectively. This in turn, allows to generate an abstractive version of sei (i.e., s a i ) that benefits from a wider local context. Inspired by previous softattention methods, we further utilize the extractor\u2019s sentence selection likelihoods P (\u00b7) for enhancing the abstractor\u2019s attention mechanism, as follows. LetC(wj) denote the abstractor\u2019s original attention value of a given word wj occurring in (se\u2212, s e i , s e +); we then recalculate this value to be C \u2032(wj) = C(wj)\u00b7P (s)\nZ , with wj \u2208 s and s \u2208 {s e \u2212, s e i , s e +}; Z = \u2211 s\u2032\u2208{se\u2212,sei ,se+} \u2211 wj\u2208s\u2032 C(wj)\u00b7P (s \u2032) denotes the normalization term."
            },
            {
                "heading": "2.3 Sentence representation",
                "text": "Recall that, in order to compare sei with s a i , we need to represent both sentence versions on as similar grounds as possible. To achieve that, we first replace sei with s a i within the original document D. By doing so, we basically treat sentence sai as if it was an ordinary sentence within D, where the rest of the document remains untouched. We then obtain sai \u2019s representation by encoding it using the extractor\u2019s encoder in a similar way in which sentence sei was originally supposed to be encoded. This results in a representation asi that provides a comparable alternative to esi , whose encoding is expected to be effected by similar contextual grounds."
            },
            {
                "heading": "2.4 Network training",
                "text": "We conclude this section with the description of how we train the editor using a novel soft labeling approach. Given text S (with l extracted sentences), let \u03c0 = (\u03c01, . . . , \u03c0l) denote its editing decisions\n1Such a representation is basically a combination of a temporal convolutional model followed by a biLSTM encoder.\n2The first and last chunks would only have two consecutive sentences.\n(sequence). We define the following \u201csoft\" crossentropy loss:\nL(\u03c0|S) = \u22121 l \u2211 si\u2208S \u2211 \u03c0i\u2208{E,A,R} y(\u03c0i) log p(\u03c0i), (3) where, for a given sentence si \u2208 S, y(\u03c0i) denotes its soft-label for decision. We next explain how each soft-label y(\u03c0i) is estimated. To this end, we utilize a given summary quality metric r(S\u2032) which can be used to evaluate the quality of any given summary S\u2032 (e.g., ROUGE (Lin, 2004)). Overall, for a given text input S with l sentences, there are 3l possible summaries S\u2032 to consider. Let \u03c0\u2217 = (\u03c0\u22171, . . . , \u03c0 \u2217 l ) denote the best decision sequence which results in the summary which maximizes r(\u00b7). For i \u2208 {1, 2, . . . , l}, let r\u0304(\u03c0\u22171, . . . , \u03c0\u2217i\u22121, \u03c0i) denote the average r(\u00b7) value obtained by decision sequences that start with the prefix (\u03c0\u22171, . . . , \u03c0 \u2217 i\u22121, \u03c0i). Based on \u03c0\u2217, the soft label y(\u03c0i) is then calculated3 as follows:\ny(\u03c0i) = r\u0304(\u03c0\u22171, . . . , \u03c0 \u2217 i\u22121, \u03c0i)\u2211\n\u03c0j\u2208{E,A,R} r\u0304(\u03c0 \u2217 1, . . . , \u03c0 \u2217 i\u22121, \u03c0j)\n(4)"
            },
            {
                "heading": "3 Evaluation",
                "text": ""
            },
            {
                "heading": "3.1 Dataset and Setup",
                "text": "We trained, validated and tested our approach using the non-annonymized version of the CNN/DailyMail dataset (Hermann et al., 2015). Following (Nallapati et al., 2016), we used the story highlights associated with each article as its ground truth summary. We further used the F-measure versions of ROUGE-1, ROUGE-2 and ROUGEL as our evaluation metrics (Lin, 2004).\nThe extractor and abstractor were trained similarly to (Chen and Bansal, 2018) (including the same hyperparameters). The Editorial Network (hereinafter denoted EditNet) was trained according to Section 2.4, using the ADAM optimizer with a learning rate of 10\u22124 and a batch size of 32. Following (Dong et al., 2018; Wu and Hu, 2018a), we set the reward metric to be r(\u00b7) = \u03b1R-1(\u00b7) + \u03b2R-2(\u00b7) + \u03b3R-L(\u00b7); with \u03b1 = 0.4, \u03b2 = 1 and \u03b3 = 0.5, which were further suggested by (Wu and Hu, 2018a).\nWe further applied the Teacher-Forcing approach (Lamb et al., 2016) during training, where we considered the true-label instead of the\n3For i = 1 we have: r\u0304(\u03c0\u22171 , . . . , \u03c0\u22170 , \u03c01) = r\u0304(\u03c01).\neditor\u2019s decision (including when updating gi at each step i according to Eq. 2). Following (Chen and Bansal, 2018), we set m = 512 and n = 512. We trained for 20 epochs, which has taken about 72 hours on a single GPU. We chose the best model over the validation set for testing. Finally, all components were implemented in Python 3.6 using the pytorch 0.4.1 package."
            },
            {
                "heading": "3.2 Results",
                "text": "Table 1 compares the quality of EditNet with that of several state-of-the-art extractive-only or abstractive-only baselines. This includes the extractor (rnn-ext-RL) and abstractor (rnn-ext-absRL) components of (Chen and Bansal, 2018) that we utilized for implementing EditNet 4.\nWe further report the quality of EditNet when it was being enforced to take an extract-only or abstract-only decision, denoted hereinafter as EditNetE and EditNetA, respectively. The comparison of EditNet to both EditNetE and EditNetA variants provides a strong empirical proof that, by utilizing an hybrid decision approach, a\n4The rnn-ext-RL extractor results reported in Table 1 are the ones that were reported by (Chen and Bansal, 2018). Training the public extractor released by these authors, we obtained the following significantly lower results: see EditNetE\nbetter summarization quality is obtained. Overall, EditNet provides a highly competitive summary quality, where it outperforms most baselines. Interestingly, EditNet\u2019s summarization quality is quite similar to that of NeuSum (Zhou et al., 2018). Yet, while NeuSum applies an extraction-only approach, summaries generated by EditNet include a mixture of sentences that have been either extracted or abstracted.\nTwo models outperform EditNet, BERTSUM (Liu, 2019) and DCA (Celikyilmaz et al., 2018). The BERTSUM model gains an impressive accuracy, yet it is an extractive model that utilizes many attention layers running in parallel with millions of parameters (Devlin et al., 2019). DCA gains a comparable quality to EditNet, it outperforms on R-2 and slightly on R-1. The contextual encoder of DCA is comprised of several LSTM layers one on top of the other with varied number of agents (hyper-tuned) that transmit messages to each other. Considering the complexity of these models, and the slow down that can incur during training and inference, we think that EditNet still provides a useful, high quality and relatively simple extension on top of standard encoder aligned decoder architectures.\nOn average, 56% and 18% of EditNet\u2019s decisions were to abstract (A) or reject (R), respectively. Moreover, on average, per summary, EditNet keeps only 33% of the original (extracted) sentences, while the rest (67%) are abstracted ones. This demonstrates that, EditNet has a high capability of utilizing abstraction, while being also able to maintain or reject the original extracted text whenever it is estimated to provide the best benefit for the summary\u2019s quality."
            },
            {
                "heading": "4 Conclusions and Future Work",
                "text": "We have proposed EditNet \u2013 a novel alternative summarization approach that instead of solely applying extraction or abstraction, mixes both together. Moreover, EditNet implements a novel sentence rejection decision, allowing to \u201ccorrect\u201d initial sentence selection decisions which are predicted to negatively effect summarization quality. As future work, we plan to evaluate other alternative extractor-abstractor configurations and try to train the network end-to-end. We further plan to explore reinforcement learning (RL) as an alternative decision making approach."
            }
        ],
        "references": [
            {
                "title": "Deep communicating agents for abstractive summarization",
                "author": [
                    "Asli Celikyilmaz",
                    "Antoine Bosselut",
                    "Xiaodong He",
                    "Yejin Choi."
                ],
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
                "citeRegEx": "Celikyilmaz et al\\.,? 2018",
                "shortCiteRegEx": "Celikyilmaz et al\\.",
                "year": 2018
            },
            {
                "title": "Fast abstractive summarization with reinforce-selected sentence rewriting",
                "author": [
                    "Yen-Chun Chen",
                    "Mohit Bansal."
                ],
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
                "citeRegEx": "Chen and Bansal.,? 2018",
                "shortCiteRegEx": "Chen and Bansal.",
                "year": 2018
            },
            {
                "title": "Abstractive sentence summarization with attentive recurrent neural networks",
                "author": [
                    "Sumit Chopra",
                    "Michael Auli",
                    "Alexander M. Rush."
                ],
                "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational",
                "citeRegEx": "Chopra et al\\.,? 2016",
                "shortCiteRegEx": "Chopra et al\\.",
                "year": 2016
            },
            {
                "title": "A discourse-aware attention model for abstractive summarization of long documents",
                "author": [
                    "Arman Cohan",
                    "Franck Dernoncourt",
                    "Doo Soon Kim",
                    "Trung Bui",
                    "Seokhwan Kim",
                    "Walter Chang",
                    "Nazli Goharian."
                ],
                "venue": "Proceedings of the 2018 Conference",
                "citeRegEx": "Cohan et al\\.,? 2018",
                "shortCiteRegEx": "Cohan et al\\.",
                "year": 2018
            },
            {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "author": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova."
                ],
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
                "citeRegEx": "Devlin et al\\.,? 2019",
                "shortCiteRegEx": "Devlin et al\\.",
                "year": 2019
            },
            {
                "title": "Banditsum: Extractive summarization as a contextual bandit",
                "author": [
                    "Yue Dong",
                    "Yikang Shen",
                    "Eric Crawford",
                    "Herke van Hoof",
                    "Jackie Chi Kit Cheung."
                ],
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels,",
                "citeRegEx": "Dong et al\\.,? 2018",
                "shortCiteRegEx": "Dong et al\\.",
                "year": 2018
            },
            {
                "title": "Bottom-up abstractive summarization",
                "author": [
                    "Sebastian Gehrmann",
                    "Yuntian Deng",
                    "Alexander Rush."
                ],
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098\u20134109. Association for Computational",
                "citeRegEx": "Gehrmann et al\\.,? 2018",
                "shortCiteRegEx": "Gehrmann et al\\.",
                "year": 2018
            },
            {
                "title": "Soft layer-specific multi-task summarization with entailment and question generation",
                "author": [
                    "Han Guo",
                    "Ramakanth Pasunuru",
                    "Mohit Bansal."
                ],
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume",
                "citeRegEx": "Guo et al\\.,? 2018",
                "shortCiteRegEx": "Guo et al\\.",
                "year": 2018
            },
            {
                "title": "Teaching machines to read and comprehend",
                "author": [
                    "Karl Moritz Hermann",
                    "Tom\u00e1\u0161 Ko\u010disk\u00fd",
                    "Edward Grefenstette",
                    "Lasse Espeholt",
                    "Will Kay",
                    "Mustafa Suleyman",
                    "Phil Blunsom."
                ],
                "venue": "Proceedings of the 28th International Conference on Neural",
                "citeRegEx": "Hermann et al\\.,? 2015",
                "shortCiteRegEx": "Hermann et al\\.",
                "year": 2015
            },
            {
                "title": "A unified model for extractive and abstractive summarization using inconsistency loss",
                "author": [
                    "Wan-Ting Hsu",
                    "Chieh-Kai Lin",
                    "Ming-Ying Lee",
                    "Kerui Min",
                    "Jing Tang",
                    "Min Sun."
                ],
                "venue": "Proceedings of the 56th Annual Meeting of the Association for",
                "citeRegEx": "Hsu et al\\.,? 2018",
                "shortCiteRegEx": "Hsu et al\\.",
                "year": 2018
            },
            {
                "title": "Closedbook training to improve summarization encoder memory",
                "author": [
                    "Yichen Jiang",
                    "Mohit Bansal."
                ],
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4067\u20134077. Association for Computational",
                "citeRegEx": "Jiang and Bansal.,? 2018",
                "shortCiteRegEx": "Jiang and Bansal.",
                "year": 2018
            },
            {
                "title": "Professor forcing: A new algorithm for training recurrent networks",
                "author": [
                    "Alex M Lamb",
                    "Anirudh Goyal ALIAS PARTH GOYAL",
                    "Ying Zhang",
                    "Saizheng Zhang",
                    "Aaron C Courville",
                    "Yoshua Bengio."
                ],
                "venue": "Advances In Neural Information",
                "citeRegEx": "Lamb et al\\.,? 2016",
                "shortCiteRegEx": "Lamb et al\\.",
                "year": 2016
            },
            {
                "title": "Guiding generation for abstractive text summarization based on key information guide network",
                "author": [
                    "Chenliang Li",
                    "Weiran Xu",
                    "Si Li",
                    "Sheng Gao."
                ],
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association",
                "citeRegEx": "Li et al\\.,? 2018",
                "shortCiteRegEx": "Li et al\\.",
                "year": 2018
            },
            {
                "title": "Rouge: A package for automatic evaluation of summaries",
                "author": [
                    "Chin-Yew Lin."
                ],
                "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8. Barcelona, Spain.",
                "citeRegEx": "Lin.,? 2004",
                "shortCiteRegEx": "Lin.",
                "year": 2004
            },
            {
                "title": "Generating wikipedia by summarizing long sequences",
                "author": [
                    "Peter J Liu",
                    "Mohammad Saleh",
                    "Etienne Pot",
                    "Ben Goodrich",
                    "Ryan Sepassi",
                    "Lukasz Kaiser",
                    "Noam Shazeer."
                ],
                "venue": "arXiv preprint arXiv:1801.10198.",
                "citeRegEx": "Liu et al\\.,? 2018",
                "shortCiteRegEx": "Liu et al\\.",
                "year": 2018
            },
            {
                "title": "Fine-tune bert for extractive summarization",
                "author": [
                    "Yang Liu."
                ],
                "venue": "arXiv preprint arXiv:1903.10318.",
                "citeRegEx": "Liu.,? 2019",
                "shortCiteRegEx": "Liu.",
                "year": 2019
            },
            {
                "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
                "author": [
                    "Ramesh Nallapati",
                    "Feifei Zhai",
                    "Bowen Zhou."
                ],
                "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9,",
                "citeRegEx": "Nallapati et al\\.,? 2017",
                "shortCiteRegEx": "Nallapati et al\\.",
                "year": 2017
            },
            {
                "title": "Abstractive text summarization using sequence-tosequence rnns and beyond",
                "author": [
                    "Ramesh Nallapati",
                    "Bowen Zhou",
                    "C\u00edcero Nogueira dos Santos",
                    "\u00c7aglar G\u00fcl\u00e7ehre",
                    "Bing Xiang."
                ],
                "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural",
                "citeRegEx": "Nallapati et al\\.,? 2016",
                "shortCiteRegEx": "Nallapati et al\\.",
                "year": 2016
            },
            {
                "title": "Ranking sentences for extractive summarization with reinforcement learning",
                "author": [
                    "Shashi Narayan",
                    "Shay B. Cohen",
                    "Mirella Lapata."
                ],
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
                "citeRegEx": "Narayan et al\\.,? 2018",
                "shortCiteRegEx": "Narayan et al\\.",
                "year": 2018
            },
            {
                "title": "Multireward reinforced summarization with saliency and entailment",
                "author": [
                    "Ramakanth Pasunuru",
                    "Mohit Bansal."
                ],
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
                "citeRegEx": "Pasunuru and Bansal.,? 2018",
                "shortCiteRegEx": "Pasunuru and Bansal.",
                "year": 2018
            },
            {
                "title": "A deep reinforced model for abstractive summarization",
                "author": [
                    "Romain Paulus",
                    "Caiming Xiong",
                    "Richard Socher."
                ],
                "venue": "CoRR, abs/1705.04304.",
                "citeRegEx": "Paulus et al\\.,? 2017",
                "shortCiteRegEx": "Paulus et al\\.",
                "year": 2017
            },
            {
                "title": "Get to the point: Summarization with pointer-generator networks",
                "author": [
                    "Abigail See",
                    "Peter J. Liu",
                    "Christopher D. Manning."
                ],
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver,",
                "citeRegEx": "See et al\\.,? 2017",
                "shortCiteRegEx": "See et al\\.",
                "year": 2017
            },
            {
                "title": "Sequence to sequence learning with neural networks",
                "author": [
                    "Ilya Sutskever",
                    "Oriol Vinyals",
                    "Quoc V. Le."
                ],
                "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201914, pages 3104\u20133112, Cambridge, MA,",
                "citeRegEx": "Sutskever et al\\.,? 2014",
                "shortCiteRegEx": "Sutskever et al\\.",
                "year": 2014
            },
            {
                "title": "Abstractive document summarization with a graphbased attentional neural model",
                "author": [
                    "Jiwei Tan",
                    "Xiaojun Wan",
                    "Jianguo Xiao."
                ],
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
                "citeRegEx": "Tan et al\\.,? 2017",
                "shortCiteRegEx": "Tan et al\\.",
                "year": 2017
            },
            {
                "title": "Pointer networks",
                "author": [
                    "Oriol Vinyals",
                    "Meire Fortunato",
                    "Navdeep Jaitly."
                ],
                "venue": "Advances in Neural Information Processing Systems, pages 2692\u20132700.",
                "citeRegEx": "Vinyals et al\\.,? 2015",
                "shortCiteRegEx": "Vinyals et al\\.",
                "year": 2015
            },
            {
                "title": "Learning to extract coherent summary via deep reinforcement learning",
                "author": [
                    "Yuxiang Wu",
                    "Baotian Hu."
                ],
                "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI18), the 30th innovative Applications of Artificial",
                "citeRegEx": "Wu and Hu.,? 2018a",
                "shortCiteRegEx": "Wu and Hu.",
                "year": 2018
            },
            {
                "title": "Learning to extract coherent summary via deep reinforcement learning",
                "author": [
                    "Yuxiang Wu",
                    "Baotian Hu"
                ],
                "venue": "In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence,",
                "citeRegEx": "Wu and Hu.,? \\Q2018\\E",
                "shortCiteRegEx": "Wu and Hu.",
                "year": 2018
            },
            {
                "title": "Neural latent extractive document summarization",
                "author": [
                    "Xingxing Zhang",
                    "Mirella Lapata",
                    "Furu Wei",
                    "Ming Zhou."
                ],
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 779\u2013784. Association",
                "citeRegEx": "Zhang et al\\.,? 2018",
                "shortCiteRegEx": "Zhang et al\\.",
                "year": 2018
            },
            {
                "title": "Neural document summarization by jointly learning to score and select sentences",
                "author": [
                    "Qingyu Zhou",
                    "Nan Yang",
                    "Furu Wei",
                    "Shaohan Huang",
                    "Ming Zhou",
                    "Tiejun Zhao."
                ],
                "venue": "Proceedings of the 56th Annual Meeting of the Association for",
                "citeRegEx": "Zhou et al\\.,? 2018",
                "shortCiteRegEx": "Zhou et al\\.",
                "year": 2018
            }
        ],
        "abstractText": "We suggest a new idea of Editorial Network \u2013 a mixed extractive-abstractive summarization approach, which is applied as a postprocessing step over a given sequence of extracted sentences. We further suggest an effective way for training the \u201ceditor\" based on a novel soft-labeling approach. Using the CNN/DailyMail dataset we demonstrate the effectiveness of our approach compared to state-of-the-art extractive-only or abstractiveonly baselines."
    },
    {
        "title": "MetAdapt: Meta-Learned Task-Adaptive Architecture for Few-Shot Classification",
        "sections": [
            {
                "heading": null,
                "text": "Keywords: Few-Shot, Few-Shot Learning, Meta-Learning, Task-Adaptive Architecture"
            },
            {
                "heading": "1 Introduction",
                "text": "Recently, there has been a lot of exciting progress in the field of few-shot learning in general, and in few-shot classification (FSC) in particular. A popular method for approaching FSC is meta-learning, or learning-to-learn. In meta-learning, the inputs to both train and test phases are not images, but instead a set of few-shot tasks, {Ti}, each K-shot / N -way task containing a small amount K (usually 1-5) of labeled support images and some amount of unlabeled query images for each of the N categories of the task. The goal of meta-learning is to find a base model that is easily adapted to the specific task at hand, so that it will generalize well to tasks built from novel unseen categories and fulfill the goal of FSC (see Section 2 for further review).\n* Equal contributors Corresponding authors: Sivan Doveh sivan.doveh@ibm.com and Leonid Karlinsky leonidka@il.ibm.com\nar X\niv :1\n91 2.\n00 41\n2v 3\n[ cs\n.C V\n] 9\nM ar\n2 02\nMany successful meta-learning based approaches have been developed for FSC [60,55,13,39,51,41,29] advancing its state-of-the-art. Besides continuous improvements offered by the FSC methods, some general trends affecting the performance of FSC have become apparent. One of such major factors is the CNN backbone architecture at the basis of all the modern FSC methods. Carefully reviewing and placing on a single chart the test accuracies of top-performing FSC approaches w.r.t. the backbone architecture employed reveals an interesting trend (Figure 1). It is apparent that larger architectures increase FSC performance, up to a certain size, where performance seems to saturate or even degrade. This happens since bigger backbones carry higher risk of over-fitting. It seems the overall performance of the FSC techniques cannot continue to grow by simply expanding the backbone size.\nIn light of the above, in this paper we set to explore methods for architecture search, their meta-adaptation and optimization for FSC. Neural Architecture Search (NAS) is a very active research field that has contributed significantly to overall improvement of the state of the art in supervised classification. Some of the recent NAS techniques, and in particular Differentiable-NAS (D-NAS), such as DARTS [34], are capable of finding optimal (and transferable) architectures given a particular task using a single GPU in the course of 1-2 days. This is\ndue to incorporating the architecture as an additional set of neural network parameters to be optimized, and solving this optimization using SGD. Due to this use of additional architectural parameters, the training tends to over-fit. D-NAS optimization techniques are especially designed to mitigate over-fitting, making them attractive to extreme situations with the greatest risk of overfitting, such as in the case of FSC.\nSo far, D-NAS techniques have been explored mainly in the context of large scale tasks, involving thousands of labeled examples for each class. Very little work has been done on NAS for few-shot. D-NAS in particular, to the best of our knowledge, has not been applied to few-shot problems yet. Meta-adaption of the architecture in task dependent manner to accommodate for novel tasks also has not been explored.\nIn this work, we build our few-shot task-adaptive architecture search upon a technique from D-NAS (DARTS [34]). Our goal is to learn a neural network where connections are controllable and adapt to the few-shot task with novel categories. Similarly to DARTS, we have a neural network in the form of a Directed Acyclic Graph (DAG), where the nodes are the intermediate feature maps tensors, and edges are operations. Each edge is a weighted sum of operations (with weights summing to 1), each operation is a different preset sequence of layers (convolution, pooling, BatchNorm and non-linearity). The operations set includes the identity-operation and the zero-operation to either keep the representation untouched or cut the connection. To avoid over-fitting, a bi-level (two-fold) optimization is performed where first the operation layers\u2019 weights are trained on one fold of the data and then the connections\u2019 weights are trained on the other fold.\nHowever, unlike DARTS, our goal is not to learn a one time architecture to be used for all tasks. To be successful at FSC, we need to make our architecture task adaptive so it would be able to quickly rewire for each new target task. To this end, we employ a set of small neural networks, MetAdapt Controllers, responsible for controlling the connections in the DAG given the current task. The MetAdapt Controllers adjust the weights of the different operations, such that if some operations are better for the current task they will get higher weights, thus, effectively modifying the architecture and adapting it to the task.\nTo summarize, our contributions in this work are as follows: (1) We show that DARTS-like bi-level iterative optimization of layer weights and network connections performs well for few-shot classification without suffering from overfitting due to over-parameterization; (2) We show that adding small neural networks, MetAdapt Controllers, that adapt the connections in the main network according to the given task further (and significantly) improves performance; (3) using the proposed method, we obtain improvements over FSC state-of-the-art on two popular FSC benchmarks: miniImageNet [60] and FC100 [41]."
            },
            {
                "heading": "2 Related Work",
                "text": "Few-Shot Learning. The major approaches to few-shot learning include: metric learning, generative (or augmentation) based methods, and meta learning (or learning-to-learn).\nFew-shot learning by metric learning. This type of methods [64,55,49] learn a non-linear embedding into a metric space where L2 nearest neighbor (or similar) approach is used to classify instances of new categories according to their proximity to the few labeled training examples embedded in the same space. Additional proposed variants include using a metric learning method based on graph neural networks [16], that goes beyond the L2 metric. Similarly, [52,58] introduce metric learning methods where the similarity is computed by an implicit learned function rather than via the L2 metric over an embedding space.\nThe embedding space based metric-learning approaches are either posed as a general discriminative distance metric learning [49,8], or optimized on the fewshot tasks [55,64,16,41], via the meta-learning paradigm that will be described next. These approaches show a great promise, and in some cases are able to learn embedding spaces with some meaningful semantics embedded in the metric [49]. Improved performance in the metric learning based methods has been achieved when combined with some additional semantic information. In [24], class conditioned embedding is used. In [66], the visual prototypes are refined using a corresponding label embedding and in [53] it is extended to using multiple semantics, such as textual descriptions.\nAugmentation-based few-shot learning. This family of approaches refers to methods that (learn to) generate more samples from the one or a few examples available for training in a given few-shot learning task. These methods include synthesizing new data from few examples using a generative model, or using external data for obtaining additional examples that facilitate learning on a given few shot task. These approaches include: (i) semi-supervised approaches using additional unlabeled data [9,14]; (ii) fine tuning from pre-trained models [31,62,63]; (iii) applying domain transfer by borrowing examples from relevant categories [32] or using semantic vocabularies [3,15]; (iv) rendering synthetic examples [42,10,56]; (v) augmenting the training examples using geometric and photometric transformations [28] or learning adaptive augmentation strategies [21]; (vi) example synthesis using Generative Adversarial Networks (GANs) [69,25,20,48,45,35,11,23,2].\nIn [22,54] additional examples are synthesized via extracting, encoding, and transferring to the novel category instances, of the intra-class relations between pairs of instances of reference categories. In [61], a generator sub-net is added to a classifier network and is trained to synthesize new examples on the fly in order to improve the classifier performance when being fine-tuned on a novel (few-shot) task. In [48], a few-shot class density estimation is performed with an auto-regressive model, augmented with an attention mechanism, where examples are synthesized by a sequential process. In [7,67] label and attribute semantics\nare used as additional information for training an example synthesis network. In [1] models are trained to perform set-operations (e.g. union) and then can be used to synthesise samples for few-shot multi-label classifications.\nFew-shot meta-learning (learning-to-learn). These methods are trained on a set of few-shot tasks (also known as \u2018episodes\u2019) instead of a set of object instances, with the motivation to learn a learning strategy that will allow effective adaptation to new such (few-shot) tasks using one or few examples.\nAn important sub-category of meta learning methods is metric-meta-learning, combining metric learning as explained above with task-based (episodic) training of meta-learning. In Matching Networks [60], a non-parametric k-NN classifier is meta-learned such that for each few-shot task the learned model generates an adaptive embedding space for which the task can be better solved. In [55] the metric (embedding) space is optimized such that in the resulting space different categories form compact and well separated uni-modal distributions around the category \u2018prototypes\u2019 (centers of the category modes).\nAnother family of meta-learning approaches is the so-called \u2018gradient based approaches\u2019, that try to maximize the \u2018adaptability\u2019, or speed of convergence, of the networks they train to new (few-shot) tasks (usually assuming an SGD optimizer). In other words, the meta-learned classifiers are optimized to be easily fine-tuned on new few-shot tasks using small training data. The first of these approaches is MAML [13] that due to its universality was later extended through many works such as, Meta-SGD [30], DEML+Meta-SGD [68], MetaLearn LSTM [46], and Meta-Networks [37]. In LEO [51], a MAML like loss is applied not directly on the model parameters, but rather on a latent representation encoding them. This approach featured an encoder and a decoder to and from that latent space and achieved state-of-the-art results on miniImagenet few-shot benchmark among models relying on visual information alone.\nIn MetaOptNet [29] a CNN backbone is trained end-to-end with an unrolled convex optimization solution of an optimal classifier, such as SVM. In this work, we use their suggested construction of performing SGD through an unrolled SVM optimization to train the backbone. Our work is focused on optimizing the backbone architecture.\nOther methods focus on regularization for mitigating the over-fitting. In BF3S [17] auxiliary self-supervision tasks are added, such as predicting image rotation or patch location. In Robust-dist [12] first an ensemble of up to 20 models is learned, so each model by itself cannot overfit the data. Then, the the final model is distilled from all those models. Notably, our method which also deals with training large architecture without over-fitting is orthogonal to these two approaches. It is likely that further improvement can be achieved by combining these methods with ours.\nNotably, in all previous meta-learning methods, only the parameters of a (fixed) neural network are optimized through meta-learning in order to become adaptable (or partially adaptable) to novel few-shot tasks. In this work, we both learn a specialized backbone architecture that would facilitate this adaptability, as well as meta-learn to become capable of adapting that architecture itself to\nthe task, thus going beyond the parameter only adaptation of all previous metalearning approaches.\nNeural Architecture Search. Over the last few years Neural Architecture Search (NAS) have enabled automatic design of novel architectures that outperformed previous hand-designed architectures in terms of accuracy. Two notable works on NAS are AmoebaNet [47] and NASnet [70]. The first one used a genetic algorithm and the second used a reinforcement learning based method. Although achieving state of the art performance at the time, these methods required enormous amount of GPU-hours . Efficient NAS (ENAS) [43], a reinforcement learning based method, used weight sharing across its child models, which are sub graphs of a larger one. By that, they managed to accelerate the search process. The work in [59] shows how to scale the size of such learned architectures with the size of the input data.\nRecently, differentiable methods with lower demand for computing have been introduced. Notable among them are differentiable architecture search (DARTS) [34] and SNAS [65]. These methods managed to search for architecture in just a few GPU days. DARTS relaxes the search space into a continuous one, allowing a differentiable search. The DARTS method includes two stages. First, basic structures are learned, by placing coefficients attached to operations between feature maps. These coefficients indicate the importance of the attached operations and connections. After the search is done, the final basic structures are formed by pruning and keeping only the most important operations. At the second stage, the final network is built by repeatedly concatenating the found basic structures.\nASAP [40] addresses the issue that harsh pruning at the end of the search makes the found architecture sub-optimal. It does so by performing gradual pruning. ASAP achieves higher accuracy with a shorter training time. In SNAS [65], the search is done by learning a continuous architectures distribution and sampling from it. This distribution is pushed closer to binary by using a temperature parameter and gradually decreasing it. Then, the chosen architecture is the one that has the higher probability. In [4] a binary mask is learned and used to keep a single path of the network graph. By doing so, they managed to search for the whole network and not only cells. PNAS [33] suggested a method for progressively searching for a larger architecture. P-DARTS [6] do the same but with differentiable architecture search.\nThese methods are mostly focused, and perform well, on datasets such as CIFAR and ImageNet. So far, little attention has been given to their adaptation to few-shot learning. Auto-Meta [26] used PNAS [33] based search for few-shot classification, but with a focus on searching for a small architecture (resulting in a relatively low performance w.r.t. to current state-of-the-art). In particular, the possibility of adapting the architecture at test-time to a specific novel task, as proposed in this work, has not been explored before."
            },
            {
                "heading": "3 MetAdapt",
                "text": "In this section we describe the architecture and training procedure for MetAdapt. We introduce the task-adaptable block, it has a graph structure with adaptable connections that can modulate the architecture, adapting it to the few-shot task at hand. We then describe the sub-models, MetAdapt Controllers, that predict the change in connectivity that is needed in the learned graph as a function of the current task. Finally, we describe the training procedure."
            },
            {
                "heading": "3.1 Task-Adaptable Block",
                "text": "The architecture of the adaptable block used in MetAdapt is defined, similarly to DARTS [34], as a Directed Acyclic Graph (DAG). The block is built from feature maps V = {xi} that are linked by mixtures of operations. The input feature map to the block is x0 and its output is x|V |\u22121. A Mixed Operation, o\u0304(i,j), is defined as\no\u0304(i,j)(x) =\n\u2211 o\u2208O exp(\u03b1\n(i,j) o )o(x)\u2211\no\u2208O exp(\u03b1 (i,j) o )\n, (1)\nwhere O is a set of the search space operations, o(x) is an operation applied to x, and \u03b1 (i,j) o is an optimised coefficient for operation o at edge (i, j). Later, we will describe how \u03b1s can be adapted per task (K-shot, N -way episode). The list of search space operations used in our experiments is provided in Table 1. This list includes the zero-operation and identity-operation that can fully or partially (depending on the corresponding \u03b1 (i,j) o ) cut the connection or make it\na residual one (skip-connection). Each feature map xi in the block is connected to all previous maps by setting it to be:\nxi = \u2211 j<i o\u0304(j,i)(xj). (2)\nThe task-adaptive block defined above can be appended to any backbone feature extractor. Potentially, more than one block can be used. We use ResNet9 followed by a single task-adaptive block with 4 nodes (|V | = 4) in our experiments, resulting in about 8 times more parameters compared with the original ResNet12 (due to large set of operations on all connections combined). Note that as we use 4 nodes in our block, there exists a single path in our search space that is the regular residual block (ResNet3 block). Figure 2a schematically illustrates the block architecture."
            },
            {
                "heading": "3.2 MetAdapt Controllers",
                "text": "The task-adaptive block is accompanied by a set of MetAdapt Controller modules, one per edge. They are responsible for predicting, given a few-shot task, the best way of adapting the mixing coefficients (\u03b1 (i,j) o ) for the corresponding edge operations. Let \u03b1(i,j) be the vector of all \u03b1 (i,j) o . Let \u03b1\u0302(i,j) be the globally optimized coefficients (optimization process described below), then MetAdapt\ncontrollers predict the task-specific residuals \u2206\u03b1(i,j), that is the modification required to make to \u03b1\u0302(i,j) for the current task (few-shot episode). Finally,\n\u03b1(i,j) = \u03b1\u0302(i,j) +\u2206\u03b1(i,j) (3)\nare the final task-adapted coefficients used for the Mixed Operation calculation as in Equation 1.\nThe architecture for each MetAdapt Controller, predicting \u2206\u03b1(i,j), is as follows. It receives the input feature maps of the corresponding edge xi, computed for all the support samples of the episode. For a support-set of size S, number of channels D and feature map spatial resolution M \u00d7M , the input is a tensor of dimensions (S,D,M,M). We perform global average pooling to obtain a (S,D) tensor, followed by a bottleneck linear layer (with ReLU activation) that operates on each sample individually, to get a (S,Dbottleneck) size tensor. Then, all support samples representations are concatenated to form a single vector of size S \u00b7 Dbottleneck. Finally, another linear layer maps the concatenation of all support-samples to the predicted \u2206\u03b1(i,j). The MetAdapt controller architecture and the way it is used in our adaptable block structure are schematically illustrated on Figure 2b+c. Figures 4 and 3 present an example of adaptation made by the MetAdapt Controller for a specific episode."
            },
            {
                "heading": "3.3 Training",
                "text": "Replacing simple sequence of convolutional layers with the suggested DAG, with its many layers and parameters, in conventional gradient descent training will result in a larger over-fitting. This is even worse for FSL, where it is harder to achieve generalization due to scarcity of the data and the domain differences between the training and test sets. Researchers have faced the same problem with differentiable architecture search, where the objective is to train a large neural network with weighted connections that are then pruned to form the final chosen architecture.\nWe follow the solution proposed in DARTS [34], solving a bi-level iterative optimization of the layers\u2019 weights w and the coefficients of operations \u03b1 between the nodes. The training set is split to trainw for weights training and train\u03b1 for training the \u03b1\u2019s. Iteratively optimizing w and \u03b1 to convergence is prohibitively slow. So, like in DARTS, w is optimized with a standard SGD:\nw = w \u2212 \u00b5\u2207wLosstrainw(w,\u03b1), (4)\nwhere \u00b5 is the learning rate. The \u03b1\u2019s are optimized using SGD with a secondorder approximation of the model after convergence of w, by applying:\n\u03b1 = \u03b1\u2212 \u03b7\u2207\u03b1Losstrain\u03b1(w \u2212 \u00b5Losstrainw(w,\u03b1), \u03b1) (5)\nwhere \u03b7 is the learning rate for \u03b1. The MetAdapt Controllers\u2019 parameters are trained as a final step, with all other parameters freezed, using SGD on the entire training set for a single epoch."
            },
            {
                "heading": "4 Experiments",
                "text": "Datasets. We use the popular miniImageNet and FC100 few-shot benchmarks to evaluate our method.\nThe miniImageNet dataset [60] is a standard benchmark for few-shot image classification, that has 100 randomly chosen classes from ILSVRC-2012 [50]. These classes are randomly split into 64 meta-training, 16 meta-validation, and 20 meta-testing classes. Each class has 600 images of size 84 \u00d7 84. We use the same classes splits as [29] and prior works.\nThe FC100 dataset [41] is constructed from the CIFAR-100 dataset [27], which contains 100 classes that are grouped into 20 super-classes. These are in turn partitioned into 60 classes from 12 super-classes for meta-training, 20 classes from 4 super-classes for meta-validation, and 20 classes from 4 super-classes for meta-testing. This minimizes the semantic overlap between classes of different splits. Each class contains 600 images of size 32\u00d7 32."
            },
            {
                "heading": "4.1 Implementation Details",
                "text": "We use the SVM classifier head as suggested in MetaOptNet [29]. We begin with training a ResNet12 backbone on the training set of the relevant dataset for 60 epochs. We then replace the last residual block of the ResNet12 backbone\nwith our DAG task-adaptive block, keeping the first 3 ResNet blocks (ResNet9) fixed and perform the architecture search for 10 epochs. Finally, we train the \u2018MetAdapt controllers\u2019 for a single epoch. Each epoch consists of 8000 episodes with mini-batch of 4 episodes.\nFor the initial training we use the SGD optimizer with intial learning rate = 0.1, momentum = 0.9 and weight decay = 5 \u00b7 10\u22124. Decreasing the learning rate to 0.006 at epoch 20, 0.0012 at epoch 40 and 0.00024 at epoch 50. For weights optimization during the search and meta adaptation phases we use the SGD optimizer with learning rate = 0.001, momentum = 0.9 and weight decay = 5 \u00b7 10\u22124. For the architecture optimization we use Adam optimizer with learning rate = 3 \u00b7 10\u22124, \u03b2 = [0.5, 0.99], weight decay = 10\u22123 and the Cosine Annealing learning rate scheduler with \u03b7min = 0.004.\nFollowing previous works, e.g. [13,5], we perform test time augmentations and fine-tuning. We perform horizontal flip augmentation, effectively doubling the number of support-set. We fine-tune the DAG weights for 10 iterations where the horizontally flipped support set serves as our labeled query set."
            },
            {
                "heading": "4.2 Results",
                "text": "Tables 2 and 3 compare the MetAdapt performance with the state-of-the-art few-shot classification methods that use plain ResNet backbones. We observe improved results for FC100 1-shot (+3.46%) and 5-shot (+3.17%) and also for miniImageNet 1-shot (+1.74%) and similar results for 5-shot. We see that despite having a larger model we do not suffer from severe over-fitting and perform comparably or better than top performing methods.\nArchitecture Transferability. It has been shown, in the case of architecture search, that it is possible to learn an architecture on a smaller dataset, e.g. CIFAR-10, and then the optimized architecture is transferable to a larger datasets, e.g. ImageNet. This helps mitigating the costly architecture search process. We follow this route, showing in our experiment that we can learn the \u03b1\u0302 values on FC100 and transfer them to miniImageNet (and then train the weights w of the transferred architecture on this dataset).\nFor miniImageNet we set the \u03b1\u2019s to be fixed to values obtained for the FC100, while the rest of the parameters of the searched top block are randomly initialized. We find that the architecture transferred from FC100 to miniImageNet is performing well, with results comparable to other state-of-the-art methods, 62.82/79.35 for 1/5-shot. But a search performed on the actual dataset (miniImageNet training set) is outperforming the transferred one."
            },
            {
                "heading": "4.3 Ablation studies",
                "text": "Next, we explore the effect of the different design choices made in our approach.\nLarge model effect. We hypothesize that simply using the same algorithm with a larger model architecture would not result in better performance and it might even harm performance. This is evident in Figure 1 when comparing the performance of different methods across increasingly larger architectures. This is also evident by observing the architectures usually used in the few-shot literature.\nIt is already been shown in DARTS that training \u03b1 together with w simultaneously decrease performance. They attribute this decrease to \u03b1 over-fitting the training-set. To confirm our hypothesis, we used SGD to train our suggested DAG architecture, using fixed uniform \u03b1 instead of the learned \u03b1 (making it even less likely to over-fit compared to the ablation in DARTS). To this end \u03b1i,j are initialized so each operation is given the same weight and are kept fixed. We observed that indeed in this case our large architecture is not performing as well as ResNet12 (a smaller architecture). See Table 5b.\n40\n1 2 3 4 5 6 7 8 9 10\nEpochs\nFig. 5: Training curves with and without optimization of \u03b1 for FC100. The generalization gap (gap between training-set and validationset accuracies) is smaller when \u03b1 is optimized using our method, suggesting it has a regularization effect. The uniform-\u03b1 and optimized-\u03b1 experiments are described in Sec. 4.3.\nFC100 Method 1-shot 5-shot\nProtoNet [55] 37.50 52.50 TADAM [41] 40.10 56.10 MetaOptNet [29] 41.37 55.30\nMetAdapt (Ours) 44.83 58.47\nTable 4: MetAdapt vs. S-MetAdapt (Stochastic MetAdapt); CIFAR-100 (FC100) 5-way accuracy\nFC100 Method 1-shot 5-shot\nS-MetAdapt (Ours) 41.97 55.31 MetAdapt (Ours) 44.83 58.47\nDARTS Without Meta-Adaptation. Next we test the effect of optimizing \u03b1 using iterative intermittent optimization for w and \u03b1 using different folds of the training set. Here, w and \u03b1 are updated intermittently one mini-batch at a time. In order to see the importance of using second-order approximation of w after convergence, we perform training with and without it.\nWithout: The \u03b1 updates are done without the second-order approximation of w after a gradient descent step, i.e., the updates are performed according to:\nw = w \u2212 \u00b5\u2207wLosstrainw(w,\u03b1) (6)\nWe find the \u03b1 optimization is helping at improving the performance by about 1% compared to the fixed architecture (See Table 5c).\nWith: The \u03b1 updates are done not according to current value of w but at an approximation of its value after convergence (see Equation 5). The update of w is performed according to Equations 4. We find that this change gives a moderate improvement of about 0.3% (see Table 5d).\nFigure 5 presents the training curves for training with the proposed bi-level optimization of w and \u03b1 vs. training the large model when \u03b1 is fixed. It shows that the generalization gap is larger for the latter case and confirms our hypothesis that simply adding more parameters is not sufficient for good performance.\nNumber of Operations. In the ablation experiments described till now, we used a slightly smaller model. Each edge is composed of 6 operations out of the 8 listed in Table 1, excluding the 5\u00d7 5 operations. Now, we add these operations to test the effect of a larger set of operations. Adding these operations improves slightly further the performance (see Table 5e).\nMetAdapt Controllers. Then, we add the MetAdapt Controllers, so the architecture is adapted to the current task according to the support samples. This brings us to the full MetAdapt method. We find that indeed the adaptations to each task are beneficial. The meta-adaptations improve the accuracy by around 2% (see Table 5f).\nTest time augmentations and fine-tuning. Finally, we add test time flip augmentations and fine-tuning as described in 4.1. This helps in the case of 1-shot with +0.41% improvement, but has no noticeable effect for 5-shot (see Table 5g-h).\nS-MetAdapt. A recent approach suggested for architecture search is Stochastic Neural Architecture Search (SNAS [65]). Usually for D-NAS, e.g. DARTS, at search time the training is done on the full model at each iteration where each edge is a weighted-sum of its operations according to \u03b1i,j . Contrarily, in SNAS \u03b1i,j are treated as probabilities of a Multinomial Distribution and at each iteration a single operation is sampled accordingly. So at each iteration only a single operation per edge affects the classification outcome and only this operation is be updated in the gradient descent backward step. Of course sampling from a Multinomial Distribution directly is not differentiable, so at training time the Gumbel Distribution is used as a differentiable approximation.\nWe tested a SNAS version of MetAdapt, named S-MetAdapt, on the few-shot classification task. Other than the modifications specified below S-MetAdapt is similar to MetAdapt. At training time, instead of the Mixed Operation defined in Equation 1, we define the Mixed Operation to be:\no\u0304i,j(x) = \u2211 o\u2208O zi,jo o(x) (8)\nwhere z(i,j) is a continuous approximation of a one-hot vector sampled from a Gumbel Distribution:\nzi,j \u223c Gumbel(\u03b1i,j). (9)\nHere \u03b1i,j are after softmax normalization and summed to 1. At test time, rather than the one-hot approximation, we use the operation with the top probability\nzi,jk =\n{ 1, if k = argmax(\u03b1i,j)\n0, otherwise (10)\nUsing this method we get better results for FC100 1-shot and comparable results for 5-shot, compared to vanilla MetaOptNet. However, it does not perform as well as the non-stochastic version of MetAdapt. See Table 4."
            },
            {
                "heading": "5 Conclusions",
                "text": "In this work we have proposed MetAdapt, a few-shot learning approach that enables meta-learned network architecture that is adaptive to novel few-shot tasks. The proposed approach effectively applies tools from the Neural Architecture Search (NAS) literature, extended with the concept of \u2018MetAdapt Controllers\u2019, in order to learn adaptive architectures. These tools help mitigate over-fitting to the extremely small data of the few-shot tasks and domain shift between the training set and the test set. We demonstrate that the proposed approach successfully improves state-of-the-art results on two popular few-shot benchmarks, miniImageNet and FC100, and carefully ablate the different optimization steps and design choices of the proposed approach.\nSome interesting future work directions include extending the proposed approach to progressively searching the full network architecture (instead of just\nthe last block), applying the approach to other few-shot tasks such as detection and segmentation, and researching into different variants of task-adaptivity including global connections modifiers and inter block adaptive wiring."
            }
        ],
        "references": [
            {
                "title": "Laso: Label-set operations networks for multi-label few-shot learning",
                "author": [
                    "A. Alfassy",
                    "L. Karlinsky",
                    "A. Aides",
                    "J. Shtok",
                    "S. Harary",
                    "R. Feris",
                    "R. Giryes",
                    "A.M. Bronstein"
                ],
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6548\u20136557",
                "citeRegEx": "1",
                "shortCiteRegEx": null,
                "year": 2019
            },
            {
                "title": "Data Augmentation Generative Adversarial Networks",
                "author": [
                    "A. Antoniou",
                    "A. Storkey",
                    "H. Edwards"
                ],
                "venue": null,
                "citeRegEx": "2",
                "shortCiteRegEx": "2",
                "year": 2017
            },
            {
                "title": "Predicting deep zero-shot convolutional neural networks using textual descriptions",
                "author": [
                    "J.L. Ba",
                    "K. Swersky",
                    "S. Fidler",
                    "R. Salakhutdinov"
                ],
                "venue": "Proceedings of the IEEE International Conference on Computer Vision 2015 Inter, 4247\u20134255",
                "citeRegEx": "3",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Proxylessnas: Direct neural architecture search on target task and hardware",
                "author": [
                    "H. Cai",
                    "L. Zhu",
                    "S. Han"
                ],
                "venue": "ICLR",
                "citeRegEx": "4",
                "shortCiteRegEx": null,
                "year": 2019
            },
            {
                "title": "A Closer Look At Few-Shot Classification",
                "author": [
                    "W.Y. Chen"
                ],
                "venue": "ICLR. pp. 1\u201316",
                "citeRegEx": "5",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Progressive differentiable architecture search: Bridging the depth gap between search and evaluation",
                "author": [
                    "X. Chen",
                    "L. Xie",
                    "J. Wu",
                    "Q. Tian"
                ],
                "venue": "arXiv preprint arXiv:1904.12760",
                "citeRegEx": "6",
                "shortCiteRegEx": null,
                "year": 2019
            },
            {
                "title": "Semantic Feature Augmentation in Few-shot Learning",
                "author": [
                    "Z. Chen",
                    "Y. Fu",
                    "Y. Zhang",
                    "Y.G. Jiang",
                    "X. Xue",
                    "L. Sigal"
                ],
                "venue": null,
                "citeRegEx": "7",
                "shortCiteRegEx": "7",
                "year": 2018
            },
            {
                "title": "Learning a Similarity Metric Discriminatively , with Application to Face Verification",
                "author": [
                    "S. Chopra",
                    "R. Hadsell"
                ],
                "venue": "CVPR",
                "citeRegEx": "8",
                "shortCiteRegEx": null,
                "year": 2005
            },
            {
                "title": "Learning to Generate Chairs, Tables and Cars with Convolutional Networks",
                "author": [
                    "A. Dosovitskiy",
                    "J.T. Springenberg",
                    "M. Tatarchenko",
                    "T. Brox"
                ],
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 39(4), 692\u2013705",
                "citeRegEx": "10",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Generative Multi-Adversarial Networks",
                "author": [
                    "I. Durugkar",
                    "I. Gemp",
                    "S. Mahadevan"
                ],
                "venue": "International Conference on Learning Representations (ICLR) pp. 1\u201314",
                "citeRegEx": "11",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Diversity with cooperation: Ensemble methods for few-shot classification",
                "author": [
                    "N. Dvornik",
                    "C. Schmid",
                    "J. Mairal"
                ],
                "venue": "The IEEE International Conference on Computer Vision (ICCV)",
                "citeRegEx": "12",
                "shortCiteRegEx": null,
                "year": 2019
            },
            {
                "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
                "author": [
                    "C. Finn",
                    "P. Abbeel",
                    "S. Levine"
                ],
                "venue": "arXiv:1703.03400 (2017),",
                "citeRegEx": "13",
                "shortCiteRegEx": "13",
                "year": 2017
            },
            {
                "title": "Transductive Multi-View Zero-Shot Learning",
                "author": [
                    "Y. Fu",
                    "T.M. Hospedales",
                    "T. Xiang",
                    "S. Gong"
                ],
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 37(11), 2332\u20132345",
                "citeRegEx": "14",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Semi-supervised Vocabulary-informed Learning",
                "author": [
                    "Y. Fu",
                    "L. Sigal"
                ],
                "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 5337\u20135346",
                "citeRegEx": "15",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Few-Shot Learning with Graph",
                "author": [
                    "V. Garcia",
                    "J. Bruna"
                ],
                "venue": "Neural Networks",
                "citeRegEx": "16",
                "shortCiteRegEx": "16",
                "year": 2017
            },
            {
                "title": "Boosting few-shot visual learning with self-supervision",
                "author": [
                    "S. Gidaris",
                    "A. Bursuc",
                    "N. Komodakis",
                    "P. P\u00e9rez",
                    "M. Cord"
                ],
                "venue": "Proceedings of the IEEE International Conference on Computer Vision",
                "citeRegEx": "17",
                "shortCiteRegEx": null,
                "year": 2019
            },
            {
                "title": "Dynamic few-shot visual learning without forgetting",
                "author": [
                    "S. Gidaris",
                    "N. Komodakis"
                ],
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4367\u20134375",
                "citeRegEx": "18",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Generating classification weights with gnn denoising autoencoders for few-shot learning",
                "author": [
                    "S. Gidaris",
                    "N. Komodakis"
                ],
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 21\u201330",
                "citeRegEx": "19",
                "shortCiteRegEx": null,
                "year": 2019
            },
            {
                "title": "Generative Adversarial Nets",
                "author": [
                    "I. Goodfellow",
                    "J. Pouget-Abadie",
                    "M. Mirza",
                    "B. Xu",
                    "D. Warde-Farley",
                    "S. Ozair",
                    "A. Courville",
                    "Y. Bengio"
                ],
                "venue": "Advances in Neural Information Processing Systems 27 pp. 2672\u20132680",
                "citeRegEx": "20",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Generating Sentences by Editing Prototypes",
                "author": [
                    "K. Guu",
                    "T.B. Hashimoto",
                    "Y. Oren",
                    "P. Liang"
                ],
                "venue": null,
                "citeRegEx": "21",
                "shortCiteRegEx": "21",
                "year": 2017
            },
            {
                "title": "Low-shot Visual Recognition by Shrinking and Hallucinating Features",
                "author": [
                    "B. Hariharan",
                    "R. Girshick"
                ],
                "venue": "IEEE International Conference on Computer Vision (ICCV) (2017),",
                "citeRegEx": "22",
                "shortCiteRegEx": "22",
                "year": 2017
            },
            {
                "title": "Densely Connected Convolutional Networks",
                "author": [
                    "G. Huang",
                    "Z. Liu",
                    "L. v. d. Maaten",
                    "K.Q. Weinberger"
                ],
                "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 2261\u20132269",
                "citeRegEx": "23",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Learning to learn with conditional class dependencies",
                "author": [
                    "X. Jiang",
                    "M. Havaei",
                    "F. Varno",
                    "G. Chartrand",
                    "N. Chapados",
                    "S. Matwin"
                ],
                "venue": "ICLR",
                "citeRegEx": "24",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Generative Visual Manipulation on the Natural Image Manifold",
                "author": [
                    "Jun-Yan Zhu",
                    "E.S. Philipp Krahenbuhl",
                    "A. Efros"
                ],
                "venue": "European Conference on Computer Vision (ECCV). pp. 597\u2013613",
                "citeRegEx": "25",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Auto-meta: Automated gradient based meta learner search",
                "author": [
                    "J. Kim",
                    "S. Lee",
                    "S. Kim",
                    "M. Cha",
                    "J.K. Lee",
                    "Y. Choi",
                    "Y. Choi",
                    "D.Y. Cho",
                    "J. Kim"
                ],
                "venue": "arXiv preprint arXiv:1806.06927",
                "citeRegEx": "26",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Learning Multiple Layers of Features from Tiny Images",
                "author": [
                    "A. Krizhevsky"
                ],
                "venue": "Technical report. Science Department, University of Toronto, Tech. pp. 1\u201360",
                "citeRegEx": "27",
                "shortCiteRegEx": null,
                "year": 2009
            },
            {
                "title": "ImageNet Classification with Deep Convolutional Neural Networks",
                "author": [
                    "A. Krizhevsky",
                    "I. Sutskever",
                    "G.E. Hinton"
                ],
                "venue": "Advances In Neural Information Processing Systems pp. 1\u20139",
                "citeRegEx": "28",
                "shortCiteRegEx": null,
                "year": 2012
            },
            {
                "title": "Meta-learning with differentiable convex optimization",
                "author": [
                    "K. Lee",
                    "S. Maji",
                    "A. Ravichandran",
                    "S. Soatto"
                ],
                "venue": "CVPR",
                "citeRegEx": "29",
                "shortCiteRegEx": null,
                "year": 2019
            },
            {
                "title": "Meta-SGD: Learning to Learn Quickly for FewShot Learning",
                "author": [
                    "Z. Li",
                    "F. Zhou",
                    "F. Chen",
                    "H. Li"
                ],
                "venue": null,
                "citeRegEx": "30",
                "shortCiteRegEx": "30",
                "year": 2017
            },
            {
                "title": "Learning without Forgetting",
                "author": [
                    "Z. Li",
                    "D. Hoiem"
                ],
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence pp. 1\u201313",
                "citeRegEx": "31",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Transfer Learning by Borrowing Examples for Multiclass Object Detection",
                "author": [
                    "J.J. Lim",
                    "R. Salakhutdinov",
                    "A. Torralba"
                ],
                "venue": "Advances in Neural Information Processing Systems 26 (NIPS) pp. 1\u20139",
                "citeRegEx": "32",
                "shortCiteRegEx": null,
                "year": 2012
            },
            {
                "title": "Progressive neural architecture search",
                "author": [
                    "C. Liu",
                    "B. Zoph",
                    "M. Neumann",
                    "J. Shlens",
                    "W. Hua",
                    "L.J. Li",
                    "L. Fei-Fei",
                    "A. Yuille",
                    "J. Huang",
                    "K. Murphy"
                ],
                "venue": "Proceedings of the European Conference on Computer Vision (ECCV). pp. 19\u201334",
                "citeRegEx": "33",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Darts: Differentiable architecture search",
                "author": [
                    "H. Liu",
                    "K. Simonyan",
                    "Y. Yang"
                ],
                "venue": "International Conference on Learning Representations (ICLR)",
                "citeRegEx": "34",
                "shortCiteRegEx": null,
                "year": 2019
            },
            {
                "title": "Least Squares Generative Adversarial Networks",
                "author": [
                    "X. Mao",
                    "Q. Li",
                    "H. Xie",
                    "R.Y.K. Lau",
                    "Z. Wang",
                    "S.P. Smolley"
                ],
                "venue": "IEEE International Conference on Computer Vision (ICCV) pp. 1\u201316",
                "citeRegEx": "35",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "A Simple Neural Attentive Meta-Learner",
                "author": [
                    "N. Mishra",
                    "M. Rohaninejad",
                    "X. Chen",
                    "P. Abbeel"
                ],
                "venue": "Advances In Neural Information Processing Systems",
                "citeRegEx": "36",
                "shortCiteRegEx": "36",
                "year": 2017
            },
            {
                "title": "Meta Networks",
                "author": [
                    "T. Munkhdalai",
                    "H. Yu"
                ],
                "venue": "arXiv:1703.00837",
                "citeRegEx": "37",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Rapid adaptation with conditionally shifted neurons",
                "author": [
                    "T. Munkhdalai",
                    "X. Yuan",
                    "S. Mehri",
                    "A. Trischler"
                ],
                "venue": "International Conference on Machine Learning. pp. 3661\u20133670",
                "citeRegEx": "38",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "On first-order meta-learning algorithms",
                "author": [
                    "A. Nichol",
                    "J. Achiam",
                    "J. Schulman"
                ],
                "venue": "arXiv preprint arXiv:1803.02999",
                "citeRegEx": "39",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Asap: Architecture search, anneal and prune",
                "author": [
                    "A. Noy",
                    "N. Nayman",
                    "T. Ridnik",
                    "N. Zamir",
                    "S. Doveh",
                    "I. Friedman",
                    "R. Giryes",
                    "L. Zelnik-Manor"
                ],
                "venue": "arXiv:1904.04123",
                "citeRegEx": "40",
                "shortCiteRegEx": null,
                "year": 2019
            },
            {
                "title": "TADAM: Task dependent adaptive metric for improved few-shot learning",
                "author": [
                    "B.N. Oreshkin",
                    "P. Rodriguez",
                    "A. Lacoste"
                ],
                "venue": "NeurIPS",
                "citeRegEx": "41",
                "shortCiteRegEx": "41",
                "year": 2018
            },
            {
                "title": "Articulated pose estimation with tiny synthetic videos",
                "author": [
                    "D. Park",
                    "D. Ramanan"
                ],
                "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2015Octob, 58\u201366",
                "citeRegEx": "42",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Efficient neural architecture search via parameter sharing",
                "author": [
                    "H. Pham",
                    "M. Y Guan",
                    "B. Zoph",
                    "Q.V. Le",
                    "J. Dean"
                ],
                "venue": "International Conference on Machine Learning (ICML)",
                "citeRegEx": "43",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Few-shot image recognition by predicting parameters from activations",
                "author": [
                    "S. Qiao",
                    "C. Liu",
                    "W. Shen",
                    "A.L. Yuille"
                ],
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7229\u20137238",
                "citeRegEx": "44",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
                "author": [
                    "A. Radford",
                    "L. Metz",
                    "S. Chintala"
                ],
                "venue": "arXiv:1511.06434 pp. 1\u201316",
                "citeRegEx": "45",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Optimization As a Model for Few-Shot Learning",
                "author": [
                    "S. Ravi",
                    "H. Larochelle"
                ],
                "venue": "ICLR pp",
                "citeRegEx": "46",
                "shortCiteRegEx": "46",
                "year": 2017
            },
            {
                "title": "Regularized evolution for image classifier architecture search",
                "author": [
                    "E. Real",
                    "A. Aggarwal",
                    "Y. Huang",
                    "Q.V. Le"
                ],
                "venue": "International Conference on Machine Learning ICML AutoML Workshop",
                "citeRegEx": "47",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Few-shot autoregressive density estimation: towards learning to learn distributions",
                "author": [
                    "S. Reed",
                    "Y. Chen",
                    "T. Paine",
                    "A. van den Oord",
                    "S.M.A. Eslami",
                    "D. Rezende",
                    "O. Vinyals",
                    "N. de Freitas"
                ],
                "venue": "arXiv:1710.10304",
                "citeRegEx": "48",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Metric Learning with Adaptive Density Discrimination",
                "author": [
                    "O. Rippel",
                    "M. Paluri",
                    "P. Dollar",
                    "L. Bourdev"
                ],
                "venue": "arXiv:1511.05939 pp",
                "citeRegEx": "49",
                "shortCiteRegEx": "49",
                "year": 2015
            },
            {
                "title": "ImageNet Large Scale Visual Recognition Challenge",
                "author": [
                    "O. Russakovsky",
                    "J. Deng",
                    "H. Su",
                    "J. Krause",
                    "S. Satheesh",
                    "S. Ma",
                    "Z. Huang",
                    "A. Karpathy",
                    "A. Khosla",
                    "M. Bernstein",
                    "A.C. Berg",
                    "L. Fei-Fei"
                ],
                "venue": null,
                "citeRegEx": "50",
                "shortCiteRegEx": "50",
                "year": 2015
            },
            {
                "title": "Meta-Learning with Latent Embedding Optimization",
                "author": [
                    "A.A. Rusu",
                    "D. Rao",
                    "J. Sygnowski",
                    "O. Vinyals",
                    "R. Pascanu",
                    "S. Osindero",
                    "R. Hadsell"
                ],
                "venue": null,
                "citeRegEx": "51",
                "shortCiteRegEx": "51",
                "year": 2018
            },
            {
                "title": "Meta-Learning with Memory-Augmented Neural Networks",
                "author": [
                    "A. Santoro",
                    "S. Bartunov",
                    "M. Botvinick",
                    "D. Wierstra",
                    "T. Lillicrap"
                ],
                "venue": "Journal of Machine Learning Research 48(Proceedings of The 33rd International Conference on Machine Learning), 1842\u2013 1850",
                "citeRegEx": "52",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Baby steps towards few-shot learning with multiple semantics",
                "author": [
                    "E. Schwartz",
                    "L. Karlinsky",
                    "R. Feris",
                    "R. Giryes",
                    "A.M. Bronstein"
                ],
                "venue": "arXiv preprint arXiv:1906.01905",
                "citeRegEx": "53",
                "shortCiteRegEx": null,
                "year": 2019
            },
            {
                "title": "Delta-Encoder: an Effective Sample Synthesis Method for Few-Shot Object Recognition",
                "author": [
                    "E. Schwartz",
                    "L. Karlinsky",
                    "J. Shtok",
                    "S. Harary",
                    "M. Marder",
                    "A. Kumar",
                    "R. Feris",
                    "R. Giryes",
                    "A.M. Bronstein"
                ],
                "venue": null,
                "citeRegEx": "54",
                "shortCiteRegEx": "54",
                "year": 2018
            },
            {
                "title": "Prototypical Networks for Few-shot Learning",
                "author": [
                    "J. Snell",
                    "K. Swersky",
                    "R. Zemel"
                ],
                "venue": "NIPS (2017),",
                "citeRegEx": "55",
                "shortCiteRegEx": "55",
                "year": 2017
            },
            {
                "title": "Render for CNN Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views.pdf",
                "author": [
                    "H. Su",
                    "C.R. Qi",
                    "Y. Li",
                    "L.J. Guibas"
                ],
                "venue": "IEEE International Conference on Computer Vision (ICCV) pp",
                "citeRegEx": "56",
                "shortCiteRegEx": "56",
                "year": 2015
            },
            {
                "title": "Learning to Compare: Relation Network for Few-Shot Learning",
                "author": [
                    "F. Sung",
                    "Y. Yang",
                    "L. Zhang",
                    "T. Xiang",
                    "P.H.S. Torr",
                    "T.M. Hospedales"
                ],
                "venue": null,
                "citeRegEx": "57",
                "shortCiteRegEx": "57",
                "year": 2017
            },
            {
                "title": "Learning to compare: Relation network for few-shot learning",
                "author": [
                    "F. Sung",
                    "Y. Yang",
                    "L. Zhang",
                    "T. Xiang",
                    "P.H. Torr",
                    "T.M. Hospedales"
                ],
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1199\u20131208",
                "citeRegEx": "58",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
                "author": [
                    "M. Tan",
                    "Q.V. Le"
                ],
                "venue": "arXiv preprint arXiv:1905.11946",
                "citeRegEx": "59",
                "shortCiteRegEx": null,
                "year": 2019
            },
            {
                "title": "Matching Networks for One Shot Learning",
                "author": [
                    "O. Vinyals",
                    "C. Blundell",
                    "T. Lillicrap",
                    "K. Kavukcuoglu",
                    "D. Wierstra"
                ],
                "venue": "NIPS",
                "citeRegEx": "60",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Low-Shot Learning from Imaginary Data",
                "author": [
                    "Y.X. Wang",
                    "R. Girshick",
                    "M. Hebert",
                    "B. Hariharan"
                ],
                "venue": null,
                "citeRegEx": "61",
                "shortCiteRegEx": "61",
                "year": 2018
            },
            {
                "title": "Learning from Small Sample Sets by Combining Unsupervised Meta-Training with CNNs",
                "author": [
                    "Y.X. Wang",
                    "M. Hebert"
                ],
                "venue": "Advances In Neural Information Processing Systems",
                "citeRegEx": "62",
                "shortCiteRegEx": "62",
                "year": 2016
            },
            {
                "title": "Learning to Learn: Model Regression Networks for Easy Small Sample Learning",
                "author": [
                    "Y.X. Wang",
                    "M. Hebert"
                ],
                "venue": "European Conference on Computer Vision (ECCV) pp. 616\u2013634",
                "citeRegEx": "63",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification",
                "author": [
                    "K.Q. Weinberger",
                    "L.K. Saul"
                ],
                "venue": "The Journal of Machine Learning Research 10, 207\u2013244",
                "citeRegEx": "64",
                "shortCiteRegEx": null,
                "year": 2009
            },
            {
                "title": "Snas: Stochastic neural architecture search",
                "author": [
                    "S. Xie",
                    "H. Zheng",
                    "C. Liu",
                    "L. Lin"
                ],
                "venue": "International Conference on Learning Representations (ICLR)",
                "citeRegEx": "65",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Adaptive Cross-Modal Few-Shot Learning",
                "author": [
                    "C. Xing",
                    "N. Rostamzadeh",
                    "B.N. Oreshkin",
                    "P.O. Pinheiro"
                ],
                "venue": "Arxiv (2019),",
                "citeRegEx": "66",
                "shortCiteRegEx": "66",
                "year": 1902
            },
            {
                "title": "Semantic Jitter: Dense Supervision for Visual Comparisons via Synthetic Images",
                "author": [
                    "A. Yu",
                    "K. Grauman"
                ],
                "venue": "Proceedings of the IEEE International Conference on Computer Vision 2017-Octob, 5571\u20135580",
                "citeRegEx": "67",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Deep Meta-Learning: Learning to Learn in the Concept Space",
                "author": [
                    "F. Zhou",
                    "B. Wu",
                    "Z. Li"
                ],
                "venue": null,
                "citeRegEx": "68",
                "shortCiteRegEx": "68",
                "year": 2018
            },
            {
                "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks",
                "author": [
                    "J.Y. Zhu",
                    "T. Park",
                    "P. Isola",
                    "A.A. Efros"
                ],
                "venue": "Proceedings of the IEEE International Conference on Computer Vision 2017-Octob, 2242\u20132251",
                "citeRegEx": "69",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Neural architecture search with reinforcement learning",
                "author": [
                    "B. Zoph",
                    "Q.V. Le"
                ],
                "venue": "International Conference on Learning Representations (ICLR)",
                "citeRegEx": "70",
                "shortCiteRegEx": null,
                "year": 2017
            }
        ],
        "abstractText": "Few-Shot Learning (FSL) is a topic of rapidly growing interest. Typically, in FSL a model is trained on a dataset consisting of many small tasks (meta-tasks) and learns to adapt to novel tasks that it will encounter during test time. This is also referred to as meta-learning. Another topic closely related to meta-learning with a lot of interest in the community is Neural Architecture Search (NAS), automatically finding optimal architecture instead of engineering it manually. In this work we combine these two aspects of meta-learning. So far, meta-learning FSL methods have focused on optimizing parameters of pre-defined network architectures, in order to make them easily adaptable to novel tasks. Moreover, it was observed that, in general, larger architectures perform better than smaller ones up to a certain saturation point (where they start to degrade due to over-fitting). However, little attention has been given to explicitly optimizing the architectures for FSL, nor to an adaptation of the architecture at test time to particular novel tasks. In this work, we propose to employ tools inspired by the Differentiable Neural Architecture Search (D-NAS) literature in order to optimize the architecture for FSL without over-fitting. Additionally, to make the architecture task adaptive, we propose the concept of \u2018MetAdapt Controller\u2019 modules. These modules are added to the model and are meta-trained to predict the optimal network connections for a given novel task. Using the proposed approach we observe state-of-the-art results on two popular few-shot benchmarks: miniImageNet and FC100."
    },
    {
        "title": "Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions",
        "sections": [
            {
                "heading": "1 Introduction",
                "text": "Explaining a complex system through their cause and effect relations is one of the fundamental challenges in science. Data is collected and experiments are performed with the intent of understanding how a certain phenomenon comes about, or how the underlying system works, which could be social, biological, artificial, among others. The study of causal relations can be seen through the lens of learning and inference [16, 21]. The learning component is concerned with discovering the causal structure, which is the very subject of interest in many domains, since they can provide insight about\n\u2217Equal contribution.\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nhow a complex system works and lead to better understanding about the phenomenon under investigation. The latter, inference, attempts to leverage the causal structure to compute quantitative claims about the effect of interventions and retrospective counterfactuals, which are critical to assign credit, understand blame and responsibility, and perform judgement about fairness in decision-making.\nOne of the most popular languages used to encode the invariances needed to reason about causal relations, for both learning and inference, is based on graphical models, and appears under the rubric of causal graphs [16, 21, 2]. A causal graph is a directed acyclic graph (DAG) with latent variables, where each edge encodes a causal relationship between its endpoints: X is a direct cause of Y , i.e., X \u2192 Y , if, when the remaining factors are held constant, forcing X to take a specific value affects the realization of Y , where X,Y are random variables representing some relevant features of the system.\nThe task of learning the causal structure entails a search over the space of causal graphs that are compatible with the observed data; the collection of these graphs forms what is called an equivalence class. The most popular mark imprinted on the data by the underlying causal structure that is used to delineate an equivalence class are conditional independence (CI) relations. These relations are the most basic type of probabilistic invariances used in the field and have been studied at large in the context of graphical models since, at least, [15] (see also [5]). While CIs are powerful and have been the driving force behind some of the most prominent structural learning algorithms in the field [16, 21], including the PC, FCI, these are constraints specific for one distribution.\nIn this paper, we start by noting something very simple, albeit powerful, that happens when a combination of observational and experimental distributions are available: There are constraints over the graphical structure that emerge by comparing these different distributions, and which are not of CI-type2. Remarkably, and unknown until our work, the converse of the causal calculus developed by Pearl [18] offers a systematic way of reading these constraints and tying them back to the underlying graphical structure. In reference to their connection to the do-calculus rules (or a generalization, as discussed later), we call these constraints the do-constraints. For concreteness, consider the graph in Fig. 1(a), where the dashed-bidirected arrow represents hidden variables that generate variations of the two observed variables, X,Y in this case. Suppose the observational (conditional) distribution and an interventional distribution on X are available, which are written as P(y|x), P(y|do(x)), respectively. Suppose we contrast these two distributions and the test evaluating the expression P(y|do(x)) = P(y|x) comes out as false. This is called a do-see test since the experimental (or \u201cdo\u201d) and observational (\u201csee\u201d) distributions are contrasted. Based on the second rule of do-calculus, one can infer that there is an open backdoor path from X to Y , where the edge adjacent to X on this path has an arrowhead into X. In our setting, we do not have access to the true graph, but we leverage this and the other do-constraints to reverse engineer the process and try to learn the structure. Broadly speaking, do-constraints will play a critical role for learning, in the same way CI/d-separation plays in learning when only observational data is available. To the best of our knowledge, this type of constraints appeared first at the very definition of causal Bayesian networks (CBNs) in [1] and then were leveraged to design efficient experiments to learn the causal graph in [12].\nWe assume throughout this work that interventions are soft. A soft intervention affects the mechanism that generates the variable, while keeping the causal connections intact. Soft-interventions are widely employed in biology and medicine, where it is hard to change the underlying system, but possibly\n2Recall that a CI represents a constraint readable from one specific distribution saying that the value of Z is irrelevant for computing the likelihood of Y once we know the value of X, i.e., P(Y |X,Z) = P(Y |X),\u2200X,Y,Z.\neasier to perturb it. For our characterization, we utilize an extension of the causal calculus to soft interventions introduced in [4]. Under soft-interventions, the do-see test can be written as checking if Px(y|x) = P(y|x), where Px is the distribution obtained after a soft intervention on X. The second observation leveraged here follows from another realization by Pearl that interventions can be represented explicitly in the graphical model [17]. He then introduced what we call F-nodes, which graphically encode the changes due to an intervention and the corresponding parametrization (see also [16, Sec. 3.2.2]). This is important in our context since the do-calculus tests will be visible more explicitly in the graph. The graph obtained by adding F-nodes to the causal graph is called the augmented graph. The same construct was used more prominently in [6] in the context of inference and identification. Going back to Fig. 1b, the existence of the backdoor path from X to Y , as detected by rule 2 of the calculus, can be captured by the statement FX is not d-separated from Y given X. In the context of structure learning, similar constructions have been leveraged in the literature [13, 24].\nWe further make a specific assumption throughout the paper about the soft-interventions. We call it the controlled experiment setting, where each variable is intervened with the same mechanism change across different interventions. For example, in Fig. 1c, suppose we are given distributions from two controlled experiments Px, Px,z along with observational data. We can then use Fz to capture the invariances between Px,z and Px. For example, if Px,z(y) , Px(y), for some y, we can read that FZ 6\u22a5 Y\n\u2223\u2223\u2223FX , FX,Z . Accordingly, given a set of interventional distributions, we construct an augmented graph by introducing an F-node for every unique set difference between pairs of controlled intervention sets (more on that later on). Without the controlled experiment assumption, our machinery can still be used if one knows which mechanism changes are identical and by constructing F-nodes to reflect and capture the mechanism difference across two interventions. For simplicity of presentation, however, we restrict ourselves to the controlled experiment setting and do not pursue this route explicitly.\nTo encapsulate the distributional invariants directly induced by the causal calculus rules3, we call a set of interventional distributions I-Markov to a graph, if these distributions respect the causal calculus rules relative to that graph. Note that the notion of I-Markov is first introduced in [9, 10] for causally sufficient systems without the use of do-constraints4. For our characterization, we first extend the causal calculus rules to operate between arbitrary sets of interventions. We call two causal graphs D1,D2 I-Markov equivalent if the set of distributions that are I-Markov toD1 andD2 are the same. Using the augmented graph, we identify a graphical condition that is necessary and sufficient for two CBNs with latents to be I-Markov equivalent. Finally, we propose a sound algorithm for learning the augmented graph from interventional data. Our contributions can be summarized as follows:\n\u2022 We propose a characterization of I-Markov equivalence between two causal graphs with latent variables for a given intervention set I that is based on a generalization of do-calculus rules to arbitrary subsets of interventions.\n\u2022 We show a graphical characterization of I-Markov equivalence of causal graphs with latents. \u2022 We introduce a learning algorithm for inferring the graphical structure using a combination\nof observational and interventional data and utilizing the corresponding new constraints. This procedure comes with a new set of orientation rules. We formally show its soundness."
            },
            {
                "heading": "2 Background and Related Work",
                "text": "In this section, we introduce necessary concepts that we use throughout the paper. Upper case letters denote variables and lower case letters denote an assignment. Also, bold letters denote sets.\nCausal Bayesian Network (CBN): Let P(v) be a probability distribution over a set of variables V, and let Px(v) denote the distribution resulting from the hard intervention do(X = x), which sets X \u2286 V to constants x. Let P\u2217 denote the set of all interventional distributions Px(v), for all X \u2286 V, including P(V). A directed acyclic graph (DAG) over V is said to be a causal Bayesian network compatible with P\u2217 if and only if, for all X \u2286 V, Px(v) = \u220f {i|Vi<X} P(vi|pai), for all v consistent with x, and where Pai is the set of parents of Vi [16, 1, pp. 24]. If so, we refer to the DAG as causal.\n3There may be constraints that can be obtained by applying the rules multiple times we do not consider here. 4In the causally sufficient case, name is in reference to both global and local Markov conditions. However, in our work, the name stems from our observation that the do-constraints correspond to the global Markov conditions in the augmented graph.\nGiven that a subset of the variables are unmeasured or latent,D(V\u222aL,E) represents the causal graph where V and L denote the measured and latent variables, respectively, and E denotes the edges. A dashed bi-directed edge is used instead of\u2190 L \u2192, where L \u2208 L, whenever L is a root node with exactly two children. The observed distribution P(v) is obtained by marginalizing L out.\nP(v) = \u2211\nL \u220f {i|Ti\u2208V\u222aL} P(ti|pai)\nClearly, the joint distribution over V does not factorize relative to D in a typical fashion, since Markovianity is no longer valid, but it does relative to both V and L. Still, CI relations can be read from the graph using a graphical criterion known as d-separation. Also, two causal graphs are called Markov equivalent whenever they share the same set of conditional independences over V.\nSoft Interventions: Another common type of intervention is soft, where the original conditional distributions of the intervened variables X are replaced with new ones, without completely eliminating the causal effect of the parents. Accordingly, the interventional distribution Px(v) becomes as follows, where P\u2032(Xi|Pai) , P(Xi|Pai) is the new conditional distribution set by the intervention:\nPx(v) = \u2211\nL \u220f {i|Xi\u2208X} P\u2032(xi|pai) \u220f { j|T j<X} P(t j|pa j)\nIn this work, we assume that all the soft interventions are controlled. This means that for any two interventions I, J \u2286 V where Xi \u2208 I \u2229 J, we have PI(Xi|Pai) = PJ(Xi|Pai). Ancestral graphs: We now introduce a graphical representation of equivalence classes of causal graphs with latent nodes. A mixed graph can contain directed and bi-directed edges. A is an ancestor of B if there is a directed path from A to B. A is a spouse of B if A \u2194 B is present. If A is both a spouse and an ancestor of B, this creates an almost directed cycle. An inducing path relative to L is a path on which every non-endpoint node X < L is a collider on the path (i.e., both edges incident to the node are into it) and every collider is an ancestor of an endpoint of the path. A mixed graph is ancestral if it does not contain a directed or almost directed cycle. It is maximal if there is no inducing path (relative to the empty set) between any two non-adjacent nodes. A Maximal Ancestral Graph (MAG) is a graph that is both ancestral and maximal [19]. Given a causal graphD(V,L), a MAGMD over V can be constructed such that both the independence and the ancestral relations among variables in V are retained, see, for example, [27, p. 6].\nA triple \u3008X,Y,Z\u3009 is an unshielded triple if X and Y are adjacent, Y and Z are adjacent, and X and Z are not adjancent. If both edges are into Y , then the triple is referred to as unshielded collider. A path between X and Y , p = \u3008X, . . . ,W,Z,Y\u3009, is a discriminating path for Z if (1) p includes at least three edges; (2) Z is a non-endpoint node on p, and is adjacent to Y on p; and (3) X is not adjacent to Y , and every node between X and Z is a collider on p and is a parent of Y . Two MAGs are Markov equivalent if and only if (1) they have the same adjacencies; (2) they have the same unshielded colliders; and (3) if a path p is a discriminating path for a vertex Z in both graphs, then Z is a collider on the path in one graph if and only if it is a collider on the path in the other. A PAG, which represents a Markov equivalence class of a MAG, is learnable from the independence model over the observed variables, and the FCI algorithm is a standard sound and complete method to learn such an object [28].\nRelated Work: Learning causal graphs from a combination of observational and interventional data has been studied in the literature [3, 11, 7, 20, 8, 12, 23]. For causally sufficient systems, the notion and characterization of interventional Markov equivalence has been introduced in [9, 10]. More recently, [24] showed that the same characterization can be used for both hard and soft interventions. For causally insufficient systems, [22] uses SAT solvers to learn a summary graph over the observed variables given data from different experimental conditions. [13] introduces an algorithm to pool experimental datasets together and runs a modification of FCI to learn an augmented graph; however, they do not consider characterizing an equivalence class.\nNotations: For random variables X,Y,Z, the CI relation X is independent of Y conditioned on Z is shown by X \u22a5 Y |Z . The d-separation statement node X is d-separated from Y given Z in graphD is shown by (X \u22a5 Y |Z )D. I \u2286 2V is reserved for a set of interventions, where 2V is the power set of V. We show the symmetric difference by I4J B (I \\ J) \u222a (J \\ I). DX denotes the graph obtained fromD where all the incoming edges to the set of nodes in X are removed. Similarly,DX denotes the removal of outgoing edges. We assume that there is no selection bias. A star on an endpoint of an edge \u2217\u2212\u2217 is used as a wildcard to denote circle, arrowhead, or tail."
            },
            {
                "heading": "3 Do-Constraints \u2013 Combining Observational and Experimental Distributions",
                "text": "One of the most celebrated results in causal inference comes under the rubric of do-calculus (or causal calculus) [18, 16]. The calculus consists of a set of inference rules that allows one to create a map between distributions generated by a causal graph when certain graphical conditions hold in the graph. The calculus was developed in the context of hard interventions, and recent work presented a generalization of this result for soft interventions [4], which we state next:\nTheorem 1 (Special case of Thm. 1 in [4]). Let D = (V \u222a L,E) be a causal graph. Then, the following holds for any strictly positive distribution consistent withD. Rule 1 (see-see): For any X \u2286 V and disjoint Y,Z,W \u2286 V\nPx(y|w, z) = Px(y|w) if Y \u22a5 Z |W inD. Rule 2 (do-see): For any disjoint X,Y,Z \u2286 V and W \u2282 V \\ (Z \u222a Y)\nPx,z(y|z,w) = Px(y|z,w) if Y \u22a5 Z |W inDZ . Rule 3 (do-do): For any disjoint X,Y,Z \u2286 V and W \u2282 V \\ (Z \u222a Y)\nPx,z(y|w) = Px(y|w) if Y \u22a5 Z |W inDZ(W), where Z(W) \u2286 Z are non-ancestors of W inD.\nThe first rule of the calculus is a d-separation type of statement relative to a specific interventional distribution Px, which says that Y \u22a5 Z |W inD implies the corresponding conditional independence Px(y|w, z) = Px(y|w). Note that the converse of this rule is the work horse underlying most of the structure learning algorithms found in practice, which says that if some independence hold in P, this would imply a corresponding graphical separation (under faithfulness). In the case just mentioned, this would imply that Y and Z should be separated inD, meaning, they have neither a directed nor a bidirected arrow connecting them.\nFrom this understanding, we make a very simple, albeit powerful observation \u2013 i.e., the converse of the other two rules should offer insights about the underlying graphical structure as well. To witness, consider the causal graph D = {X \u2192 Y, X cd Y}, and suppose we have the observational and interventional distributions P(Y, X) and PX(Y, X), respectively. Using the CI tests P(Y, X) , P(Y)\u00b7P(X) and PX(Y, X) , PX(Y) \u00b7 PX(X), we infer that the two variables are dependent (or not independent) and consequently d-connected in the graph, while no claim can be made about the causal relation between them. Given the inequality PX(Y) , P(Y), we infer that the condition for rule 3 does not hold and Y 6\u22a5 X inDX . Hence, X must be a cause of Y \u2013 changing the value of X has a downstream effect on Y . Similarly, given the inequality PX(Y |X) , P(Y |X), the condition related to rule 2 does not hold, and Y 6\u22a5 X inDX . The implication in this case is that there is an unblockable backdoor path between X and Y that is into X, i.e., a latent variable. Alternatively, ifD = {X \u2192 Y}, then PX(Y |X) = P(Y |X), under faithfulness, implies the absence of a latent variable by the converse of rule 2.\nBroadly speaking, rule 3 allows one to infer causal relations between variables, and consequently directed edges in the causal graph. Since the compared interventional distributions differ by a subset of interventions (Z), we call this the do-do test. On the other hand, rule 2 allows one to infer spurious relations between variables, and consequently latent variables in the causal graph5. The do-see naming of the test stems from the fact that we compare a distribution with an intervention on a subset Z (do) versus another which only conditions on Z (see). Naturally, rule 1 is the usual conditional independence test that allows one to detect that neither directed nor bidirected arrow exists.\nPutting together these rules, we show in Corollary 1 a generalization of rules 2 and 3 . Note that rule 2 appears when J \u2282 I and I \\ J \u2286W; similarly, rule 3 can be seen when J \u2282 I and (I \\ J) \u2229W = \u2205. Corollary 1 (mixed do-do/do-see). Let D = (V \u222a L,E) be a causal graph. Under the controlled intervention assumption, for any I, J \u2286 V and disjoint Y,W \u2286 V, we have the following:\nPI(y|w) = PJ(y|w) if Y \u22a5 K |W \\Wk inDWk,R(W),\nwhere K B I4J, Wk CW \u2229K, R B K\\Wk, and R(W) \u2286 R are non-ancestors of W inD. 5More precisely, rule 2 allows us to detect inducing paths that are into both variables.\nIn general, the proposed rule is a mixture of rules 2 and 3 as we could be conditioning in W on a subset of the symmetrical difference set I4J. For instance, consider the causal graph D = {C cd A \u2192 B,C cd B} and suppose we have the interventional distributions PA,B and PC,B. Since B \u22a5 {A,C} in DA,C , then PA,B(B|A) = PB,C(B|A). This generalization will soon play a significant role in the characterization and learning of the interventional equivalence class."
            },
            {
                "heading": "4 Interventional Markov Equivalence under Do-constraints",
                "text": "In this section, the new do-constraints will be used to define the notion of interventional Markov equivalence. Then, we will characterize when two causal graphs are equivalent in accordance to the proposed definition. We start by defining the notion of interventional Markov as shown below. Definition 1. Consider the tuples of absolutely continuous probability distributions (PI)I\u2208I over a set of variables V. A tuple (PI)I\u2208I satisfies the I-Markov property with respect to a graphD = (V\u222aL,E) if the following holds for disjoint Y,Z,W \u2286 V:\n(1) For I \u2208 I: PI(y|w, z) = PI(y|w) if Y \u22a5 Z |W inD.\n(2) For I, J \u2208 I: PI(y|w) = PJ(y|w) if Y \u22a5 K |W \\Wk inDWk,R(W),\nwhere K B I4J, Wk CW \u2229K, R B K\\Wk, and R(W) \u2286 R are non-ancestors of W inD. The set of all tuples that satisfy the I-Markov property with respect toD are denoted by PI (D,V).\nThe two conditions used in the definition correspond to rule 1 of Theorem 1 and that of Corollary 1. Notice that the traditional Markov definition only considers the first condition over the observational distribution P(V); a case included in the I-Markov whenever \u2205 \u2208 I . Accordingly, two causal graphs are said to be I-Markov equivalent if they license the same set of distribution tuples. This notion is formalized in the following definition. Definition 2. Given two causal graphsD1 = (V\u222aL1,E1) andD2 = (V\u222aL2,E2), and an intervention set I \u2286 2V,D1 andD2 are called I-Markov equivalent if PI (D1,V) = PI (D2,V).\nOne challenge with Definition 1 is that testing for the d-separation statement in condition (2) requires a mutilated graph where we cut some of the edges in D. This makes it harder to represent all the constraints imposed by a causal graph compactly. Accordingly, we use the notion of an augmented graph that is introduced below (Definition 3). In words, the construction of the augmented graph goes as follows. First, initialize the augmented graph to the input causal graph. Then, for every distinct symmetric set difference between I, J \u2208 I , denoted by Si, introduce a new node Fi and make it a parent to each node in Si, i.e., Fi \u2192 S \u2208 Si. Note that this type of construction has been used in the literature to model interventions [17, 6]. For example, for I = {\u2205, {X}}, Figure 2a presents the augmented graph corresponding to the causal graph, which is the induced subgraph over {X,W,Z,Y}. Node Fx is added in accordance with the symmetrical difference set (\u2205 \\ {X}) \u222a ({X} \\ \u2205) = {X}. Definition 3 (Augmented graph). Consider a causal graphD = (V \u222a L,E) and an intervention set I \u2286 2V. Let S = {S1,S2, . . . ,Sk} = {S : \u2203I, J \u2208 I s.t. I4J = S }. The augmented graph of D with respect to I , denoted as AugI (D), is the graph constructed as follows: AugI (D) = (V \u222a F ,E \u222a E) where F B {Fi}i\u2208[k] and E = {(Fi, j)}i\u2208[k], j\u2208Si .\nThe significance of the augmented graph construction is illustrated by Proposition 1, which provides criteria to test the d-separation statements in Definition 1 equivalently from the corresponding augmented graph of a causal graph. Back to the example in Figure 2a, the statement Y \u22a5 X |Z in\nDX can be equivalently tested by the statement Y \u22a5 Fx |Z in the corresponding augmented graph. Similarly, Y \u22a5 X inDX can be equivalently tested by Y \u22a5 Fx |X in AugI (D). Proposition 1. Consider a causal graphD = (V \u222a L,E) and the corresponding augmented graph AugI (D) = (V \u222a L \u222a F ,E \u222a E) with respect to an intervention set I , where F = {Fi}i\u2208[k]. Let Si be the set of nodes adjacent to Fi,\u2200i \u2208 [k]. We have the following equivalence relations. For disjoint Y,Z,W \u2286 V:\n(Y \u22a5 Z |W )D \u21d0\u21d2 (Y \u22a5 Z \u2223\u2223\u2223W, F[k] )Aug(D) (1)\nFor disjoint Y,W \u2286 V, where Wi BW \u2229 Si,R B Si \\Wi: (Y \u22a5 Si |W \\Wi )DWi ,R(W) \u21d0\u21d2 (Y \u22a5 Fi \u2223\u2223\u2223W, F[k]\\{i} )Aug(D) (2) In order to characterize causal graphs that are I-Markov equivalent, we draw some insight from the Markov equivalence of causal graphs with latents. Ancestral graphs, and more specifically MAGs, were proposed as a representation to encode the d-separation statements of a causal graph among the measured variables while not explicitly encoding the latent nodes. The definition below (Def. 4) introduces the augmented MAG that is constructed over an augmented graph. Since all the constraints in the I-Markov definition can be tested by d-separation statements in the augmented graph, then an augmented MAG preserves all those constraints. For example, Figs. 2c and 2d present the augmented MAGs corresponding to the augmented graphs in Figs. 2a and 2b, respectively. Notice that Fx and Y are adjacent in both MAGs since they are not separable by any set in the augmented graphs. Definition 4 (Augmented MAG). Given a causal graphD = (V \u222a L,E) and an intervention set I , the augmented MAG is the MAG constructed over V from AugI (D), i.e., MAG(AugI (D)).\nBelow, we derive a characterization for two causal graphs to be I-Markov equivalent \u2013 two causal graphs are I-Markov equivalent if their corresponding augmented MAGs satisfy the three conditions given in Theorem 2. For example, the two augmented MAGs in Figures 2c and 2d satisfy the three conditions, hence the original causal graphs are in the same I-Markov equivalence class. Theorem 2. Two causal graphs D1 = (V \u222a L1,E1) and D2 = (V \u222a L2,E2) are I-Markov equivalent for a set of controlled experiments I if and only if for M1 = MAG(AugI(D1)) and M2 = MAG(AugI (D2)):\n1. M1 andM2 have the same skeleton;\n2. M1 andM2 have the same unshielded colliders;\n3. If a path p is a discriminating path for a node Y in bothM1 andM2, then Y is a collider on the path in one graph if and only if it is a collider on the path in the other."
            },
            {
                "heading": "5 Learning by Combining Observations and Experiments",
                "text": "In this section, we develop an algorithm to learn the augmented graph from a combination of observational and interventional data, which consequently recovers the causal graph. However, similar to the observational case, it is typically impossible to completely determine the causal graph from the available measured data, especially when latents are present. Then, the objective is to learn a class of augmented MAGs consistent with data. For this, we define an augmented PAG as follows. Definition 5. Given a causal graph D and an intervention set I , letM = MAG(AugI(D)) and let [M] be the set of augmented MAGs corresponding to all the causal graphs that are I-Markov equivalent toD. An Augmented PAG forD, denoted G = PAG(AugI (D)), is a graph such that:\n1. G has the same adjacencies asM, and any member of [M] does; and\n2. every non-circle mark in G is an invariant mark in [M].\nAs with any learning algorithm, some faithfulness assumption is needed to infer graphical properties from the corresponding distributional constraints. Hence, we assume that the given interventional distributions are c-faithful to the causal graphD as defined below.\nAlgorithm 1 Algorithm for Learning Augmented PAG 1: function LearnAugPAG(I , (PI)I\u2208I ,V) 2: (F ,S, \u03c3)\u2190 CreateAugmentedNodes(I ,V) 3: V\u2190 V \u222a F 4: Phase I: Learn Adjacencies and Seperating Sets 5: Form the complete graph G on V where between every pair of nodes there is an edge \u25e6\u2212\u25e6. 6: for Every pair X,Y \u2208 V do 7: if X \u2208 F \u2227 Y \u2208 F then 8: S epS et(X,Y)\u2190 \u2205, S epFlag(X,Y) = True 9: else 10: (S epS et(X,Y), S epFlag)\u2190 Do-Constraints((PI)I\u2208I , X,Y,V,F , \u03c3) 11: if S epFlag = True then 12: Remove the edge between X,Y in G. 13: Phase II: Learn Unshielded Colliders 14: For every unshielded triple \u3008X,Z,Y\u3009 in G, orient it as X\u2217\u2192 Z \u2190\u2217Y iff Z < S epS et(X,Y) 15: Phase III: Apply Orientation Rules 16: Apply 7 FCI rules in [28] together with the following 2 additional rules until none applies. 17: Rule 8: For any Fk \u2208 F , orient adjacent edges out of Fk. 18: Rule 9: For any Fk \u2208 F that is adjacent to a node Y < Sk 19: If |Sk | = 1, orient X \u2217\u2212\u2217 Y as X \u2192 Y for X \u2208 Sk.\nAlgorithm 2 Creating F-nodes. 1: function CreateAugmentedNodes(I ,V) 2: F = \u2205,S = \u2205, k = 0, \u03c3 : N\u2192 2V \u00d7 2V 3: for all pairs I, J \u2208 I , if I4J < S do 4: Set k \u2190 k + 1, set Sk = I4J, add Fk to F , add Sk to S, set \u03c3(k) = (I, J).\nreturn F ,S, \u03c3\nDefinition 6. Consider a causal graphD = (V \u222a L,E). A tuple of distributions (PI)I\u2208I \u2208 P(D,V) is called c-faithful to graphD if the converse for each of the conditions given in Definition 1 holds.\nAlgorithm 1 presents a modification of the FCI algorithm to learn augmented PAGs. To explain the algorithm, we first describe FCI which, given an independence model over the measured variables, proceeds in three phases [25]: In phase I, the algorithm initializes a complete graph with circle edges (\u25e6\u2212\u25e6), then it removes the edge between any pair of nodes if a separating set between the pair exists and records the set. In phase II, the algorithm identifies unshielded triples \u3008A, B,C\u3009 and orients the edges into B if B is not in the separating set of A and C. Finally, in phase III, FCI applies the orientation rules. Only one of the rules uses separating sets while the rest use MAG properties, and soundness and completeness of the previous phases \u2013 the skeleton is correct and all the unshielded colliders are discovered. We note that FCI looks for any separating sets, and not necessarily the minimal ones. We also observe that if two nodes X,Y are separated given Z in AugI (D), they are also separated given Z \u222a F since F are root nodes by construction, i.e., all the edges incident on F-nodes are out of them. Algorithm 1 follows a similar flow to that of the FCI. In phase I, it learns the skeleton of the augmented PAG. Function CreateAugmentedNodes(\u00b7) in Alg. 2 creates the F-nodes by computing the set S of unique symmetric difference sets from all pairs of interventions in I . Sigma (\u03c3) maps every F-node to a source pair of interventions, which is used later on to perform the do-tests. The algorithm starts by creating a complete graph of circle edges between V \u222a F . Then, it removes the edge between any two nodes X and Y if a separating set exists. If the two nodes are F-nodes, then they are separated by the empty set by construction. Otherwise, it calls the function Do-Constraints(\u00b7) in Alg. 3 to search for a separating set using the corresponding do-constraints. The function routine works as follows: If the two nodes are random variables (and not F-nodes), then an arbitrary distribution is chosen and we find a subset W that establishes conditional independence between X and Y (rule 1 of Thm. 1). Else, one of the two nodes is an F-node; without loss of generality, we choose it to be X. The algorithm then looks for a subset W that satisfies the invariance of Corollary 1, i.e., PI(y|w) = PJ(y|w). Phase II of Alg. 1 is similar to the FCI counterpart. For the edge orientation phase, note that the augmented MAG is a MAG indeed, hence all the FCI orientation rules still apply. Therefore, phase III\nAlgorithm 3 Find m-separation sets via Calculus Tests. 1: function Do-Constraints(I , (PI)I\u2208I , X,Y,V,F , \u03c3) 2: S epS et = \u2205, S epFlag = False 3: if X < F \u2227 Y < F then 4: Pick I \u2208 I arbitrarily. 5: for W \u2286 V \\ F do 6: if PI(y|w, x) = PI(y|w) then S epS et = W \u222a F , S epFlag = True 7: else 8: Suppose X \u2208 F ,Y < F and X = Fi without loss of generality. 9: (I, J) = \u03c3(i) 10: for W \u2286 V \\ (F \u222a Y) do 11: if PI(y|w) = PJ(y|w) then S epS et = W,F \\ {Fi}, S epFlag = True\nuses the FCI orientation rules along with the following two new ones. The algorithm keeps applying the rules until none applies anymore.\nRule 8 (F-node Edges): For any edge adjacent to an F node, orient the edge out of the F node.\nRule 9 (Inducing Paths): If Fk \u2208 F is adjacent to a node Y < Sk and |Sk | = 1, e.g., Sk = {X}, then orient X \u2217\u2212\u2217 Y out of X, i.e., X \u2192 Y . The intuition for this rule is as follows: If Fk is adjacent to a node Y < Sk in G, then there is an inducing path p between Fk and Y in AugI(D), where D is any causal graph in the equivalence class. Since Fk is a root node and by the properties of inducing paths, the subpath of p from X to Y is an inducing path as well and X is an ancestor of Y in AugI (D). Hence, the edge between X and Y is out of X and into Y in MAG(AugI (D)) and consequently in G. We give an example to illustrate the steps of the algorithm in Figure 3, where I = {\u2205, {X}}. Figure 3a shows the augmented causal graph, i.e., AugI (D), and Figure 3b shows the corresponding augmented MAG, i.e., MAG(AugI (D)). Nodes Fx and Z are separable in AugI (D) given the empty set and this can be tested by the do-constraint P(Z) = PX(Z). Similarly, we can infer the separation of Fx and W by the test P(W |X) = PX(W |X). Figure 3c shows the graph obtained after applying the seven rules of the FCI together with Rule 8. Finally, by applying Rule 9, we infer that the edge between X and Y has a tail at X and we obtain the graph in Figure 3d. The soudness of the algorithm is shown next. Theorem 3. Consider a set of interventional distributions (PI)I\u2208I c-faithful to a causal graph D = (V \u222a L), where I is a set of controlled experiments. Algorithm 1 is sound, i.e., every adjacency and orientation is common for all MAG(Aug(D\u2032)) whereD\u2032 is I-Markov equivalent toD."
            },
            {
                "heading": "6 Conclusions",
                "text": "We investigate the problem of learning the causal structure underlying a phenomenon of interest from a combination of observational and experimental data. We pursue this endeavor by noting that a generalization of the converse of Pearl\u2019s do-calculus (Thm. 1) leads to new tests that can be evaluated against data. These tests, in turn, translate into constraints over the structure itself. We then define an interventional equivalence class based on such criteria (Def. 1), and then derive a graphical characterization for the equivalence of two causal graphs (Thm. 2). Finally, we develop an algorithm to learn an interventional equivalence class from data, which includes new orientation rules."
            },
            {
                "heading": "Acknowledgements",
                "text": "Bareinboim and Jaber are supported in parts by grants from NSF IIS-1704352, IIS-1750807 (CAREER), IBM Research, and Adobe Research. Kocaoglu and Shanmugam are supported by the MIT-IBM Watson AI Lab."
            }
        ],
        "references": [
            {
                "title": "Local characterizations of causal bayesian networks. In Graph Structures for Knowledge Representation and Reasoning (IJCAI), pages 1\u201317",
                "author": [
                    "Elias Bareinboim",
                    "Carlos Brito",
                    "Judea Pearl"
                ],
                "venue": null,
                "citeRegEx": "1",
                "shortCiteRegEx": "1",
                "year": 2012
            },
            {
                "title": "Causal inference and the data-fusion problem",
                "author": [
                    "Elias Bareinboim",
                    "Judea Pearl"
                ],
                "venue": "Proceedings of the National Academy of Sciences,",
                "citeRegEx": "2",
                "shortCiteRegEx": "2",
                "year": 2016
            },
            {
                "title": "Optimal structure identification with greedy search",
                "author": [
                    "David Maxwell Chickering"
                ],
                "venue": "Journal of Machine Learning Research,",
                "citeRegEx": "3",
                "shortCiteRegEx": "3",
                "year": 2002
            },
            {
                "title": "A calculus for stochastic interventions: Causal effect identification and surrogate experiments",
                "author": [
                    "Juan Correa",
                    "Elias Bareinboim"
                ],
                "venue": "Technical report, R-51,",
                "citeRegEx": "4",
                "shortCiteRegEx": "4",
                "year": 2019
            },
            {
                "title": "Conditional independence in statistical theory",
                "author": [
                    "A Philip Dawid"
                ],
                "venue": "Journal of the Royal Statistical Society, Series B,",
                "citeRegEx": "5",
                "shortCiteRegEx": "5",
                "year": 1979
            },
            {
                "title": "Influence diagrams for causal modelling and inference",
                "author": [
                    "A Philip Dawid"
                ],
                "venue": "International Statistical Review,",
                "citeRegEx": "6",
                "shortCiteRegEx": "6",
                "year": 2002
            },
            {
                "title": "Causation and Intervention",
                "author": [
                    "Frederick Eberhardt"
                ],
                "venue": "PhD thesis,",
                "citeRegEx": "7",
                "shortCiteRegEx": "7",
                "year": 2007
            },
            {
                "title": "Budgeted experiment design for causal structure learning",
                "author": [
                    "AmirEmad Ghassami",
                    "Saber Salehkaleybar",
                    "Negar Kiyavash",
                    "Elias Bareinboim"
                ],
                "venue": "In International Conference on Machine Learning (ICML),",
                "citeRegEx": "8",
                "shortCiteRegEx": "8",
                "year": 2018
            },
            {
                "title": "Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs",
                "author": [
                    "Alain Hauser",
                    "Peter B\u00fchlmann"
                ],
                "venue": "Journal of Machine Learning Research,",
                "citeRegEx": "9",
                "shortCiteRegEx": "9",
                "year": 2012
            },
            {
                "title": "Two optimal strategies for active learning of causal networks from interventional data",
                "author": [
                    "Alain Hauser",
                    "Peter B\u00fchlmann"
                ],
                "venue": "In Proceedings of Sixth European Workshop on Probabilistic Graphical Models,",
                "citeRegEx": "10",
                "shortCiteRegEx": "10",
                "year": 2012
            },
            {
                "title": "Experiment selection for causal discovery",
                "author": [
                    "Antti Hyttinen",
                    "Frederick Eberhardt",
                    "Patrik Hoyer"
                ],
                "venue": "Journal of Machine Learning Research,",
                "citeRegEx": "11",
                "shortCiteRegEx": "11",
                "year": 2013
            },
            {
                "title": "Experimental design for learning causal graphs with latent variables",
                "author": [
                    "Murat Kocaoglu",
                    "Karthikeyan Shanmugam",
                    "Elias Bareinboim"
                ],
                "venue": "In Advances in Neural Information Processing Systems,",
                "citeRegEx": "12",
                "shortCiteRegEx": "12",
                "year": 2017
            },
            {
                "title": "Joint causal inference on observational and experimental datasets",
                "author": [
                    "Sara Magliacane",
                    "Tom Claassen",
                    "Joris M Mooij"
                ],
                "venue": "arXiv preprint arXiv:1611.10351,",
                "citeRegEx": "13",
                "shortCiteRegEx": "13",
                "year": 2016
            },
            {
                "title": "Strong completeness and faithfulness in bayesian networks",
                "author": [
                    "Christopher Meek"
                ],
                "venue": "In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence,",
                "citeRegEx": "14",
                "shortCiteRegEx": "14",
                "year": 1995
            },
            {
                "title": "Probabilistic Reasoning in Intelligent Systems",
                "author": [
                    "J. Pearl"
                ],
                "venue": null,
                "citeRegEx": "15",
                "shortCiteRegEx": "15",
                "year": 1988
            },
            {
                "title": "Causality: Models, Reasoning, and Inference",
                "author": [
                    "J. Pearl"
                ],
                "venue": null,
                "citeRegEx": "16",
                "shortCiteRegEx": "16",
                "year": 2000
            },
            {
                "title": "Aspects of graphical models connected with causality",
                "author": [
                    "Judea Pearl"
                ],
                "venue": "Proceedings of the 49th Session of the International Statistical Institute,",
                "citeRegEx": "17",
                "shortCiteRegEx": "17",
                "year": 1993
            },
            {
                "title": "Causal diagrams for empirical research",
                "author": [
                    "Judea Pearl"
                ],
                "venue": "Biometrika, 82(4):669\u2013688,",
                "citeRegEx": "18",
                "shortCiteRegEx": "18",
                "year": 1995
            },
            {
                "title": "Ancestral graph markov models",
                "author": [
                    "Thomas Richardson",
                    "Peter Spirtes"
                ],
                "venue": "The Annals of Statistics,",
                "citeRegEx": "19",
                "shortCiteRegEx": "19",
                "year": 2002
            },
            {
                "title": "Learning causal graphs with small interventions",
                "author": [
                    "Karthikeyan Shanmugam",
                    "Murat Kocaoglu",
                    "Alexandros G Dimakis",
                    "Sriram Vishwanath"
                ],
                "venue": "In Advances in Neural Information Processing Systems,",
                "citeRegEx": "20",
                "shortCiteRegEx": "20",
                "year": 2015
            },
            {
                "title": "Causation, Prediction, and Search",
                "author": [
                    "Peter Spirtes",
                    "Clark Glymour",
                    "Richard Scheines"
                ],
                "venue": "A Bradford Book,",
                "citeRegEx": "21",
                "shortCiteRegEx": "21",
                "year": 2001
            },
            {
                "title": "Constraint-based causal discovery from multiple interventions over overlapping variable sets",
                "author": [
                    "Sofia Triantafillou",
                    "Ioannis Tsamardinos"
                ],
                "venue": "J. Mach. Learn. Res.,",
                "citeRegEx": "22",
                "shortCiteRegEx": "22",
                "year": 2015
            },
            {
                "title": "Permutation-based causal inference algorithms with interventions",
                "author": [
                    "Yuhao Wang",
                    "Liam Solus",
                    "Karren Yang",
                    "Caroline Uhler"
                ],
                "venue": "In Advances in Neural Information Processing Systems,",
                "citeRegEx": "23",
                "shortCiteRegEx": "23",
                "year": 2017
            },
            {
                "title": "Characterizing and learning equivalence classes of causal dags under interventions",
                "author": [
                    "Karren Yang",
                    "Abigail Katoff",
                    "Caroline Uhler"
                ],
                "venue": "In ICML,",
                "citeRegEx": "24",
                "shortCiteRegEx": "24",
                "year": 2018
            },
            {
                "title": "Causal inference and reasoning in causally insufficient systems",
                "author": [
                    "Jiji Zhang"
                ],
                "venue": "PhD thesis,",
                "citeRegEx": "25",
                "shortCiteRegEx": "25",
                "year": 2006
            },
            {
                "title": "A characterization of markov equivalence classes for directed acyclic graphs with latent variables",
                "author": [
                    "Jiji Zhang"
                ],
                "venue": "In Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence,",
                "citeRegEx": "26",
                "shortCiteRegEx": "26",
                "year": 2007
            },
            {
                "title": "Causal reasoning with ancestral graphs",
                "author": [
                    "Jiji Zhang"
                ],
                "venue": "Journal of Machine Learning Research,",
                "citeRegEx": "27",
                "shortCiteRegEx": "27",
                "year": 2008
            },
            {
                "title": "On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias",
                "author": [
                    "Jiji Zhang"
                ],
                "venue": "Artificial Intelligence,",
                "citeRegEx": "28",
                "shortCiteRegEx": "28",
                "year": 2008
            }
        ],
        "abstractText": "The challenge of learning the causal structure underlying a certain phenomenon is undertaken by connecting the set of conditional independences (CIs) readable from the observational data, on the one side, with the set of corresponding constraints implied over the graphical structure, on the other, which are tied through a graphical criterion known as d-separation (Pearl, 1988). In this paper, we investigate the more general setting where multiple observational and experimental distributions are available. We start with the simple observation that the invariances given by CIs/dseparation are just one special type of a broader set of constraints, which follow from the careful comparison of the different distributions available. Remarkably, these new constraints are intrinsically connected with do-calculus (Pearl, 1995) in the context of soft-interventions. We then introduce a novel notion of interventional equivalence class of causal graphs with latent variables based on these invariances, which associates each graphical structure with a set of interventional distributions that respect the do-calculus rules. Given a collection of distributions, two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions, where the elements of the family are indistinguishable using the invariances obtained from a direct application of the calculus rules. We introduce a graphical representation that can be used to determine if two causal graphs are interventionally equivalent. We provide a formal graphical characterization of this equivalence. Finally, we extend the FCI algorithm, which was originally designed to operate based on CIs, to combine observational and interventional datasets, including new orientation rules particular to this setting."
    },
    {
        "title": "Detecting Egregious Conversations between Customers and Virtual Agents",
        "sections": [
            {
                "heading": null,
                "text": "Proceedings of NAACL-HLT 2018, pages 1802\u20131811 New Orleans, Louisiana, June 1 - 6, 2018. c\u00a92018 Association for Computational Linguistics"
            },
            {
                "heading": "1 Introduction",
                "text": "Automated conversational agents (chatbots) are becoming widely used for various tasks such as personal assistants or as customer service agents. Recent studies project that 80% of businesses plan to use chatbots by 20201, and that chatbots will power 85% of customer service interactions by the year 20202. This increasing usage is mainly due to advances in artificial intelligence and natural language processing (Hirschberg and Manning, 2015)\n1http://read.bi/2gU0szG 2http://gtnr.it/2z428RS\nalong with increasingly capable chat development environments, leading to improvements in conversational richness and robustness.\nStill, chatbots may behave extremely badly, leading to conversations so off-the-mark that only a human agent could step in and salvage them. Consequences of these failures may include loss of customer goodwill and associated revenue, and even exposure to litigation if the failures can be shown to include fraudulent claims. Due to the increasing prevalence of chatbots, even a small fraction of such egregious3 conversations could be problematic for the companies deploying chatbots and the providers of chatbot services.\nIn this paper we study detecting these egregious conversations that can arise in numerous ways. For example, incomplete or internally inconsistent training data can lead to false classification of user intent. Bugs in dialog descriptions can lead to dead ends. Failure to maintain adequate context can cause chatbots to miss anaphoric references. In the extreme case, malicious actors may provide heavily biased (e.g., the Tay chatbot4) or even hacked misbehaviors.\nIn this article, we focus on customer care systems. In such setting, a conversation usually becomes egregious due to a combination of the aforementioned problems. The resulting customer frustration may not surface in easily detectable ways such as the appearance of all caps, shouting to a speech recognizer, or the use of profanity or extreme punctuation. Consequently, the chatbot will continue as if the conversation is proceeding well, usually\n3Defined by the dictionary as outstandingly bad. 4http://bit.ly/2fwYaa5\n1802\nleading to conversational breakdown. Consider, for example, the anonymized but representative conversation depicted in Figure 1. Here the customer aims to understand the details of a flight ticket. In the first two turns, the chatbot misses the customer\u2019s intentions, which leads to the customer asking \u201cAre you a real person?\u201d. The customer then tries to explain what went wrong, but the chatbot has insufficient exposure to this sort of utterance to provide anything but the default response (\u201cI\u2019m not trained on that\u201d). The response seems to upset the customer and leads to a request for a human agent, which is rejected by the system (\u201cWe don\u2019t currently have live agents\u201d). Such rejection along with the previous responses could lead to customer frustration (Amsel, 1992).\nBeing able to automatically detect such conversations, either in real time or through log analysis, could help to improve chatbot quality. If detected in real time, a human agent can be pulled in to salvage the conversation. As an aid to chatbot improvement, analysis of egregious conversations can often point to problems in training data or system logic that can be repaired. While it is possible to scan system logs by eye, the sheer volume of conversations may overwhelm the analyst or lead to random sampling that misses important failures. If, though, we can automatically detect the worst conversations (in our experience, typically under 10% of the total),\nthe focus can be on fixing the worst problems. Our goal in this paper is to study conversational features that lead to egregious conversations. Specifically, we consider customer inputs throughout a whole conversation, and detect cues such as rephrasing, the presence of heightened emotions, and queries about whether the chatbot is a human or requests to speak to an actual human. In addition, we analyze the chatbot responses, looking for repetitions (e.g. from loops that might be due to flow problems), and the presence of \u201dnot trained\u201d responses. Finally, we analyze the larger conversational context exploring, for example, where the presence of a \u201dnot trained\u201d response might be especially problematic (e.g., in the presence of strong customer emotion).\nThe main contributions of this paper are twofold: (1) This is the first research focusing on detecting egregious conversations in conversational agent (chatbot) setting and (2) this is the first research using unique agent, customer, and customer-agent interaction features to detect egregiousness.\nThe rest of this paper is organized as follows. We review related work, then we formally define the methodology for detecting egregious conversations. We describe our data, experimental setting, and results. We then conclude and suggest future directions."
            },
            {
                "heading": "2 Related Work",
                "text": "Detecting egregious conversations is a new task, however, there is related work that aim at measuring the general quality of the interactions in conversational systems. These works studied the complementary problem of detecting and measuring user satisfaction and engagement. Early work by (Walker et al., 1997, 2001) discussed a framework that maximizes the user satisfaction by considering measures such as number of inappropriate utterances, recognition rates, number of times user requests repetitions, number of turns per interaction, etc. Shortcomings of this approach are discussed by (Hajdinjak and Mihelic, 2006). Other works focus on predicting the user engagement in such systems. Examples include (Kiseleva et al., 2016b,a; Jiang et al., 2015). Specifically, these\nworks evaluated chat functionality by asking users to make conversations with an intelligent agent and measured the user satisfaction along with other features such as the automatic speech recognition (ASR) quality and intent classification quality. In (Sandbank et al., 2017) the authors presented a conversational system enhanced with emotion analysis, and suggested using emotions as triggers for human escalation. In our work, we likewise use emotion analysis as predictive features for egregious conversation. The works of (Sarikaya, 2017; Sano et al., 2017) studied reasons why users reformulated utterances in such systems. Specifically, in (Sarikaya, 2017) they reported on how the different reasons affect the users\u2019 satisfaction. In (Sano et al., 2017) they focused on how to automatically predict the reason for user\u2019s dissatisfaction using different features. Our work also explores user reformulation (or rephrasing) as one of the features to predict egregious conversations. We build on the previous work by leveraging some of the approaches in our classifier for egregious conversations. In (Walker et al., 2000; Hastie et al., 2002) the authors also looked for problems in a specific setting of spoken conversations. The main difference with our work is that we focus on chat logs for domains for which the expected user utterances are a bit more diverse, using interaction features as well as features that are not sensitive to any architectural aspects of the conversational system (e.g., ASR component). Several other approaches for evaluating chatbot conversations indirectly capture the notion of conversational quality. For example, several prior works borrowed from the field of pragmatics in various metrics around the principles of cooperative conversation (Chakrabarti and Luger, 2013; Saygin A. P., 2002). In (Steidl et al., 2004) they measured dialogue success at the turn level as a way of predicting the success of a conversation as a whole. (Webb et al., 2010) created a measure of dialogue appropriateness to determine its role in maintaining a conversation. Recently, (Liu et al., 2016) evaluated a number of popular measures for dialogue response generation systems and highlighted specific weaknesses in the measures. Simi-\nlarly, in (Sebastian et al., 2009) they developed a taxonomy of available measures for an enduser\u2019s quality of experience for multimodel dialogue systems, some of which touch on conversational quality. All these measures may serve as reasons for a conversation turning egregious, but none try to capture or predict it directly.\nIn the domain of customer service, researchers mainly studied reasons for failure of such systems along with suggestions for improved design (Mimoun et al., 2012; Gnewuch et al., 2017). In (Mimoun et al., 2012) the authors analyzed reasons sales chatbots fail by interviewing chatbots experts. They found that a combination of exaggerated customer expectations along with a reduction in agent performance (e.g., failure to listen to the consumer, being too intrusive) caused customers to stop using such systems. Based on this qualitative study, they proposed an improved model for sales chatbots. In (Gnewuch et al., 2017) they studied service quality dimensions (i.e., reliability, empathy, responsiveness, and tangibility) and how to apply them during agent design. The main difference between those works and ours is that they focus on qualitative high-level analysis while we focus on automatic detection based on the conversations logs."
            },
            {
                "heading": "3 Methodology",
                "text": "The objective of this work is to reliably detect egregious conversations between a human and a virtual agent. We treat this as a binary classification task, where the target classes are \u201cegregious\u201d and \u201cnon-egregious\u201d. While we are currently applying this to complete conversations (i.e., the classification is done on the whole conversation), some of the features examined here could likely be used to detect egregious conversations as they were unfolding in real time. To perform egregious conversation detection, features from both customer inputs and agent responses are extracted, together with features related to the combination of specific inputs and responses. In addition, some of these features are contextual, meaning that they are dependent on where in the conversation they appear.\nUsing this set of features for detecting egre-\ngious conversations is novel, and as our experimental results show, improves performance compared to a model based solely on features extracted from the conversation\u2019s text. We now describe the agent, customer, and combined customer-agent features."
            },
            {
                "heading": "3.1 Agent Response Features",
                "text": "A virtual agent is generally expected to closely simulate interactions with a human operator (Reeves and Nass, 1996; Nass and Moon,Y, 2000; Kra\u0308mer, 2008). When the agent starts losing the context of a conversation, fails in understanding the customer intention, or keeps repeating the same responses, the illusion of conversing with a human is lost and the conversation may become extremely annoying. With this in mind, we now describe the analysis of the agent\u2019s responses and associated features (summarized in the top part of Table 1)."
            },
            {
                "heading": "3.1.1 Repeating Response Analysis",
                "text": "As typically implemented, the virtual agent\u2019s task is to reliably detect the intent of each customer\u2019s utterance and respond meaningfully. Accurate intent detection is thus a fundamental characteristic of well-trained virtual agents, and incorrect intent analysis is reported as the leading cause of user dissatisfaction (Sarikaya, 2017). Moreover, since a classifier (e.g., SVM, neural network, etc.) is often used to detect intents, its probabilistic behavior can cause the agent to repeat the same (or semantically similar) response over and over again, despite the user\u2019s attempt to rephrase the same intent.\nSuch agent repetitions lead to an unnatural interaction (Klu\u0308wer, 2011). To identify the agent\u2019s repeating responses, we measured similarity between agent\u2019s subsequent (not necessarily sequential) turns. We represented each sentence by averaging the pre-trained embeddings5 of each word in the sentence, calculating the cosine similarity between the representations. Turns with a high similarity value6 are considered as repeating responses.\n5https://code.google.com/archive/p/word2vec 6Empirically, similarity values \u2265 0.8"
            },
            {
                "heading": "3.1.2 Unsupported Intent Analysis",
                "text": "Given that the knowledge of a virtual agent is necessarily limited, we can expect that training would not cover all customer intents. If the classifier technology provides an estimate of classification confidence, the agent can respond with some variant of \u201cI\u2019m not trained on that\u201d when confidence is low. In some cases, customers will accept that not all requests are supported. In other cases, unsupported intents can lead to customer dissatisfaction (Sarikaya, 2017), and cascade to an egregious conversation (as discussed below in Section 3.3). We extracted the possible variants of the unsupported intent messages directly from the system, and later matched them with the agent responses from the logs."
            },
            {
                "heading": "3.2 Customer Inputs Features",
                "text": "From the customer\u2019s point of view, an ineffective interaction with a virtual agent is clearly undesirable. An ineffective interaction requires the expenditure of relatively large effort from the customer with little return on the investment (Zeithaml et al., 1990; Mimoun et al., 2012). These efforts can appear as behavioral cues in the customer\u2019s inputs, and include emotions, repetitions, and more. We used the following customer analysis in our model. Customer features are summarized in the middle part of Table 1."
            },
            {
                "heading": "3.2.1 Rephrasing Analysis",
                "text": "When a customer repeats or rephrases an utterance, it usually indicates a problem with the agent\u2019s understanding of the customer\u2019s intent. This can be caused by different reasons as described in (Sano et al., 2017). To measure the similarity between subsequent customer turns to detect repetition or rephrasing, we used the same approach as described in Section 3.1.1. Turns with a high similarity value6 are considered as rephrases."
            },
            {
                "heading": "3.2.2 Emotional Analysis",
                "text": "The customer\u2019s emotional state during the conversation is known to correlate with the conversation\u2019s quality (Oliver, 2014). In order to analyze the emotions that customers exhibit in each turn, we utilized the IBM Tone Analyzer service, available publicly online7.\n7https://ibm.co/2hnYkCv\nThis service was trained using customer care interactions, and infers emotions such as frustration, sadness, happiness. We focused on negative emotions (denoted as NEG EMO) to identify turns with a negative emotional peak (i.e., single utterances that carried high negative emotional state), as well as to estimate the aggregated negative emotion throughout the conversation (i.e., the averaged negative emotion intensity). In order to get a more robust representation of the customer\u2019s negative emotional state, we summed the score of the negative emotions (such as frustration, sadness, anger, etc.) into a single negative sentiment score (denoted as NEG SENT). Note that we used the positive emotions as a filter for other customer features, such as the rephrasing analysis. Usually, high positive emotions capture different styles of \u201cthanking the agent\u201d, or indicate that the customer is somewhat satisfied (Rychalski and Hudson, 2017), thus, the conversation is less likely to become egregious."
            },
            {
                "heading": "3.2.3 Asking for a Human Agent",
                "text": "In examining the conversation logs, we noticed that it is not unusual to find a customer asking to be transferred to a human agent. Such a request might indicate that the virtual agent is not providing a satisfactory service. Moreover, even if there are human agents, they might not be available at all times, and thus, a rejection of such a request is sometimes reasonable, but might still lead to customer frustration (Amsel, 1992)."
            },
            {
                "heading": "3.2.4 Unigram Input",
                "text": "In addition to the above analyses, we also detected customer turns that contain exactly one word. The assumption is that single word (unigram) sentences are probably short customer responses (e.g., no, yes, thanks, okay), which in most cases do not contribute to the egregiousness of the conversation. Hence, calculating the percentage of those turns out of the whole conversation gives us another measurable feature."
            },
            {
                "heading": "3.3 Customer-Agent Interaction Features",
                "text": "We also looked at features across conversation utterance-response pairs in order to capture a more complete picture of the interac-\ntion between the customer and the virtual agent. Here, we considered a pair to be customer utterance followed by an agent response. For example, a pair may contain a turn in which the customer expressed negative emotions and received a response of \u201cnot trained\u201d by the agent. In this case, we would leverage the two analyses: emotional and unsupported intent. Figure 1 gives an example of this in the customer\u2019s penultimate turn. Such interactions may divert the conversation towards becoming egregious. These features are summarized in the last part of Table 1."
            },
            {
                "heading": "3.3.1 Similarity Analysis",
                "text": "We also calculated the similarity between the customer\u2019s turn and the virtual agent\u2019s response in cases of customer rephrasing. This analysis aims to capture the reason for the customer rephrasing. When a similarity score between the customer\u2019s turn and the agent\u2019s response is low, this may indicate a misclassified intent, as the agent\u2019s responses are likely to share some textual similarity to the customer\u2019s utterance. Thus, a low score may indicate a poor interaction, which might lead the conversation to become egregious. Another similarity feature is between two customer\u2019s subsequent turns when the agent\u2019s response was \u201cnot trained\u201d."
            },
            {
                "heading": "3.4 Conversation Egregiousness Prediction Classifier",
                "text": "We trained a binary SVM classifier with a linear kernel. A feature vector for a sample in the training data is generated using the scores calculated for the described features, where each feature value is a number between [0,1]. After the model was trained, test conversations are classified by the model, after being transformed to a feature vector in the same way a training sample is transformed. The SVM classification model (denoted EGR) outputs a label \u201cegregious\u201d or \u201cnon-egregious\u201d as a prediction for the conversation."
            },
            {
                "heading": "4 Experiments",
                "text": ""
            },
            {
                "heading": "4.1 Dataset",
                "text": "We extracted data from two commercial systems that provide customer support via conversational bots (hereafter denoted as company A and company B). Both agents are using similar underlying conversation engines, each embedded in a larger system with its own unique business logic. Company A\u2019s system deals with sales support during an online purchase, while company B\u2019s system deals with technical support for purchased software products. Each system logs conversations, and each conversation is a sequence of tuples, where each tuple consists of {conversation id, turn id, customer input, agent response}. From each system, we randomly extracted 10000 conversations. We further removed conversations that contained fewer than 2 turns, as these are too short to be meaningful since the customer never replied or provided more details about the issue at hand. Figure 2 depicts the frequencies of conversation lengths which follow a power-law relationship. The conversations from company A\u2019s system tend to be longer, with an average of 8.4 turns vs. an average of 4.4 turns for company B."
            },
            {
                "heading": "4.2 Experimental Setup",
                "text": "The first step in building a classification model is to obtain ground truth data. For this purpose, we randomly sampled conversations from our datasets. This sample included 1100 and 200 conversations for company A and company B respectively. The\nsampled conversations were tagged using an in-house tagging system designed to increase the consistency of human judgements. Each conversation was tagged by four different expert judges8. Given the full conversation, each judge tagged whether the conversation was egregious or not following this guideline: \u201cConversations which are extraordinarily bad in some way, those conversations where you\u2019d like to see a human jump in and save the conversation\u201d.\nWe generated true binary labels by considering a conversation to be egregious if at least three of the four judges agreed. The interrater reliability between all judges, measured by Cohen\u2019s Kappa, was 0.72 which indicates high level agreement. This process generated the egregious class sizes of 95 (8.6%) and 16 (8%) for company A and company B, respectively. This verifies the unbalanced data expectation as previously discussed.\nWe also implemented two baseline models, rule-based and text-based, as follows:\nRule-based. In this approach, we look for cases in which the virtual agent responded with a \u201cnot trained\u201d reply, or occurrences of the customer requesting to talk to a human agent. As discussed earlier, these may be indicative of the customer\u2019s dissatisfaction with the nature of the virtual agent\u2019s responses.\nText-based. A model that was trained to predict egregiousness given the conversation\u2019s text (all customer and agent\u2019s text dur-\n8judges that are HCI experts and have experience in designing conversational agents systems.\ning the conversation). This model was implemented using state-of-the-art textual features as in (Herzig et al., 2017). In (Herzig et al., 2017) emotions are detected from text, which can be thought of as similar to our task of predicting egregious conversations.\nWe evaluated these baseline methods against our classifier using 10-fold crossvalidation over company A\u2019s dataset (we did not use company B\u2019s data for training due to the low number of tagged conversations). Since class distribution is unbalanced, we evaluated classification performance by using precision (P), recall (R) and F1-score (F) for each class. The EGR classifier was implemented using an SVM with a linear kernel9."
            },
            {
                "heading": "4.3 Classification Results",
                "text": "Table 2 depicts the classification results for both classes and the three models we explored. The EGR model significantly outperformed both baselines10. Specifically, for the egregious class, the precision obtained by the text-based and EGR models were similar. This indicates that the text analyzed by both models encodes some information about egregiousness. On the other hand, for the recall and hence the F1-score, the EGR model relatively improved the text-based model by 41% and 18%, respectively. We will further analyze the models below."
            },
            {
                "heading": "4.4 Feature Set Contribution Analysis",
                "text": "To better understand the contributions of different sets of features to our EGR model, we examined various features in an incremental fashion. Based on the groups of feature sets that we defined in Section 3, we tested the performance of different group combinations, added in the following order: agent, customer and customer-agent interactions.\n9http://scikit-learn.org/stable/modules/svm.html 10EGR with p < 0.001, using McNemar\u2019s test.\nFigure 3 depicts the results for the classification task. The x-axis represents specific combinations of groups, and the y-axis represents the performance obtained. Figure 3 shows that adding each group improved performance, which indicates the informative value of each group. The figure also suggests that the most informative group in terms of prediction ability is the customer group."
            },
            {
                "heading": "4.5 Cross-Domain Analysis",
                "text": "We also studied how robust our features were: If our features generalize well, performance should not drop much when testing company B with the classifier trained exclusively on the data from company A. Although company A and company B share similar conversation engine platforms, they are completely different in terms of objectives, domain, terminology, etc. For this task, we utilized the 200 annotated conversations of company B as test data, and experimented with the different models, trained on company A\u2019s data. The rule-based baseline does not require training, of course, and could be applied directly.\nTable 3 summarizes the results showing that the performance of the EGR model is relatively stable (w.r.t the model\u2019s performance when it was trained and tested on the same domain), with a degradation of only 9% in F1-score11. In addition, the results also show that the text-based model performs poorly when applied to a different domain (F1-score of 0.11). This may occur since textual features are closely tied to the training domain.\n11EGR model results are statistically significant compared to the baselines models with p < 0.001, using McNemar\u2019s test."
            },
            {
                "heading": "4.6 Models Analysis",
                "text": ""
            },
            {
                "heading": "4.6.1 Customer Rephrasing Analysis",
                "text": "Inspired by (Sarikaya, 2017; Sano et al., 2017) we analyzed the customer rephrasing motivations for both the egregious and the non-egregious classes. First, we detected customer rephrasing as described in Section 3.2.1, and then assigned to each its motivation. Specifically, in our setting, the relevant motivations are12: (1) Natural language understanding (NLU) error - the agent\u2019s intent detection is wrong, and thus the agent\u2019s response is semantically far from the customer\u2019s turn; (2) Language generation (LG) limitation - the intent is detected correctly, but the customer is not satisfied by the response (for example, the response was too generic); (3) Unsupported intent error - the customer\u2019s intent is not supported by the agent.\nIn order to detect NLU errors, we measured the similarity between the first customer turn (before the rephrasing) and the agent response. We followed the methodology presented in (Jovita et al., 2015) claiming that the best answer given by the system has the highest similarity value between the customer turn and the agent answer. Thus, if the similarity was < 0.8 we considered this as an erroneous detection. If the similarity was \u2265 0.8 we considered the detection as correct, and thus the rephrasing occurred due to LG limitation. To detect unsupported intent error we used the approach described in Section 3.1.2. As reported in table 4, rephrasing due to an unsupported intent is more common in egregious conversations (18% vs. 14%), whereas, rephrasing due to generation limitations (LG limitation) is more common in\n12We did not consider other motivations like automatic speech recognition (ASR) errors, fallback to search, and backend failure as they are not relevant to our setting.\nnon-egregious conversations (37% vs. 33%). This indicates that customers are more tolerant of cases where the system understood their intent, but the response is not exactly what they expected, rather than cases where the system\u2019s response was \u201cnot trained\u201d. Finally, the percentage of rephrasing due to wrong intent detection (NLU errors) is similar for both classes, which is somewhat expected as similar underlying systems provided NLU support."
            },
            {
                "heading": "4.6.2 Recall Analysis",
                "text": "We further investigated why the EGR model was better at identifying egregious conversations (i.e., its recall was higher compared to the baseline models). We manually examined 26 egregious conversations that were identified justly so by the EGR model, but misclassified by the other models. Those conversations were particularly prevalent with the agent\u2019s difficulty to identify correctly the user\u2019s intent due to NLU errors or LG limitation. We did not encounter any unsupported intent errors leading to customer rephrasing, which affected the ability of the rule-based model to classify those conversations as egregious. In addition, the customer intents that appeared in those conversations were very diverse. While customer rephrasing was captured by the EGR model, for the text-based model some of the intents were new (did not appear in the training data) and thus were difficult for the model to capture."
            },
            {
                "heading": "5 Conclusions and Future Work",
                "text": "In this paper, we have shown how it is possible to detect egregious conversations using a combination of customer utterances, agent responses, and customer-agent interactional features. As explained, the goal of this work is to give developers of automated agents tools to detect and then solve problems cre-\nated by exceptionally bad conversations. In this context, future work includes collecting more data and using neural approaches (e.g., RNN, CNN) for analysis, validating our models on a range of domains beyond the two explored here. We also plan to extend the work to detect egregious conversations in real time (e.g., for escalating to a human operators), and create log analysis tools to analyze the root causes of egregious conversations and suggest possible remedies."
            }
        ],
        "references": [
            {
                "title": "Frustration Theory: An Analysis of Dispositional Learning and Memory",
                "author": [
                    "Abram Amsel."
                ],
                "venue": "Problems in the Behavioural Sciences. Cambridge University Press. https://doi.org/10.1017/CBO9780511665561.",
                "citeRegEx": "Amsel.,? 1992",
                "shortCiteRegEx": "Amsel.",
                "year": 1992
            },
            {
                "title": "A framework for simulating and evaluating artificial chatter bot conversations",
                "author": [
                    "Chayan Chakrabarti",
                    "George F. Luger."
                ],
                "venue": "FLAIRS Conference.",
                "citeRegEx": "Chakrabarti and Luger.,? 2013",
                "shortCiteRegEx": "Chakrabarti and Luger.",
                "year": 2013
            },
            {
                "title": "Towards designing cooperative and social conversational agents for customer service",
                "author": [
                    "Ulrich Gnewuch",
                    "Stefan Morana",
                    "Alexander Maedche."
                ],
                "venue": "Proceedings of the International Conference on Information Systems (ICIS).",
                "citeRegEx": "Gnewuch et al\\.,? 2017",
                "shortCiteRegEx": "Gnewuch et al\\.",
                "year": 2017
            },
            {
                "title": "The paradise evaluation framework: Issues and findings",
                "author": [
                    "Melita Hajdinjak",
                    "France Mihelic."
                ],
                "venue": "Comput. Linguist. 32(2).",
                "citeRegEx": "Hajdinjak and Mihelic.,? 2006",
                "shortCiteRegEx": "Hajdinjak and Mihelic.",
                "year": 2006
            },
            {
                "title": "What\u2019s the problem: Automatically identifying problematic dialogues in DARPA communicator dialogue systems",
                "author": [
                    "Helen Wright Hastie",
                    "Rashmi Prasad",
                    "Marilyn A. Walker."
                ],
                "venue": "Proceedings of the 40th Annual Meeting of the As-",
                "citeRegEx": "Hastie et al\\.,? 2002",
                "shortCiteRegEx": "Hastie et al\\.",
                "year": 2002
            },
            {
                "title": "Emotion detection from text via ensemble classification using word embeddings",
                "author": [
                    "Jonathan Herzig",
                    "Michal Shmueli-Scheuer",
                    "David Konopnicki."
                ],
                "venue": "Proceedings of the ACM SIGIR International Conference on",
                "citeRegEx": "Herzig et al\\.,? 2017",
                "shortCiteRegEx": "Herzig et al\\.",
                "year": 2017
            },
            {
                "title": "Advances in natural language processing",
                "author": [
                    "Julia Hirschberg",
                    "Christopher D. Manning."
                ],
                "venue": "Science 349(6245):261\u2013266.",
                "citeRegEx": "Hirschberg and Manning.,? 2015",
                "shortCiteRegEx": "Hirschberg and Manning.",
                "year": 2015
            },
            {
                "title": "Automatic online evaluation of intelligent assistants",
                "author": [
                    "Jiepu Jiang",
                    "Ahmed Hassan Awadallah",
                    "Rosie Jones",
                    "Umut Ozertem",
                    "Imed Zitouni",
                    "Ranjitha Gurunath Kulkarni",
                    "Omar Zia Khan."
                ],
                "venue": "Proceedings of the 24th Inter-",
                "citeRegEx": "Jiang et al\\.,? 2015",
                "shortCiteRegEx": "Jiang et al\\.",
                "year": 2015
            },
            {
                "title": "Using vector space model in question answering system",
                "author": [
                    "Jovita",
                    "Linda",
                    "Andrei Hartawan",
                    "Derwin Suhartono."
                ],
                "venue": "Procedia Computer Science 59:305 \u2013 311. International Conference on Computer Science and Computational Intel-",
                "citeRegEx": "Jovita et al\\.,? 2015",
                "shortCiteRegEx": "Jovita et al\\.",
                "year": 2015
            },
            {
                "title": "Predicting user satisfaction with intelligent assistants",
                "author": [
                    "Julia Kiseleva",
                    "Kyle Williams",
                    "Ahmed Hassan Awadallah",
                    "Aidan C. Crook",
                    "Imed Zitouni",
                    "Tasos Anastasakos."
                ],
                "venue": "Proceedings of the 39th International ACM SIGIR Con-",
                "citeRegEx": "Kiseleva et al\\.,? 2016a",
                "shortCiteRegEx": "Kiseleva et al\\.",
                "year": 2016
            },
            {
                "title": "Understanding user satisfaction with intelligent assistants",
                "author": [
                    "Julia Kiseleva",
                    "Kyle Williams",
                    "Jiepu Jiang",
                    "Ahmed Hassan Awadallah",
                    "Aidan C. Crook",
                    "Imed Zitouni",
                    "Tasos Anastasakos."
                ],
                "venue": "Proceedings of the 2016 ACM on Con-",
                "citeRegEx": "Kiseleva et al\\.,? 2016b",
                "shortCiteRegEx": "Kiseleva et al\\.",
                "year": 2016
            },
            {
                "title": "I Like Your Shirt\u201d - Dialogue Acts for Enabling Social Talk in Conversational Agents, pages 14\u201327",
                "author": [
                    "Tina Kl\u00fcwer"
                ],
                "venue": null,
                "citeRegEx": "Kl\u00fcwer.,? \\Q2011\\E",
                "shortCiteRegEx": "Kl\u00fcwer.",
                "year": 2011
            },
            {
                "title": "Social effects of virtual assistants",
                "author": [
                    "Nicole C. Kr\u00e4mer."
                ],
                "venue": "a review of empirical results with regard to communication. In Proceedings of the 8th International Conference on Intelligent Virtual Agents. IVA \u201908.",
                "citeRegEx": "Kr\u00e4mer.,? 2008",
                "shortCiteRegEx": "Kr\u00e4mer.",
                "year": 2008
            },
            {
                "title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue",
                "author": [
                    "Chia-Wei Liu",
                    "Ryan Lowe",
                    "Iulian Serban",
                    "Michael Noseworthy",
                    "Laurent Charlin",
                    "Joelle Pineau"
                ],
                "venue": null,
                "citeRegEx": "Liu et al\\.,? \\Q2016\\E",
                "shortCiteRegEx": "Liu et al\\.",
                "year": 2016
            },
            {
                "title": "Case study embodied virtual agents: An analysis on reasons for failure",
                "author": [
                    "Mohammed Slim Ben Mimoun",
                    "Ingrid Poncin",
                    "Marion Garnier."
                ],
                "venue": "Journal of Retailing and Consumer Services 19(6):605 \u2013 612.",
                "citeRegEx": "Mimoun et al\\.,? 2012",
                "shortCiteRegEx": "Mimoun et al\\.",
                "year": 2012
            },
            {
                "title": "Machines and mindlessness: Social responses to computers",
                "author": [
                    "C. Nass",
                    "Y. Moon"
                ],
                "venue": "Journal of Social Issues",
                "citeRegEx": "Nass and Moon,? \\Q2000\\E",
                "shortCiteRegEx": "Nass and Moon",
                "year": 2000
            },
            {
                "title": "Satisfaction: A behavioral perspective on the consumer",
                "author": [
                    "Richard L Oliver."
                ],
                "venue": "Routledge.",
                "citeRegEx": "Oliver.,? 2014",
                "shortCiteRegEx": "Oliver.",
                "year": 2014
            },
            {
                "title": "The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Places",
                "author": [
                    "Byron Reeves",
                    "Clifford Nass."
                ],
                "venue": "Cambridge University Press, New York, NY, USA.",
                "citeRegEx": "Reeves and Nass.,? 1996",
                "shortCiteRegEx": "Reeves and Nass.",
                "year": 1996
            },
            {
                "title": "Asymmetric effects of customer emotions on satisfaction and loyalty in a utilitarian service context",
                "author": [
                    "Aude Rychalski",
                    "Sarah Hudson."
                ],
                "venue": "Journal of Business Research 71:84 \u2013 91.",
                "citeRegEx": "Rychalski and Hudson.,? 2017",
                "shortCiteRegEx": "Rychalski and Hudson.",
                "year": 2017
            },
            {
                "title": "Ehctool: Managing emotional hotspots for conversational agents",
                "author": [
                    "Tommy Sandbank",
                    "Michal Shmueli-Scheuer",
                    "Jonathan Herzig",
                    "David Konopnicki",
                    "Rottem Shaul."
                ],
                "venue": "Proceedings of the 22nd Inter-",
                "citeRegEx": "Sandbank et al\\.,? 2017",
                "shortCiteRegEx": "Sandbank et al\\.",
                "year": 2017
            },
            {
                "title": "Predicting causes of reformulation in intelligent assistants",
                "author": [
                    "Shumpei Sano",
                    "Nobuhiro Kaji",
                    "Manabu Sassano."
                ],
                "venue": "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue. pages 299\u2013309.",
                "citeRegEx": "Sano et al\\.,? 2017",
                "shortCiteRegEx": "Sano et al\\.",
                "year": 2017
            },
            {
                "title": "The technology behind personal digital assistants: An overview of the system architecture and key components",
                "author": [
                    "R. Sarikaya."
                ],
                "venue": "IEEE Signal Processing Magazine 34(1):67\u201381.",
                "citeRegEx": "Sarikaya.,? 2017",
                "shortCiteRegEx": "Sarikaya.",
                "year": 2017
            },
            {
                "title": "Pragmatics in humancomputer conversations",
                "author": [
                    "P. Cicekli I. Saygin A."
                ],
                "venue": "Journal of Pragmatics 34(3).",
                "citeRegEx": "A.,? 2002",
                "shortCiteRegEx": "A.",
                "year": 2002
            },
            {
                "title": "A taxonomy of quality of service and quality of experience of multimodal human-machine interaction",
                "author": [
                    "Moller Sebastian",
                    "Klaus-Peter Engelbrecht",
                    "Christine Kuhnel",
                    "Ina Wechsung",
                    "Benjamin Weiss."
                ],
                "venue": "Quality of Mul-",
                "citeRegEx": "Sebastian et al\\.,? 2009",
                "shortCiteRegEx": "Sebastian et al\\.",
                "year": 2009
            },
            {
                "title": "Looking at the Last Two Turns, I\u2019d Say",
                "author": [
                    "Stefan Steidl",
                    "Christian Hacker",
                    "Christine Ruff",
                    "Anton Batliner",
                    "Elmar N\u00f6th",
                    "J\u00fcrgen Haas"
                ],
                "venue": null,
                "citeRegEx": "Steidl et al\\.,? \\Q2004\\E",
                "shortCiteRegEx": "Steidl et al\\.",
                "year": 2004
            },
            {
                "title": "Paradise: A framework for evaluating spoken dialogue agents",
                "author": [
                    "Marilyn A. Walker",
                    "Diane J. Litman",
                    "Candace A. Kamm",
                    "Alicia Abella."
                ],
                "venue": "Proceedings of the 35th Annual Meeting of the Association for Computational Linguis-",
                "citeRegEx": "Walker et al\\.,? 1997",
                "shortCiteRegEx": "Walker et al\\.",
                "year": 1997
            },
            {
                "title": "Quantitative and qualitative evaluation of darpa communicator spoken dialogue systems",
                "author": [
                    "Marilyn A. Walker",
                    "Rebecca Passonneau",
                    "Julie E. Boland."
                ],
                "venue": "Proceedings of the 39th Annual Meeting on Association for Computational",
                "citeRegEx": "Walker et al\\.,? 2001",
                "shortCiteRegEx": "Walker et al\\.",
                "year": 2001
            },
            {
                "title": "Using natural language processing and discourse features to identify understanding errors",
                "author": [
                    "Marilyn A. Walker",
                    "Jeremy H. Wright",
                    "Irene Langkilde."
                ],
                "venue": "Proceedings of the Seventeenth International Conference on Machine Learn-",
                "citeRegEx": "Walker et al\\.,? 2000",
                "shortCiteRegEx": "Walker et al\\.",
                "year": 2000
            },
            {
                "title": "Evaluating human-machine conversation for appropriateness",
                "author": [
                    "Nick Webb",
                    "David Benyon",
                    "Preben Hansen",
                    "Oil Mival."
                ],
                "venue": "Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC\u201910).",
                "citeRegEx": "Webb et al\\.,? 2010",
                "shortCiteRegEx": "Webb et al\\.",
                "year": 2010
            },
            {
                "title": "Delivering quality service: Balancing customer perceptions and expectations",
                "author": [
                    "Valarie Zeithaml",
                    "A Parsu Parasuraman",
                    "Leonard Berry."
                ],
                "venue": "1811",
                "citeRegEx": "Zeithaml et al\\.,? 1990",
                "shortCiteRegEx": "Zeithaml et al\\.",
                "year": 1990
            }
        ],
        "abstractText": "Virtual agents are becoming a prominent channel of interaction in customer service. Not all customer interactions are smooth, however, and some can become almost comically bad. In such instances, a human agent might need to step in and salvage the conversation. Detecting bad conversations is important since disappointing customer service may threaten customer loyalty and impact revenue. In this paper, we outline an approach to detecting such egregious conversations, using behavioral cues from the user, patterns in agent responses, and useragent interaction. Using logs of two commercial systems, we show that using these features improves the detection F1-score by around 20% over using textual features alone. In addition, we show that those features are common across two quite different domains and, arguably, universal."
    },
    {
        "title": "Unsupervised Dual-Cascade Learning with Pseudo-Feedback Distillation for Query-based Extractive Summarization",
        "sections": [
            {
                "heading": null,
                "text": "ar X\niv :1\n81 1.\n00 43\n6v 1\n[ cs\nWe propose Dual-CES \u2013 a novel unsupervised, query-focused, multi-document extractive summarizer. Dual-CES is designed to better handle the tradeoff between saliency and focus in summarization. To this end, Dual-CES employs a two-step dual-cascade optimization approach with saliency-based pseudo-feedback distillation. Overall, Dual-CES significantly outperforms all other state-of-the-art unsupervised alternatives. Dual-CES is even shown to be able to outperform strong supervised summarizers."
            },
            {
                "heading": "1 Introduction",
                "text": "The vast amounts of textual data end users need to consume motivates the need for automatic summarization [7]. An automatic summarizer gets as an input one or more documents and possibly also a limit on summary length (e.g., maximum number of words). The summarizer then needs to produce a textual summary that captures the most salient (general and informative) content parts within input documents. Moreover, the summarizer may also be required to satisfy a specific user information need, expressed by one or more queries. Therefore, the summarizer will need to produce a focused summary which includes the most relevant information to that need."
            },
            {
                "heading": "1.1 Motivation",
                "text": "While both saliency and focus goals should be considered within a query-focused summarization setting, these goals may be actually conflicting with each other [2]. Higher saliency usually comes at the expense of lower focus and vice-versa. Moreover, such a tradeoff may directly depend on summary length.\n\u2217Contact author: haggai@il.ibm.com\nTo illustrate the effect of summary length on this tradeoff, using the DUC 2007 dataset, Figure 1 reports the summarization quality which was obtained by the Cross Entropy Summarizer (CES) \u2013 a state of the art unsupervised query-focused multidocument extractive summarizer [6]. Saliency was measured according to cosine similarity between the summary\u2019s bigram representation and that of the input documents. Focus was further measured relatively to how much the summary\u2019s induced unigram model is \u201cconcentrated\u201d around query-related words.\nAs we can observe in Figure 1, with the relaxation of the summary length limit, where a more lengthy summary is being allowed, saliency increases at the expense of focus. Laying towards more saliency would result in a better coverage of general and more informative content. Yet, this would result in the inclusion of less relevant content to the specific information need in mind."
            },
            {
                "heading": "1.2 Towards a better tradeoff handling",
                "text": "Aiming at better handling the saliency versus focus tradeoff, in this work, we propose Dual-CES \u2013 an extended CES summarizer [6]. Similar to CES, Dual-CES is an unsupervised query-focused, multi-document, extractive summarizer. To this end, like CES, Dual-CES utilizes the Cross Entropy method [21] for selecting a subset of sentences extracted from input documents, whose combination is predicted to produce a good summary.\nYet, differently from CES, Dual-CES does not attempt to address both saliency and focus goals in a single optimization step. Instead, Dual-CES implements a novel two-step dual-cascade optimization approach, which utilizes two sequential CES-like invocations. Using such an approach, Dual-CES tries to handle the tradeoff by gradually shifting from generating a long summary that is more salient in the first step to generating a short summary that is more focused in the second step. Moreover, DualCES utilizes the long summary that was generated in the first step for saliency-based pseudo-feedback distillation, which allows to generate a final focused summary with better saliency. Dual-CES provides a fully unsupervised end-to-end query-focused multi-document extractive summarization solution.\nUsing an evaluation with the DUC 2005, 2006 and 2007 benchmarks, we show that, Dual-CES generates a focused (and shorter) summary which has much higher saliency (and hence a better tradeoff handling). Overall, Dual-CES provides a significantly better summarization quality compared to other alternative unsupervised summarizers; and in many cases, it even outperforms that of state-of-the art supervised summarizers."
            },
            {
                "heading": "2 Related Work",
                "text": "In this work we employ an unsupervised learning approach for the task of query-based multi-document extractive summarization. Many previous works have employed various unsupervised and/or supervised learning methods for the same task. Some learning systems rank sentences based on their surface and/or graph level features [3, 15, 18]. Others have used various sparse coding techniques for selecting a subset of sentences that minimizes a given documents reconstruction error [12, 26, 16, 9, 11] or used a variational auto-encoder for sentence representation [13].\nAttention models incorporated within deep-learning summarization architectures have further been suggested for improving sentence ranking and selection [1, 12, 20]. Such models try to simulate a human attentive reading behaviour. This allows to better account for context-sensitive features during summarization. Compared to these works, we do not try to attend for sentence ranking or selection. Alternatively, we distill informative hints from summarized documents, aiming to improve the saliency of produced focused summaries.\nFinally, reinforcement learning methods have been recently considered [4, 6, 17, 19]. Among such methods, the CES summarizer [6] is the only one which is both query-sensitive and unsupervised. Similar to CES, we also utilize the Cross Entropy (CE) method [21], a global policy search optimization framework, for solving the sentence subset selection problem. Yet, differently from CES, we utilize the CE method twice, each time with a slightly-different summarization goal in mind (i.e., first saliency and then focus). Moreover, we utilize the distilled saliency-based pseudo-feedback to improve the summarization policy search between such switched (dual) goals. To the best of our knowledge, this on its own, serves as a novel aspect of our work."
            },
            {
                "heading": "3 Background",
                "text": "Here we provide background details on our summarization task and the Cross Entropy method which we use for implementing Dual-CES."
            },
            {
                "heading": "3.1 Summarization task",
                "text": "We address the query-focused, multi-document summarization task. Formally, let q denote some user information need for documents summarization, which may be expressed by one or more queries. Let D denote a set of one or more matching documents to be summarized and Lmax be the maximum allowed summary length (in words).\nWe implement an extractive summarization approach. Our goal is to produce a length-limited summary S by extracting salient content parts in D which are further relevant (focused) to q.\nFollowing [6], we now cast the summarization task as a sentence subset selection problem. To this end, we produce summary S (with maximum length Lmax) by choosing a subset of sentences s \u2208 D which maximizes a given quality target Q(S|q,D)."
            },
            {
                "heading": "3.2 Unsupervised summarization",
                "text": "Dual-CES is an unsupervised summarizer. Similar to CES, it utilizes the Cross Entropy method [21] for selecting the most \u201cpromising\u201d subset of sentences in D. Since we assume an unsupervised setting, no actual reference summaries are available for training nor can we directly optimize an actual quality target Q(S|q,D). Instead, following [6], Q(S|q,D) is \u201csurrogated\u201d by several summary quality prediction measures Q\u0302i(S|q,D) (i = 1, 2, . . . ,m). Each \u201cpredictor\u201d Q\u0302i(S|q,D) is designed to estimate the level of saliency or focus of a given candidate summary S and is presumed to correlate (up to some extent) with actual summarization quality, e.g., ROUGE [14]. For simplicity, similar to CES, various predictions are assumed to be independent and are combined into a single optimization objective by taking their product, i.e.: Q\u0302(S|q,D) def =\n\u220fm i=1 Q\u0302i(S|q,D)."
            },
            {
                "heading": "3.3 Using the Cross Entropy method",
                "text": "The CE-method provides a generic Monte-Carlo optimization framework for solving hard combinatorial problems [21]. Previously, it was utilized for solving the sentence subset selection problem [6].\nTo this end, the CE-method gets as an input Q\u0302(\u00b7|q,D), a constraint on maximum summary length L and an optional pseudo-reference summary SL, whose usage will be explained later on. Let CEM(Q\u0302(\u00b7|q,D), L, SL) denote a single invocation of the CE-method. The result of such an invocation is a single length-feasible summary S\u2217 which contains a subset of sentences selected from D which maximizes Q\u0302(\u00b7|q,D). For example, CES is implemented by invoking CEM(Q\u0302CES(\u00b7|q,D), Lmax, \u2205).\nWe next briefly explain how the CE-method solves this problem. For a given sentence s \u2208 D, let \u03d5(s) denote the likelihood that it should be included in summary S. Starting with a selection policy with the highest entropy (i.e.: \u03d50(s) = 0.5), the CEMethod learns a selection policy \u03d5\u2217(\u00b7) that maximizes Q\u0302(\u00b7|q,D). To this end, \u03d5\u2217(\u00b7) is incrementally learned using an importance sampling approach [21]. At each iteration t = 1, 2, . . ., a sample of N sentence-subsets Sj is generated according to the selection policy \u03d5t\u22121(\u00b7) which was learned in the previous iteration t \u2212 1. The likelihood of picking a sentence s \u2208 D at iteration t is estimated (via cross-entropy minimization) as follows:\n\u03d5t(s) def = \u2211N j=1 \u03b4[Q\u0302(Sj |q,D)\u2265\u03b3t]\u03b4[s\u2208Sj ] \u2211N\nj=1 \u03b4[Q\u0302(Sj |q,D)\u2265\u03b3t]\n. (1)\nHere, \u03b4[\u00b7] denotes the Kronecker-delta (indicator) function and \u03b3t denotes the (1\u2212 \u03c1)-quantile (\u03c1 \u2208 (0, 1)) of the sample performances Q\u0302(Sj |q,D) (j = 1, 2, . . . , N). Therefore, the likelihood of picking a sentence s \u2208 D will increase when it is being included in more (subset) samples whose performance is above the current minimum required quality target value \u03b3t. We further smooth \u03d5t(\u00b7) as follows: \u03d5t(\u00b7) \u2032 = \u03b1\u03d5t\u22121(\u00b7) + (1\u2212 \u03b1)\u03d5t(\u00b7); with \u03b1 \u2208 [0, 1] [21]. Upon its termination, the CE-method is expected to converge to the global optimal selection policy \u03d5\u2217(\u00b7) [21]. We then produce a single summary S\u2217 \u223c \u03d5\u2217(\u00b7). To enforce\nthat only feasible summaries will be produced, following [6], we set Q\u0302(Sj |q,D) = \u2212\u221e whenever a sampled summary Sj length exceeds the L word limit."
            },
            {
                "heading": "4 The Dual-CES summarizer",
                "text": "Differently from CES, Dual-CES does not attempt to maximize both saliency and focus goals in a single optimization step. Instead, Dual-CES implements a novel twostep dual-cascade optimization approach (see Figure 2), which utilizes two CES-like invocations. Both invocations consider the same sentences powerset solution space. Yet, each such invocation utilizes a bit different set of summary quality predictors Q\u0302i(S|q,D), depending on whether the summarizer\u2019s goal should lay towards higher summary saliency or focus.\nIn the first step, Dual-CES relaxes the summary length constraint, aiming at producing a longer and more salient summary. This summary is then treated as a pseudoeffective reference summary from which saliency-based pseudo-feedback is distilled. Such pseudo-feedback is then utilized in the second step of the cascade for setting an additional auxiliary saliency-driven goal. Yet, at the second step, similar to CES, the primary goal is actually to produce a focused summary (with maximum length limit Lmax). Overall, Dual-CES is simply implemented as follows:\nCEM(Q\u0302Foc(\u00b7|q,D), Lmax,CEM(Q\u0302Sal(\u00b7|q,D), L\u0304, \u2205)).\nHere, Q\u0302Sal(\u00b7|q,D) and Q\u0302Foc(\u00b7|q,D) denote the saliency and focus summary quality objectives which are optimized during the cascade, respectively. Both Q\u0302Sal(\u00b7|q,D) and Q\u0302Foc(\u00b7|q,D) are implemented as a product of several basic predictors. L\u0304 \u2265 Lmax denotes the relaxed summary length hyperparameter. We next elaborate the implementation details of Dual-CES\u2019s dual optimization steps."
            },
            {
                "heading": "4.1 Step 1: Saliency-oriented summarization",
                "text": "The purpose of the first step is to produce a single longer summary (with length L\u0304 \u2265 Lmax) which will be used as a pseudo-reference for saliency-based feedback distillation. As illustrated in Figure 1, with a longer summary length \u2013 a more salient summary may be produced.\nThis step is simply implemented by invoking the CE-method with CEM(Q\u0302Sal(\u00b7|q,D), L\u0304, \u2205). The target measure Q\u0302Sal(\u00b7|q,D) guides the optimization towards the production of a summary with the highest possible saliency. Similar to CES, Q\u0302Sal(\u00b7|q,D) is calculated as the product of several summary quality predictors. Overall, we use five different\npredictors, four of which were previously used in CES [6]. The additional predictor that we introduce is designed to \u201cdrive\u201d the optimization even further towards higher saliency. Next, we shortly describe each predictor. The symbol \u2020 marks whether it was originally employed in CES [6]."
            },
            {
                "heading": "4.1.1 Predictor 1: coverage\u2020",
                "text": "This predictor estimates to what extent (candidate) summary S (generally) covers the document set D. Here, we represent both S and D as term-frequency vectors, considering only bigrams, which commonly represent more important content units [6]. For a given text x, let cos(S, x) def = ~S\u00b7~x\n\u2016~S\u2016\u2016~x\u2016 . The coverage predictor is then defined by\nQ\u0302cov(S|q,D) def = cos(S,D)."
            },
            {
                "heading": "4.1.2 Predictor 2: position-bias\u2020",
                "text": "This predictor biases sentence selection towards sentences that appear earlier in their containing documents. It is calculated as Q\u0302pos(S|q,D) def = |S| \u221a \u220f\ns\u2208S\n(\n1 + 1log(b+pos(s))\n)\n,\nwhere pos(s) is the relative start position (in characters) of sentence s in its containing document and b is a position-bias hyperparameter (fixed to b = 2, following [6])."
            },
            {
                "heading": "4.1.3 Predictor 3: summary length\u2020",
                "text": "This predictor biases towards selection of summaries that are closer to the maximum permitted length. Such summaries contain fewer and longer sentences, and therefore, tend to be more informative. Let len(x) denote the length of text x (in number of words). Here, x may either be a single sentence s \u2208 D or a whole summary S. This predictor is then calculated as Q\u0302len(S|q,D) def = 1|S| len(S), where len(S) = \u2211\ns\u2208S len(s)."
            },
            {
                "heading": "4.1.4 Predictor 4: asymmetric coverage",
                "text": "To target even higher saliency, we suggest a fourth predictor, inspired by the risk minimization framework [27]. To this end, we measure the Kullback-Leibler (KL) \u201csimilarity\u201d between the two (unsmoothed) unigram language models induced from the centroid representation1 of S (\u03b8\u0302S) and D (\u03b8\u0302D), formally:\nQ\u0302KL(S|q,D) def = exp\n(\n\u2212 \u2211 w p(w|\u03b8\u0302S) log p(w|\u03b8\u0302S)\np(w|\u03b8\u0302D)\n)\n."
            },
            {
                "heading": "4.1.5 Predictor 5: focus-drift\u2020",
                "text": "While producing a longer summary may result in higher saliency, as was further illustrated in Figure 1, such a summary may be less focused. Hence, to avoid such focusdrift, while we opt to optimize for higher saliency at this step, the target information\n1Such centroid representation is simply given by concatenating the text of sentences in S or documents\nin D.\nneed q should be still considered. To this end, we add a predictor: Q\u0302qf (S|q,D) def =\n\u2211\nw\u2208q p(w|\u03b8\u0302S), which acts as a \u201cquery-anchor\u201d and measures to what extent summary S\u2019s unigram model is devoted to the information need q."
            },
            {
                "heading": "4.2 Step 2: Focus-oriented summarization",
                "text": "The input to the second step of the cascade consists of the same set of documents D, summary length constraint Lmax and the pseudo-reference summary SL\u0304 that was generated in the previous step. This step is simply implemented by invoking the CEmethod with CEM(Q\u0302Foc(\u00b7|q,D), Lmax, SL\u0304). Here, the target measure Q\u0302Foc(\u00b7|q,D) guides the optimization towards the production of a focused summary, while still keeping high saliency as much as possible. To achieve that, we use an additional focusdriven predictor which bias summary production towards higher focus. Moreover, using the pseudo-reference summary SL\u0304 we introduce an additional auxiliary saliencybased predictor, whose goal is to enhance the saliency of produced focused summary. Overall, Q\u0302Foc(\u00b7|q,D) is calculated as the product of the previous five summary quality predictors (Predictors 1\u22125) and the two additional predictors, whose details are described next."
            },
            {
                "heading": "4.2.1 Predictor 6: query-relevancy\u2020",
                "text": "This predictor estimates the relevancy of summary S to q. For that, we use two similarity measures. The first, following [6], measures the Bhattacharyya similarity (coefficient) between the two (unsmoothed) unigram language models of q and S, i.e.: Q\u0302sim1(S|q,D) def = \u2211\nw\u2208q\n\u221a\np(w|\u03b8\u0302q)p(w|\u03b8\u0302S). The second measures the cosine simi-\nlarity between q and S unigram term-frequency representations, i.e.: Q\u0302sim2(S|q,D) def = cos(S, q). The two similarity measures are then combined into a single measure using their geometric mean, i.e.: Q\u0302sim(S|q,D) def = \u221a Q\u0302sim1(S|q,D) \u00b7 Q\u0302sim2(S|q,D)."
            },
            {
                "heading": "4.2.2 Predictor 7: reference summary (distillation) coverage",
                "text": "We further make use of the pseudo-reference summary SL\u0304, which was produced in the first step, and introduce an additional auxiliary saliency-based predictor. This predictor utilizes pseudo-feedback that is distilled from unique unigram words in SL\u0304. It is calculated as: Q\u0302cov\u2032(S|q,D) def = \u2211\nw\u2208SL\u0304 \u03b4[w\u2208S]. Following [10, 27], we only consider the\ntop-100 most frequent unigrams in SL\u0304.\nIntuitively speaking, SL\u0304 usually will be longer (in words) than any candidate summary S that may be chosen in the second step; hence, SL\u0304 is expected to be more salient than S. Therefore, such a predictor is expected to \u201cdrive\u201d the optimization to prefer those candidate summaries S that include as many salient words from SL\u0304, acting as if they were by themselves longer (and more salient) summaries (than those candidates that include less salient words from SL\u0304)."
            },
            {
                "heading": "4.2.3 Adaptive hyperparameter b adjustment",
                "text": "Apart from salient words in SL\u0304 that are used as feedback, we note that, sentences in SL\u0304 may also provide additional \u201chints\u201d about other properties of informative sentences in D, which may potentially be selected to improve saliency. One such property is the relative start-positions of sentences in SL\u0304. To this end, we now assign b = 1|SL\u0304| \u2211 s\u2208SL\u0304 pos(s) (i.e., the average start-position of feedback sentences in SL\u0304) as the value of the position-bias hyperparameter within Q\u0302pos(S|q,D) (Predictor 2)."
            },
            {
                "heading": "4.3 An extension: Length-adaptive Dual-CES",
                "text": "We conclude this section with a suggestion of an extension to Dual-CES that adaptively adjusts the value of hyperparameter L\u0304. To this end, we introduce a new learning parameter Lt which defines the maximum length limit for summary production (sampling) that is allowed at iteration t of the CE-method. We now assume that summary lengths have a Poisson(Lt) distribution of word occurrences with mean Lt. Using importance sampling, this parameter is estimated at iteration t as follows:\nLt def = \u2211N j=1 len(Sj) \u00b7 \u03b4[Q\u0302(Sj |q,D)\u2265\u03b3t] \u2211N\nj=1 \u03b4[Q\u0302(Sj |q,D)\u2265\u03b3t]\n. (2)\nSimilar to \u03d5(\u00b7), we further smooth Lt as follows: Lt \u2032 def= \u03b1Lt\u22121+(1\u2212\u03b1)Lt. Here, \u03b1 \u2208 [0, 1] is the same smoothing hyperparameter which was used to smooth \u03d5(\u00b7) and Lt=0 def = L\u0304."
            },
            {
                "heading": "5 Evaluation",
                "text": ""
            },
            {
                "heading": "5.1 Datasets",
                "text": "Our evaluation is based on the Document Understanding Conferences (DUC) 2005, 2006 and 2007 benchmarks2. These benchmarks are commonly used for evaluating the query-based multi-document summarization task by all of our related works. Given a topic statement, which is expressed by one or more questions, and a set of English documents, the main task is to produce a 250-word (i.e., Lmax = 250) topic-focused summary [5]. The number of topics per benchmark are 50, 50 and 45 in the DUC 2005, 2006 and 2007 benchmarks, respectively. The number of documents to be summarized per topic is 32, 25 and 25 in the DUC 2005, 2006 and 2007 benchmarks, respectively. Each document was pre-segmented (by NIST) into sentences. Following [6], we use Lucene\u2019s English analysis3 for processing the text of topics and documents.\n2http://www-nlpir.nist.gov/projects/duc/data.html 3https://lucene.apache.org/"
            },
            {
                "heading": "5.1.1 Dual-CES implementation",
                "text": "We evaluated both Dual-CES and its adaptive-length variant (hereinafter denoted DualCES-A). To this end, on the first saliency-driven step, for Dual-CES, we fixed the (strict) upper bound limit on summary length to L\u0304 = 1500. Dual-CES-A, on the other hand, adaptively adjusts such length limit and was initialized with Lt=0 = 3000. Both variants were further set with a summary limit Lmax = 250 for their second focus-driven step.\nWe implemented both Dual-CES and Dual-CES-A in Java (JRE8). Further following [6], to reduce CE-method\u2019s runtime, we applied a preliminary step of sentence pruning, where only the top-150 sentences s \u2208 D with the highest (unigram) Bhattacharyya similarity to the topic\u2019s queries were considered for summarization. Similar to [6], the CE-method hyperparameters were fixed as follows: N = 10, 000, \u03c1 = 0.01 and \u03b1 = 0.7.\nFinally, to handle DUC\u2019s complex information needs, we closely followed [6], as follows. First, for each summarized topic, we calculated the query-focused predictions (i.e., Q\u0302qf (\u00b7|q,D) and Q\u0302sim(\u00b7|q,D)) per each one of its questions. To this end, each question was represented as a sub-query by concatenating the main topic\u2019s text to the question\u2019s text. Each sub-query was further expanded with top-100 (unigram) Wikipedia related-words [25]. We then obtained the topic query-sensitive predictions by summing up its various sub-queries\u2019 predictions."
            },
            {
                "heading": "5.1.2 Evaluation measures",
                "text": "The three DUC benchmarks include four reference (ground-truth) human-written summaries per each topic [5]. We measured summarization quality using the ROUGE measure [14], which is the official one for this task [5]. To this end, we used the ROUGE 1.5.5 toolkit with its standard parameters setting4. We report both Recall and F-Measure of ROUGE-1, ROUGE-2 and ROUGE-SU4. ROUGE-1 and ROUGE-2 measure the overlap in unigrams and bigrams between the produced and the reference summaries, respectively. ROUGE-SU4 measures the overlap in skip-grams separated by up to four words.\nFinally, since Dual-CES essentially depends on the CE-method which has a stochastic nature, its quality may depend on the specific seed that was used for random sampling. Hence, following [6], to reduce sensitivity to random seed selection, per each summarization task (i.e., topic and documents pair), we run each Dual-CES variant 30 times (each time with a different random seed) and recorded its mean performance (and 95% confidence interval)."
            },
            {
                "heading": "5.2 Baselines",
                "text": "We compare the summary quality of Dual-CES to the results that were previously reported for several competitive summarization baselines. These baselines include both supervised and unsupervised methods and apply various strategies for handling the\n4ROUGE-1.5.5.pl -a -c 95 -m -n 2 -2 4 -u -p 0.5 -l 250\nsaliency versus focus tradeoff. To distinguish between both types of works, we mark supervised method names with a superscript \u00a7. The first line of baselines utilize various surface and graph level features, namely: BI-PLSA [22], CTSUM [24], HierSum [8], HybHSum\u00a7 [3], MultiMR [23], QODE [28] and SubMod-F\u00a7 [15]. The second line of baselines apply various sparse-coding or auto-encoding techniques, namely: DocRebuild [16], RA-MDS [11], SpOpt [26], and VAEs-A [13]. The third line of baselines incorporate various attention models, namely: AttSum\u00a7 [1], C-Attention [12] and CRSum+SF\u00a7 [20]. We further note that, some baselines, like DocRebuild, SpOpt and C-Attention, use hand-crafted rules for sentence compression.\nFinally, we directly compare with two CES variants, which serve as direct alternatives to Dual-CES. The first one, is the original CES summarizer, whose results are reported in [6]. The second one, denoted hereinafter CES+, utilizes Predictors 1\u22126, which are combined within a single optimized objective (by taking their product). This variant, therefore, allows to directly evaluate the contribution of our proposed dualcascade learning approach which is employed by the two Dual-CES variants."
            },
            {
                "heading": "5.3 Results",
                "text": "The main results of our evaluation are reported in Table 1 (ROUGE-X F-Measure) and Table 2 (ROUGE-X Recall). The numbers reported for the various baselines are the best numbers reported in their respective works. Unfortunately, not all baselines fully reported their results for all benchmarks and measures. Whenever a report on a measure is missing, we further use the symbol \u2019-\u2019."
            },
            {
                "heading": "5.3.1 Dual-CES vs. other baselines",
                "text": "First we note that, among the various baseline methods that we have compared with, CES on its own, serves as the strongest baseline to outperform in most cases. Overall, Dual-CES provides better results compared to any other baseline (and specifically the unsupervised ones). Specifically, on F-Measure, Dual-CES has achieved between 6%\u221214% and 1%\u22123% better ROUGE-2 and ROUGE-1, respectively. On recall, DualCES has achieved between 3%\u22129% better ROUGE-1. On ROUGE-2, in the DUC 2006 and 2007 benchmarks, Dual-CES was about 1%\u2212 9% better, while it was slightly inferior to SubMod-F and CRSum+SF in the DUC 2005 benchmark. Yet, SubMod-F and CRSum+SF are actually supervised, while Dual-CES is fully unsupervised. Therefore, overall, Dual-CES\u2019s ability to reach (even to outperform in many cases) the quality of strong supervised counterparts actually only emphasizes more its potential."
            },
            {
                "heading": "5.3.2 Dual-CES vs. CES variants",
                "text": "Dual-CES significantly improves over the two CES variants in all benchmarks. On F-Measure, Dual-CES has achieved at least between 4% \u2212 5% and 1% \u2212 2% better ROUGE-2 and ROUGE-1, respectively. On recall, Dual-CES has achieved at least between 2%\u22124% and 1%\u22122% better ROUGE-2 and ROUGE-1, respectively. By distilling saliency-based pseudo-feedback between step transitions, Dual-CES manages\nto better utilize the CE-method for selecting a more promising subset of sentences. A case in point is the CES+ variant which is even inferior to CES. A simple combination of all predictors (except Predictor 7 which is unique to Dual-CES since it requires a pseudo-reference summary) does not directly translates to a better tradeoff handling. This, therefore, serves as a strong empirical evidence of the importance of the dualcascade optimization approach implemented by Dual-CES, which allows to produce focused summarizes with better saliency."
            },
            {
                "heading": "5.3.3 Comparison with attentive baselines",
                "text": "The pseudo-feedback distillation approach employed between the two steps of DualCES has some resemblance to attention models that are used by state-of-the-art deep learning summarization methods [1, 12, 20]. First we note that, Dual-CES significantly improves over these attentive baselines on ROUGE-1. On ROUGE-2, Dual-CES is significantly better than C-Attention and AttSum, while it provides (more or less) similar quality to CRSum+SF.\nCloser analysis of the various attention strategies that are employed within these\nbaselines, reveals that, while AttSum only attends on a sentence representation level, C-Attention and CRSum+SF further attend on a word level. Such a more fine-granular attendance results in an improved saliency for the two latter. Yet, while C-Attention first attends on sentences then on words, CRSum+SF performs its attentions reversely. Using Dual-CES as a reference method for comparison, apparently, CRSum+SF attendance on salient words first and then on salient sentences based on such words seems as the better strategy.\nIn a sense, similar to CRSum+SF, Dual-CES also first \u201cattends\u201d on salient words which are distilled from the pseudo-feedback reference summary. Dual-CES then utilizes such salient words for better selection of salient sentences within its second step of\nfocused summary production. Yet, compared to CRSum+SF and similar to C-Attention, Dual-CES\u2019s saliency \u201cattention\u201d process is unsupervised. Moreover, Dual-CES further \u201cattends\u201d on salient sentence positions, which result in better tuning of the position-bias b hyperparameter."
            },
            {
                "heading": "L\u0304 R-1 R-2 R-SU4",
                "text": ""
            },
            {
                "heading": "5.3.4 Hyperparamter L\u0304 sensitivity analysis",
                "text": "Table 3 reports the sensitivity of Dual-CES (measured by ROUGE-X Recall) to the value of hyperparameter L\u0304, using the DUC 2007 benchmark. To this end, we ran DualCES with an increasing L\u0304 value. For further comparison, we also report in Table 3 the results of its adaptive-length version Dual-CES-A. Dual-CES-A is still initialized with Lt=0 = 3000 and adaptively adjusts this hyperparameter. Figure 3 illustrates the (average) learning curve of its adaptive-length parameter Lt.\nOverall, Dual-CES\u2019s summarization quality remains quite stable, exhibiting low sensitivity to L\u0304. Similar stability was further observed for the two other DUC benchmarks. In addition, Figure 3 depicts an interesting empirical outcome: Dual-CES-A converges (more or less) to the best hyperparameter L\u0304 value (i.e., L\u0304 = 1500 in Table 3). Dual-CES-A, therefore, serves as a robust alternative for flexibly estimating such hyperparameter value during runtime. Dual-CES-A can provide similar quality and may outperform Dual-CES."
            },
            {
                "heading": "6 Conclusions and Future work",
                "text": "We proposed Dual-CES, an unsupervised, query-focused, extractive multi-document summarizer. Dual-CES was shown to better handle the tradeoff between saliency and focus, providing the best summarization quality compared to other alternative stateof-the-art unsupervised summarizers. Moreover, in many cases, Dual-CES even outperforms state-of-the-art supervised summarizers. As a future work, we would like to learn to distill from additional pseudo-feedback sources."
            }
        ],
        "references": [
            {
                "title": "Attsum: Joint learning of focusing and summarization with neural attention",
                "author": [
                    "Ziqiang Cao",
                    "Wenjie Li",
                    "Sujian Li",
                    "Furu Wei",
                    "Yanran Li"
                ],
                "venue": null,
                "citeRegEx": "1",
                "shortCiteRegEx": "1",
                "year": 2016
            },
            {
                "title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries",
                "author": [
                    "Jaime Carbonell",
                    "Jade Goldstein"
                ],
                "venue": "In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,",
                "citeRegEx": "2",
                "shortCiteRegEx": "2",
                "year": 1998
            },
            {
                "title": "A hybrid hierarchical model for multidocument summarization",
                "author": [
                    "Asli Celikyilmaz",
                    "Dilek Hakkani-Tur"
                ],
                "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,",
                "citeRegEx": "3",
                "shortCiteRegEx": "3",
                "year": 2010
            },
            {
                "title": "Fast abstractive summarization with reinforce-selected sentence",
                "author": [
                    "Yen-Chun Chen",
                    "Mohit Bansal"
                ],
                "venue": "rewriting. CoRR,",
                "citeRegEx": "4",
                "shortCiteRegEx": "4",
                "year": 2018
            },
            {
                "title": "Overview of duc 2005",
                "author": [
                    "Hoa Trang Dang"
                ],
                "venue": "In Proceedings of the document understanding conference,",
                "citeRegEx": "5",
                "shortCiteRegEx": "5",
                "year": 2005
            },
            {
                "title": "Unsupervised query-focused multi-document summarization using the cross entropy method",
                "author": [
                    "Guy Feigenblat",
                    "Haggai Roitman",
                    "Odellia Boni",
                    "David Konopnicki"
                ],
                "venue": "In Proceedings of the 37th International ACM SIGIR Conference on Research & Development in Information Retrieval,",
                "citeRegEx": "6",
                "shortCiteRegEx": "6",
                "year": 2017
            },
            {
                "title": "Recent automatic text summarization techniques: A survey",
                "author": [
                    "Mahak Gambhir",
                    "Vishal Gupta"
                ],
                "venue": "Artif. Intell. Rev.,",
                "citeRegEx": "7",
                "shortCiteRegEx": "7",
                "year": 2017
            },
            {
                "title": "Exploring content models for multidocument summarization",
                "author": [
                    "Aria Haghighi",
                    "Lucy Vanderwende"
                ],
                "venue": "In Proceedings of the 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,",
                "citeRegEx": "8",
                "shortCiteRegEx": "8",
                "year": 2009
            },
            {
                "title": "Document summarization based on data reconstruction",
                "author": [
                    "Zhanying He",
                    "Chun Chen",
                    "Jiajun Bu",
                    "Can Wang",
                    "Lijun Zhang",
                    "Deng Cai",
                    "Xiaofei He"
                ],
                "venue": "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,",
                "citeRegEx": "9",
                "shortCiteRegEx": "9",
                "year": 2012
            },
            {
                "title": "Relevance based language models",
                "author": [
                    "Victor Lavrenko",
                    "W. Bruce Croft"
                ],
                "venue": "In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,",
                "citeRegEx": "10",
                "shortCiteRegEx": "10",
                "year": 2001
            },
            {
                "title": "Reader-aware multidocument summarization via sparse coding",
                "author": [
                    "Piji Li",
                    "Lidong Bing",
                    "Wai Lam",
                    "Hang Li",
                    "Yi Liao"
                ],
                "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,",
                "citeRegEx": "11",
                "shortCiteRegEx": "11",
                "year": 2015
            },
            {
                "title": "Cascaded attention based unsupervised information distillation for compressive summarization",
                "author": [
                    "Piji Li",
                    "Wai Lam",
                    "Lidong Bing",
                    "Weiwei Guo",
                    "Hang Li"
                ],
                "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
                "citeRegEx": "12",
                "shortCiteRegEx": "12",
                "year": 2017
            },
            {
                "title": "Salience estimation via variational auto-encoders for multi-document summarization",
                "author": [
                    "Piji Li",
                    "Zihao Wang",
                    "Wai Lam",
                    "Zhaochun Ren",
                    "Lidong Bing"
                ],
                "venue": "In AAAI,",
                "citeRegEx": "13",
                "shortCiteRegEx": "13",
                "year": 2017
            },
            {
                "title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out",
                "author": [
                    "Chin-Yew Lin"
                ],
                "venue": "Proceedings of the ACL-04 workshop,",
                "citeRegEx": "14",
                "shortCiteRegEx": "14",
                "year": 2004
            },
            {
                "title": "A class of submodular functions for document summarization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies ",
                "author": [
                    "Hui Lin",
                    "Jeff Bilmes"
                ],
                "venue": "Volume 1,",
                "citeRegEx": "15",
                "shortCiteRegEx": "15",
                "year": 2011
            },
            {
                "title": "An unsupervised multi-document summarization framework based on neural document model",
                "author": [
                    "Shulei Ma",
                    "Zhi-Hong Deng",
                    "Yunlun Yang"
                ],
                "venue": "In Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers,",
                "citeRegEx": "16",
                "shortCiteRegEx": "16",
                "year": 2016
            },
            {
                "title": "Ranking sentences for extractive summarization with reinforcement learning",
                "author": [
                    "Shashi Narayan",
                    "Shay B. Cohen",
                    "Lapata Mirella"
                ],
                "venue": null,
                "citeRegEx": "17",
                "shortCiteRegEx": "17",
                "year": 2018
            },
            {
                "title": "Applying regression models to query-focused multi-document summarization",
                "author": [
                    "You Ouyang",
                    "Wenjie Li",
                    "Sujian Li",
                    "Qin Lu"
                ],
                "venue": "Information Processing & Management,",
                "citeRegEx": "18",
                "shortCiteRegEx": "18",
                "year": 2011
            },
            {
                "title": "A deep reinforced model for abstractive summarization",
                "author": [
                    "Romain Paulus",
                    "Caiming Xiong",
                    "Richard Socher"
                ],
                "venue": "Proceedings of the 6th International Conference on Learning Representations,",
                "citeRegEx": "19",
                "shortCiteRegEx": "19",
                "year": 2018
            },
            {
                "title": "Leveraging contextual sentence relations for extractive summarization 15 using a neural attention model",
                "author": [
                    "Pengjie Ren",
                    "Zhumin Chen",
                    "Zhaochun Ren",
                    "Furu Wei",
                    "Jun Ma",
                    "Maarten de Rijke"
                ],
                "venue": "In Proceedings of the 40th International ACM SI- GIR Conference on Research and Development in Information Retrieval,",
                "citeRegEx": "20",
                "shortCiteRegEx": "20",
                "year": 2017
            },
            {
                "title": "The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation and machine",
                "author": [
                    "Reuven Y Rubinstein",
                    "Dirk P Kroese"
                ],
                "venue": null,
                "citeRegEx": "21",
                "shortCiteRegEx": "21",
                "year": 2004
            },
            {
                "title": "Integrating clustering and multidocument summarization by bi-mixture probabilistic latent semantic analysis (plsa) with sentence bases",
                "author": [
                    "Chao Shen",
                    "Tao Li",
                    "Chris H.Q. Ding"
                ],
                "venue": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,",
                "citeRegEx": "22",
                "shortCiteRegEx": "22",
                "year": 2011
            },
            {
                "title": "Graph-based multi-modality learning for topicfocused multi-document summarization",
                "author": [
                    "Xiaojun Wan",
                    "Jianguo Xiao"
                ],
                "venue": "In Proceedings of the 21st International Jont Conference on Artifical Intelligence,",
                "citeRegEx": "23",
                "shortCiteRegEx": "23",
                "year": 2009
            },
            {
                "title": "Ctsum: Extracting more certain summaries for news articles",
                "author": [
                    "Xiaojun Wan",
                    "Jianmin Zhang"
                ],
                "venue": "In Proceedings of the 37th International ACM SIGIR Conference on Research  Development in Information Retrieval,",
                "citeRegEx": "24",
                "shortCiteRegEx": "24",
                "year": 2014
            },
            {
                "title": "Query dependent pseudo-relevance feedback based on wikipedia",
                "author": [
                    "Yang Xu",
                    "Gareth J.F. Jones",
                    "Bin Wang"
                ],
                "venue": "In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval,",
                "citeRegEx": "25",
                "shortCiteRegEx": "25",
                "year": 2009
            },
            {
                "title": "Compressive document summarization via sparse optimization",
                "author": [
                    "Jin-ge Yao",
                    "Xiaojun Wan",
                    "Jianguo Xiao"
                ],
                "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,",
                "citeRegEx": "26",
                "shortCiteRegEx": "26",
                "year": 2015
            },
            {
                "title": "A risk minimization framework for information retrieval",
                "author": [
                    "ChengXiang Zhai",
                    "John Lafferty"
                ],
                "venue": "Inf. Process. Manage.,",
                "citeRegEx": "27",
                "shortCiteRegEx": "27",
                "year": 2006
            },
            {
                "title": "Query-oriented unsupervised multi-document summarization via deep learning model",
                "author": [
                    "Sheng-hua Zhong",
                    "Yan Liu",
                    "Bin Li",
                    "Jing Long"
                ],
                "venue": "Expert Syst. Appl.,",
                "citeRegEx": "28",
                "shortCiteRegEx": "28",
                "year": 2015
            }
        ],
        "abstractText": "We propose Dual-CES \u2013 a novel unsupervised, query-focused, multi-document extractive summarizer. Dual-CES is designed to better handle the tradeoff between saliency and focus in summarization. To this end, Dual-CES employs a two-step dual-cascade optimization approach with saliency-based pseudo-feedback distillation. Overall, Dual-CES significantly outperforms all other state-of-the-art unsupervised alternatives. Dual-CES is even shown to be able to outperform strong supervised summarizers."
    },
    {
        "title": "Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs",
        "sections": [
            {
                "heading": null,
                "text": "\u2217Now at Hike Messenger 1The NSM baseline in this work is a re-implemented\nversion, as the original code was not available."
            },
            {
                "heading": "1 Introduction",
                "text": "Structured knowledge bases (KB) like Wikidata and Freebase can support answering questions (KBQA) over a diverse spectrum of structural complexity. This includes queries with single-hop (Obama\u2019s birthplace) (Yao, 2015; Berant et al., 2013), or multi-hop (who voiced Meg in Family Guy) (Bast and Hau\u00dfmann, 2015; Yih et al., 2015; Xu et al., 2016; Guu et al., 2015; McCallum et al., 2017; Das et al., 2017), or complex queries such as \u2018\u2018how many countries have more rivers and lakes than Brazil?\u2019\u2019 (Saha et al., 2018). Complex queries require a proper assembly of selected operators from a library of graph, set, logical, and arithmetic operations into a complex procedure, and is the subject of this paper.\nRelatively simple query classes, in particular, in which answers are KB entities, can be served with feed-forward (Yih et al., 2015) and seq2seq (McCallum et al., 2017; Das et al., 2017) networks. However, such systems show copying or rote learning behavior when Boolean or open numeric domains are involved. More complex queries need to be evaluated as an acyclic expression graph over nodes representing KB access, set, logical, and arithmetic operators (Andreas et al., 2016a). A practical alternative to inferring a stateless expression graph is to generate an imperative sequential program to solve the query. Each step of the program selects an atomic operator and a set of previously defined variables as arguments and writes the result to scratch memory, which can then be used in subsequent steps. Such imperative programs are preferable to opaque, monolithic networks for their interpretability and generalization to diverse domains. Another\n185\nTransactions of the Association for Computational Linguistics, vol. 7, pp. 185\u2013200, 2019. Action Editor: Scott Wen-tau Yih. Submission batch: 8/2018; Revision batch: 11/2018; Final submission: 1/2019; Published 4/2019.\nc\u00a9 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nmotivation behind opting for the program induction paradigm for solving complex tasks, such as complex question answering, is modularizing the end-to-end complex reasoning process. With this approach it is now possible to first train separate modules for each of the atomic operations involved and then train a program induction model that learns to use these separately trained models and invoke the sub-modules in the correct fashion to solve the task. These sub-modules can even be task-agnostic generic models that can be pretrained with much more extensive training data, while the program induction model learns from examples pertaining to the specific task. This paradigm of program induction has been used for decades, with rule induction and probabilistic program induction techniques in Lake et al. (2015) and by constructing algorithms utilizing formal theorem-proving techniques in Waldinger and Lee (1969). These traditional approaches (e.g., Muggleton and Raedt, 1994) incorporated domain specific knowledge about programming languages instead of applying learning techniques. More recently, to promote generalizability and reduce dependecy on domain specific knowledge, neural approaches have been applied to problems like addition, sorting, and word algebra problems (Reed and de Freitas, 2016; Bosnjak et al., 2017) as well as for manipulating a physical environment (Bunel et al., 2018).\nProgram Induction has also seen initial promise in translating simple natural language queries into programs executable in one or two hops over a KB to obtain answers (Liang et al., 2017). In contrast, many of the complex queries from Saha et al. (2018), such as the one in Figure 1, require up to 10-step programs involving multiple relations and several arithmetic and logical operations. Sample operations include gen\u2212set: collecting {t : (h, r, t) \u2208 KB}, computing set\u2212union, counting set sizes (set\u2212count), comparing numbers or sets, and so forth. These operations need to be executed in the correct order, with correct parameters, sharing information via intermediate results to arrive at the correct answer. Note also that the actual gold program is not available for supervision and therefore the large space of possible translation actions at each step, coupled with a large number of steps needed to get any payoff, makes the reward very sparse. This renders complex KBQA in the absence of gold programs extremely challenging.\nMain Contributions\n\u2022 We present \u2018\u2018Complex Imperative Program Induction from Terminal Rewards\u2019\u2019 (CIPITR),2\nan advanced Neural Program Induction (NPI) system that is able to answer complex logical, quantitative, and comparative queries by inducing programs of length up to 7, using 20 atomic operators and 9 variable types. This, to our knowledge, is the first NPI system to be trained with only the gold answer as (very distant) supervision for inducing such complex programs.\n\u2022 CIPITR reduces the combinatorial program space to only semantically correct programs by (i) incorporating symbolic constraints guided by KB schema and inferred answer type, and (ii) adopting pragmatic programming techniques by decomposing the final goal into a hierarchy of sub-goals, thereby mitigating the sparse reward problem by considering additional auxiliary rewards in a generic, task-independent way.\nWe evaluate CIPITR on the following two challenging tasks: (i) complex KBQA posed by the recently-published CSQA data set (Saha et al., 2018) and (ii) multi-hop KBQA in one of the more\n2The code and reinforcement learning environment of CIPITR is made public inhttps://github.com/CIPITR/ CIPITR.\npopularly used KBQA data sets WebQuestionsSP (Yih et al., 2016). WebQuestionsSP involves complex multi-hop inferencing, sometimes with additional constraints, as we will describe later. However, CSQA poses a much greater challenge, with its more diverse classes of complex queries and almost 20-times larger scale. On a data set such as CSQA, contemporary models like neural symbolic machines (NSM) fail to handle exponential growth of the program search space caused by a large number of operator choices at every step of a lengthy program. Key-value memory networks (KVMnet) (Miller et al., 2016) are also unable to perform the necessary complex multi-step inference. CIPITR outperforms them both by a significant margin while avoiding exploration of unwanted program space or memorization of low-entropy answer distributions. On even moderately complex programs of length 2\u20135, CIPITR scored at least 3\u00d7 higher F1 than both. On one of the hardest class of programs of around 5\u201310 steps (i.e., comparative reasoning), CIPITR outperformed NSM by a factor of 89 and KVMnet by a factor of 9. Further, we empirically observe that among all the competing models, CIPITR shows the best generalization across diverse program classes."
            },
            {
                "heading": "2 Related Work",
                "text": "Whereas most of the earlier efforts to handle complex KBQA did not involve writable memory, some recent systems (Miller et al., 2016; Neelakantan et al., 2015, 2016; Andreas et al., 2016b; Dong and Lapata, 2016) used end-toend differentiable neural networks. One of the state-of-the-art neural models for KBQA, the keyvalue memory network KVMnet (Miller et al., 2016) learns to answer questions by attending on the relevant KB subgraph stored in its memory. Neelakantan et al. (2016) and Pasupat and Liang (2015) support simple queries over tables, for example, of the form \u2018\u2018find the sum of a specified column\u2019\u2019 or \u2018\u2018list elements in a column more than a given value.\u2019\u2019 The query is read by a recurrent neural network (RNN), and then, in each translation step, the column and operator are selected using the query representation and history of operators and columns selected in the past. Andreas et al. (2016b) use a \u2018\u2018stateless\u2019\u2019 model where neural network based subroutines are assembled using syntactic parsing.\nRecently, Reed and de Freitas (2016) took an early influential step with the NPI compositional framework that learns to decompose high level tasks like addition and sorting into program steps (carry, comparison) aided by persistent memory. It is trained by high-level task input and output as well as all the program steps. Li et al. (2016) and Bosnjak et al. (2017) took another important step forward by replacing NPI\u2019s expensive strong supervision with supervision of the programsketch. This form of supervision at every intermediate step still keeps the problem simple, by arresting the program space to a tractable size. Although such data are easy to generate for simpler problems such as arithmetic and sorting, it is expensive for KBQA. Liang et al. (2017) proposed the NSM framework in absence of the gold program, which translates the KB query to a structured program token-by-token. While being a natural approach for program induction, NSM has several inherent limitations preventing generalization towards longer programs that are critical for complex KBQA. Subsequently, it was evaluated only on WebQuestionsSP (Yih et al., 2016), that requires relatively simpler programs. We consider NSM as the primary and KVMnet as an additional baseline and show that CIPITR significantly outperforms both, especially on the more complex query types."
            },
            {
                "heading": "3 Complex KBQA Problem Set-up",
                "text": ""
            },
            {
                "heading": "3.1 CSQA Data Set",
                "text": "The CSQA data set (Saha et al., 2018) contains 1.15M natural language questions and its corresponding gold answer from WikiData Knowledge Base. Figure 1 shows a sample query from the data set along with its true program-decomposed form, the latter not provided by CSQA. CSQA is particularly suited to study the Complex Program Induction (CPI) challenge over other KBQA data sets because:\n\u2022 It contains large-scale training data of question-answer pairs across diverse classes of complex queries, each requiring different inference tools over large KB sub-graphs.\n\u2022 Poor state-of-the-art performance of memory networks on it motivates the need for sweeping changes to the NPI\u2019s learning strategy.\n\u2022 The massive size of the KB involved (13 million entities and 50 million tuples) poses a scalability challenge for prior NPI techniques.\n\u2022 Availability of KB metadata helps standardize comparisons across techniques (explained subsequently).\nWe adapt CSQA in two ways for the CPI problem.\nRemoval of extended conversations: To be consistent with the NSM work on KBQA, we discard QA pairs that depend on the previous dialogue context. This is possible as every query is annotated with information on whether it is self-contained or depends on the previous context. Relevant statistics of the resulting data set are presented in Table 3.\nUse of gold entity, type, and relation annotations to standardize comparisons: Our focus being on the reasoning aspect of the KBQA problem, we use the gold annotations of canonical KB entities, types, and relations available in the data set along with the the queries, in order to remove a prominent source of confusion in comparing KBQA systems (i.e., all systems take as inputs the natural language query, with spans identified with KB IDs of entities, types, relations, and integers). Although annotation accuracy affects a complete KBQA system, our focus here is on complex, multi-step program generation with only final answer as the distant supervision, and not entity/type/relation linking."
            },
            {
                "heading": "3.2 WebQuestionsSP Data Set",
                "text": "In Figure 2 we illustrate one of the most complex questions from the the WebQuestionsSP data set and its semantic parsed version provided by human annotator. Questions in the WebQuestionsSP data set are answerable from the Freebase KB and tyically require up to 2-hop inference chains, sometimes with additional requirements of satisfying specific constraints. These constraints can be temporal (e.g., governing\u2212position\u2212held\u2212from) ornon-temporal (e.g., government\u2212office\u2212position\u2212 or\u2212title). The human-annotated semantic parse of the questions provide the exact structure of the subgraph and the inference process on it to reach the final answer. As in this work, we are focusing on inducing programs where the gold entity relation annotations are known; for this data set as well, we use the human-annotations to collect all\nthe entities and relations in the oracle subgraph associated with the query. The NPI model has to understand the role of these gold program inputs in question-answering and learn to induce a program to reflect the same inferencing."
            },
            {
                "heading": "4 Complex Imperative Program Induction from Terminal Rewards",
                "text": ""
            },
            {
                "heading": "4.1 Notation",
                "text": "This subsection introduces the different notations commonly used by our model.\nNine variable-types: (distinct from KB types)\n\u2022 KB artifacts: ent(entity), rel(relation), type\n\u2022 Base data types: int, bool, None (empty argument type used for padding)\n\u2022 Composite data types: set (i.e., set of KB entities) or map\u2212set and map\u2212int (i.e., a mapping function from an entity to a set of KB entities or an integer)\nTwenty Operators:\n\u2022 gen\u2212set(ent, rel, type) \u2192 set\n\u2022 verify(ent, rel, ent) \u2192 bool\n\u2022 gen\u2212map set(type, rel, type) \u2192 map\u2212set\n\u2022 map\u2212count(map\u2212set) \u2192 map\u2212int\n\u2022 set {union/ints/diff}(set, set) \u2192 set\n\u2022 map\u2212{union/ints/diff}(map\u2212set, map\u2212set) \u2192 map\u2212set\n\u2022 set\u2212count(set) \u2192 int\n\u2022 select\u2212{atleast/atmost/more/less/ equal/approx}(map int, int) \u2192 set\n\u2022 select\u2212{max/min}(map int) \u2192 ent\n\u2022 no\u2212op() (i.e., no action taken)\nSymbols and Hyperparameters: (typical values)\n\u2022 num\u2212op: Number of operators (20)\n\u2022 num\u2212var\u2212types: Number of variable types (9)\n\u2022 max\u2212var: Maximum number of variables accommodated in memory for each type (3)\n\u2022 m: Maximum number of arguments for an operator (None padding for fewer arguments) (3)\n\u2022 dkey & dval: Dimension of the key and value embeddings (dkey dval) (100, 300)\n\u2022 np & nv: Number of operators and argument variables sampled per operator each time (4, 10)\n\u2022 f with subscript: some feed-forward network\nEmbedding Matrices: The model is trained with a vocabulary of operators and variable-types. In order to sample operators, two matrices Mop key \u2208 Rnum op\u00d7dkey and Mop val \u2208 Rnum op\u00d7dval are needed for encoding the operator\u2019s key and value embedding. The key embedding is used for looking up and retrieving an entry from the operator vocabulary and the corresponding value embedding encodes the operator information. The variable type has only the value embedding Mvtype val \u2208 Rnum op\u00d7dval as no lookup is needed on it.\nOperator Prototype Matrices: These matrices store the argument variable type information for the m arguments of every operator in Mop arg \u2208 {0, 1, . . . , num\u2212var\u2212types}num op\u00d7m and the output variable type created by it in Mop out \u2208 {0, 1, . . . , num\u2212var\u2212types}num op.\nMemory Matrices: This is the query-specific scratch memory for storing new program variables as they get created by CIPITR. For each variable type, we have separate key and value embedding matrices Mvar key \u2208 Rnum var type\u00d7max var\u00d7dkey and Mvar val \u2208 Rnum var type\u00d7max var\u00d7dval , respectively for looking up a variable in memory\nand accessing the information in it. In addition, we also have a variable attention matrix Mvar att \u2208 R num var type\u00d7max var which stores the attention vector over the variables declared of each type. CIPITR consists of three components:\nThe preprocessor takes the input query and the KB and performs the task of entity, relation, and type linking which acts as input to the program induction. It also pre-populates the variable memory matrices with any entity, relation, type, or integer variable directly extracted from the query.\nThe programmer model takes as input the natural language question, the KB, and the pre-populated variable memory tables to generate a program (i.e., a sequence of operators invoked with past instantiated variables as their arguments and generating new variables in memory).\nThe interpreter executes the generated program with the help of the KB and scratch memory and outputs the system answer.\nDuring training, the predicted answer is compared with the gold to obtain a reward, which is sent back to CIPITR to update its model parameters through a REINFORCE (Williams, 1992) objective. In the current version of CIPITR, the preprocessor consults an oracle to link entities, types and relations in the query to the KB. This is to isolate the programming performance of CIPITR from the effect of imperfect linkage. Extending earlier studies (Karimi et al., 2012; Khalid et al., 2008) to investigate robustness of CIPITR to linkage errors may be of future interest."
            },
            {
                "heading": "4.2 Basic Memory Operations in CIPITR",
                "text": "We describe some of the foundational modules invoked by the rest of CIPITR.\nMemory Lookup: The memory lookup looks up scratch memory with a given probe, say x (of arbitrary dimension), and retrieves the memory entry having closest key embedding to x. It first passes x through a feed-forward layer to transform its dimension to key embedding dimensionx\u2212key. Then, by computing softmax over the matrix multiplication ofMx key and xkey, the distribution over the memory variables for lookup is obtained.\nxkey = f(x), xdist = softmax(Mx keyxkey)\nFeasibility Sampling: To restrict the search space to meaningful programs, CIPITR incorporates both high-level generic or task-specific constraints when sampling any action. The generic constraints can help it adopt more pragmatic programming styles like not repeating lines of code or avoiding syntactical errors. The task specific constraints ensure that the generated program is consistent as per the KB schema or on execution gives an answer of the desired variable type. To sample from the feasible subset using these constraints, the input sampling distribution, xdist, is elementwise transformed by a feasibility vector xfeas followed by a L1-normalization. Along with the transformed distribution, the top-k entries xsampled is also returned.\nAlgorithm 1 Feasibility Sampling Input:\n\u2022 xdist \u2208 RN (where N is the size of the population set over which lookup needs to be done)\n\u2022 xfeas \u2208 {0, 1}N (boolean feasibility vector)\n\u2022 k (top-k sampled)\nProcedure: FeasSampling (xdist, xfeas, k) xdist = xdist xfeas (elementwise multiply) xdist = L1-Normalized(xdist) xsampled = k-argmax(xdist) Output: xdist, xsampled\nWriting a new variable to memory: This operation takes a newly generated variable, say x, of type xtype and adds its key and value embedding\nto the row corresponding to xtype in the memory matrices. Further, it updates the attention vector forxtype to provide maximum weight to the newest variable generated, thus, emulating a stack like behavior.\nAlgorithm 2 Write a new variable to memory Input:\n\u2022 xkey, xval the key and value embedding of x\n\u2022 xtype is a scalar denoting type of variable x\nProcedure: WriteVarToMem(xkey, xval, xtype) i is the 1st empty slot in the row Mx key[xtype, :] Mvar key[xtype, i] = xkey\nMvar val[xtype, i] = xval Mvar att[xtype, :] = L1-Normalized(Mvar att[xtype, :] + One-Hot(i))"
            },
            {
                "heading": "4.3 CIPITR Architecture",
                "text": "In Figure 3, we sketch the CIPITR components; in this section we describe them in the order they appear in the model.\nQuery Encoder: The query is first parsed into a sequence of KB-entities and non-KB words. KB entities e are embedded with the concatenated vector [TransE(e),0] using Bordes et al. (2013), and non-KB words \u03c9 with [0,GloVe(\u03c9)]. The final query representation is obtained from a GRU encoder as q.\nNPI Core: The query representation q is fed at the initial timestep to an environment encoding\nRNN, which gives out the environment state et at every timestep. This, along with the value embedding uvalt\u22121 of the last output variable generated by the NPI engine, is fed at every timestep into another RNN that finally outputs the program state ht. ht is then fed into the successive modules of the program induction engine as described below. The \u2018OutVarGen\u2019 algorithm describes how to obtain uvalt\u22121.\nProcedure: NPI Core(et\u22121, ht\u22121, uvalt\u22121) et = GRU(et\u22121, u val t\u22121)\nht = GRU(et, u val t\u22121, ht\u22121)\nOutput: et, ht\nOperator Sampler: It takes the program state ht, a Boolean vector p feas t denoting operator feasibility, and the number of operators to sample np. It passes ht through the Lookup operation followed by Feasibility Sampling to obtain the top-np operations (Pt).\nArgument Variable Sampler: For each sampled operator p, it takes: (i) program state ht, (ii) the list of variable types V typep of the m arguments obtained by looking up the operator prototype matrix Mop arg, and (iii) a Boolean vector V feasp that indicates the valid variable configurations for the m-tuple arguments of the operator p. For each of them arguments, a feed-forward network fvtype first transforms the program state ht to a vector in R max var. It is then element-wise multiplied with the current attention state over the variables in memory of that type. This provides the programstate-specific attention over variables vattp,j which is then passed through the Lookup function to obtain the distribution over the variables in memory. Next, feasibility sampling is applied over the joint distribution of its argument variables, comprised of the m individual distributions. This provides the top-nv tuples of m-variable instantiations Vp.\nOutput Variable Generator: The new variable up of type u type p = Mop out[p] is generated by the procedure OutVarGen by invoking a sampled operator p with m variables vp,1 \u00b7 \u00b7 \u00b7 vp,m of type vtypep,1 \u00b7 \u00b7 \u00b7 v type p,m as arguments. This also requires generating the key and value embedding, which are both obtained by applying different feedforward layers over the concatenated representation of the value embedding of the operator Mop val[p], argument types (Mvtype val[vtypep,1 ] \u00b7 \u00b7 \u00b7\nMvtype val[vtypep,m ]) and the instantiated variables (Mvar val[vtypep,1 , vp,1] \u00b7 \u00b7 \u00b7 Mvar val[v type p,m , vp,m]). The newly generated variable is then written to memory using Algorithm WriteVarToMem.\nProcedure: ArgVarSampler(ht, V typep , V feasp , nv) forj \u2208 1, 2, \u00b7 \u00b7 \u00b7 ,m do\nvattp,j = softmax(M var att[V typep,j ]) fvtype(ht) vdistp,j = Lookup(vattp,j , fvar,Mvar key[V type p,j ])\nV distp = v dist p,0 \u00d7 vdistp,1 \u00b7 \u00b7 \u00b7 \u00d7 vdistp,m , Joint Distribution\nV distp , Vp = FeasSampling(V distp , V feasp , nv) Output: Vp\nEnd-to-End CIPITR training: CIPITR takes a natural language query and generates an output program in a number of steps. A program is composed of actions, which are operators applied over variables (as in Figure 3). In each step, it selects an operator and a set of previously defined variables as its arguments, and writes the operator output to a dynamic memory, to be subsequently used for further search of next actions. To reduce exposure bias (Ranzato et al., 2015), CIPITR uses a beam search to obtain multiple candidate programs to provide feedback to the model from a single training instance. Algorithm 3 shows the pseudocode of the program induction algorithm (with beam size b as 1 for simplicity), which goes over T time steps, each time sampling np feasible operators conditional to the program state. Then, for each of the np operators, it samples nv feasible\nAlgorithm 3 CIPITR pseudo-code (beam size=1) Query Encoding: q = GRU(Query) Initialization: e1, h1 = f(q), A = [ ]\nfor t \u2208 1, \u00b7 \u00b7 \u00b7 , T do pfeast = FeasibleOp() Pt = OperatorSampler(ht, pfeast , np) C = {} for p \u2208 Pt do\nV typep = [v type p,1 , \u00b7 \u00b7 \u00b7 , vtypep,m ] = Mop arg[p] V feasp = FeasibleVar(p) Vp = ArgVarSampler(ht, V typep , V feasp , nv) for V \u2208 Vp do\nC = C \u22c3 (p, V, V typep )\n(p, V, V typep ) = argmax(C) ukeyp , u val p , u type p = OutVarGen(p, V typep , V ) WriteVarToMem(ukeyp , uvalp , utypep ) et+1, ht+1 = NPICore(et, ht) A.append((p, V ))\nOutput: A\nvariable instantiations, resulting in a total of np \u2217 nv candidates out of which b most-likely actions are sampled for the b beams and the corresponding newly generated variables written into memory. This way the algorithm progresses to finally output b candidate programs, each of which will feed the model back with some reward. Finally, in order to learn from the discrete action samples, the REINFORCE objective (Williams, 1992) is used. Because of lack of space, we do not provide the equation for REINFORCE, but our objective formulation remains very similar to that in Liang et al. (2017). We next describe several learning challenges that arise in the context of this overall architecture."
            },
            {
                "heading": "5 Mitigating Large Program Space and Sparse Reward",
                "text": "Handling complex queries by expanding the operator set and generating longer programs blows up the program space to a huge size of (num\u2212op \u2217 (max\u2212var)\nm)T . This, in absence of gold programs, poses serious training challenges for the programmer. Additionally, whereas the relatively simple NSM architecture could explore a large beam size (50\u2013100), the complex architecture of CIPITR entailed by the CPI problem could only afford to operate with a smaller beam size (\u2264 20), which further exacerbates the sparsity of the reward space. For example, for integer answers, only a single point in the integer space returns a positive reward, without any notion of partial reward. Such a delayed\u2014indeed, terminal\u2014 reward causes high variance, instability, and local minima issues. A problem as complex as ours requires not only generic constraints for producing semantically correct programs, but also incorporation of prior knowledge, if the model permits. We now describe how to guide CIPITR more efficiently through such a challenging environment using both generic and task-specific constraints.\nPhase change network: For complex real-word problems, the reinforcement learning community has proposed various task-abstractions (Parr and Russell, 1998; Dietterich, 2000; Bakker and Schmidhuber, 2004; Barto and Mahadevan, 2003; Sutton et al., 1999) to address the curse of dimensionality in exponential action spaces. HAMs, proposed by Parr and Russell (1998), is one such important form of abstraction aimed at restricting\nthe realizable action sequences. Inspired by HAMs, we decompose the program synthesis into phases having restricted action spaces. The first phase (retrieval phase) constitutes gathering the information from the preprocessed input variables only (i.e., KB entities, relations, types, integers). This restricts the feasible operator set to gen\u2212set, gen\u2212map\u2212set, and verify. In the second phase (algorithm phase) the model is allowed to operate on all the generated variables in order to reach the answer. The programmer learns whether to switch from the first phase to the second at any timestep t, based on parameter \u03c6t (\u03c6t=1 indicating\nchange of phase, where \u03c60 = 0) which is obtained as \u03c6t = 1{max(sigmoid(f(ht)), \u03c6t\u22121) \u2265 \u03c6thresh} if t < T/2, else 1 (T being total timesteps and \u03c6thresh is set to 0.8 in our experiments). The motivation behind this is similar to the multi-staged techniques that have been adopted in order to make QA tasks more tractable, as in Yih et al. (2015) and Iyyer et al. (2017). In contrast, here we further allow the model to learn when to switch from one stage to the next. Note that this is a generic characteristic, as for every task, this kind of phase division is possible.\nGeneratingsemanticallycorrectprograms: Other than the generic syntactical and semantic rules, the NPI paradigm also allows us to leverage prior knowledge and to incorporate task-specific symbolic constraints in the program representation learning in an end-to-end differentiable way.\n\u2022 Enforcing KB consistency: Operators used in the retrieval phase (described above) must honor the KB-imposed constraints, so as not to initialize variables that are inconsistent with respect to the KB. For example, a set variable assigned from gen\u2212set is considered valid only when the ent, rel, type arguments to gen\u2212set are consistent with the KB.\n\u2022 Biasing the last operator using answer type predictor: Answer type prediction is a standard preprocessing step in question answering (Li and Roth, 2002). For this we use a rule-based predictor that has 98% accuracy. The predicted answer type helps in directing the program search toward the correct answer type by biasing the sampling towards feasible operators that can produce the desired answer type.\n\u2022 Auxiliary reward strategy: Jaccard scores of the executed program\u2019s output and the gold answer set is used as reward. An invalid program gets a reward of \u22121. Further, to mitigate the sparsity of the extrinsic rewards, an additional auxiliary feedback is designed to reward the model on generating an answer of the predicted answer-type. A linear decay makes the effect of auxiliary reward vanish eventually. Such a curriculum learning mechanism, while being particularly useful for the more complex queries, is still\nquite generic as it does not require any additional task-specific prior knowledge.\nBeam Management and Action Sampling\n\u2022 Pruning beams by target answer type: Penalize beams that terminate with an answer type not matching the predicted answer type.\n\u2022 Length-based normalization of beam scores: To counteract the characteristic of beam search favoring shorter beams as more probable and to ensure the scoring is fair to the longer beams, we normalize the beam scores with respect to their length.\n\u2022 Penalizing beams for no\u2212op operators: Another way of biasing the beams toward generating longer sequences, is by penalizing for the number of times a beam takes no\u2212op as the action. Specifically, we reduce the beam score by a hyperparameter-controlled logarithmic factor of the number of no\u2212op actions taken till now.\n\u2022 Stochastic beam exploration with entropy annealing: To avoid early local minima where the model severely biases towards specific actions, we added techniques like (i) a stochastic version of beam search to sample operators in an -greedy fashion (ii) dropout, and (iii) entropy-based regularization of action distribution.\nSampling only feasible actions: Sampling a feasible action requires first sampling a feasible operator and then its feasible variable arguments:\n\u2022 The operator must be allowed in the current phase of the model\u2019s program induction.\n\u2022 Valid Variable instantiation: A feasible operator should be having at least one valid instantiation of its formal arguments with non-empty variable values that are also consistent with the KB.\n\u2022 Action Repetition: An action (i.e., an operator invoked with a specific argument instantiation) should not be repeated at any time step.\n\u2022 Some operators disallow some arguments; for example, union or intersection of a set with itself."
            },
            {
                "heading": "6 Experiments",
                "text": "We compare CIPITR against baselines (Miller et al., 2016; Liang et al., 2017) on complex KBQA and further identify the contributions of the ideas presented in Section 5 via ablation studies. For this work, we limit our effort on KBQA to the setting where the query is annotated with the gold KB-artifacts, which standardizes the input to the program induction for the competing models."
            },
            {
                "heading": "6.1 Hyperparameters Settings",
                "text": "We trained our model using the Adam Optimizer and tuned all hyperparameters on the validation set. Some parameters are selectively turned on/ off after few training iterations, which is itself a hyperparameter (see Table 1). We combined reward/ loss such as entropy annealing and auxiliary rewards using different weights detailed in Table 1. The key, value embedding dimensions are set to 100, 300."
            },
            {
                "heading": "6.2 WebQuestionsSP Data Set",
                "text": "We first evaluate our model on the more popularly used WebQuestionsSP data set."
            },
            {
                "heading": "6.2.1 Rule-Based Model on WebQuestionsSP",
                "text": "Though quite a few recent works on KBQA have evaluated their model on WebQuestionsSP, the reported performance is always in a setting where the gold entities/relations are not known. They either internally handle the entity and relationlinking problem or outsource it to some external or in-house model, which itself might have been trained with additional data. Additionally, the entity/relation linker outputs used by these models are also not made public, making it difficult to set up a fair ground for evaluating the program induction model, especially because we are interested in the program induction given the program inputs\nand handling the entity/relation linking is beyond the scope of this work. To avoid these issues, we use the human-annotated entity/relation linking data available along with the questions as input to the program induction model. Consequently the performance reported here is not comparable to the previous works evaluated on this data set, as the query annotation is obtained here from an oracle linker.\nFurther, to gauge the proficiency of the proposed program induction model, we construct a rule-based model which is aware of the human annotated semantic parsed form of the query\u2014that is, the inference chain of relations and the exact constraints that need to be additionally applied to reach the answer. The pseudocode below elaborates how the rule based model works on the human-annotated parse of the given query, taking as input the central entity, the inference chain, and associated constraints and their type. This\nProcedure: RuleBasedModel(parse,KB) ent1 \u2190 parse[\u2018TopicEntityMid\u2019] rel1 \u2190 parse[\u2018InferentialChain\u2019][0] ans \u2190 {x | (ent1, rel1, x) \u2208 KB} for c \u2208 parse[\u2018Constraints\u2019] c\u2212rel \u2190 c[\u2018NodePredicate\u2019] c\u2212op \u2190 c[\u2018Operator\u2019] c\u2212arg \u2190 c[\u2018Argument\u2019] if c[\u2018ArgumentType\u2019] == \u2018Entity\u2019 ans \u2190 ans \u2229 {x | (c\u2212arg, c\u2212rel, x) \u2208 KB}\nelse ans \u2190 \u22c3\nx\u2208ans {x | (x, c rel, y) \u2208 KB,\nc\u2212arg c\u2212op y} if len(parse[\u2018InferentialChain\u2019]) > 1 rel2 \u2190 parse[\u2018InferentialChain\u2019][1] ans \u2190 \u22c3\nx\u2208ans {y | (x, rel2, y) \u2208 KB}\nOutput: ans\ninference rule, manually derived, can be written out in a program form, which on execution will give the final answer. On the other hand, the task of CIPITR is to actually learn the program by looking at training examples of the query and corresponding answer. Both the models need to induce the program using the gold entity/relation data. Subsequently, the rule-based model is indeed a very strong competitor as it is generated by annotators having detailed knowledge about the KB."
            },
            {
                "heading": "6.2.2 Results on WebQuestionsSP",
                "text": "A comparative performance analysis of the proposed CIPITR model, the rule-based model and the SparQL executor is tabulated in Table 2. The main take-away from these results is that CIPITR is indeed able to learn the rules behind the multi-step inference process simply from the distance supervision provided by the questionanswer pairs and even perform slightly better in some of the query classes."
            },
            {
                "heading": "6.3 CSQA Data Set",
                "text": "We now showcase the performance of the proposed models and related baselines on the CSQA data set."
            },
            {
                "heading": "6.3.1 Baselines on CSQA",
                "text": "KVMnet with decoder (2016), which performed best on CSQA data set (Saha et al., 2018) (as discussed in Section 2), learns to attend on a KB subgraph in memory and decode the attention over memory-entries as their likelihood of being in the answer. Further, it can also decode a vocabulary of non-KB words like integers or booleans. However, because of the inherent architectural constraints, it is not possible to incorporate most of the symbolic constraints presented in Section 5 in this model, other than KB-guided consistency\nand biasing towards answer-type. More importantly, recently the usage of these models have been criticized for numerical and boolean question answering as these deep networks can easily memorize answers without \u2018\u2018understanding\u2019\u2019 the logic behind the queries simply because of the skew in the answer distribution. In our case this effect is more pronounced as CSQA evinces a curious skew in integer answers to \u2018\u2018count\u2019\u2019 queries. Fifty-six percent of training and 52% of test count-queries have single digit answers. Ninety percent of training and 81% of test count-queries have answers less than 200. Though this makes it unfair to compare NPI models (that are oblivious to the answer vocabulary) with KVMnet on such queries, we still train a KVMnet version on a balanced resample of CSQA, where, for only the count queries, the answer distribution over integers has been made uniform.\nNSM (2017) uses a key-variable memory and decodes the program as a sequence of operators and memory variables. As the NSM code was not available, we implemented it and further incorporated most of the six techniques presented in Table 4. However, constraints like action repetition, biasing last operator selection, and phase change cannot be incorporated in NSM while keeping the model generic, as it decodes the program token by token."
            },
            {
                "heading": "6.3.2 Results on CSQA",
                "text": "In Table 3 we compare the F1 scores obtained by our system, CIPITR, against the KVMnet and NSM baselines. For NSM and CIPITR, we train seven models with different hyperparameters tuned on each of the seven question types. For the train and valid splits, a rule-based query type classifier with 97% accuracy was used to bucket queries into the classes listed in Table 3. For each of these three systems, we also train and evaluate\none single model over all question types. KVMnet does not have any beam search, the NSM model uses a beam size of 50, and CIPITR uses only 20 beams for exploring the program space.\nOur manual inspection of these seven query categories show that simple and verify are simplest in nature requiring 1-line programs while logical is moderately difficult, with around 3 lines of code. The query categories next in order of complexity are quantitative and quantitative count, needing a sequence of 2\u20135 operations. The hardest types are comparative and comparative count, which translate to an average of 5\u201310 lined programs.\nAnalysis: The experiments show that on the simple to moderately difficult (i.e., first three) query classes, CIPITR\u2019s performance at the top beam is up to 3 times better than both the baselines. The superiority of CIPITR over NSM is showcased better on the more complex classes where it outperforms the latter by 5\u201310 times, with the biggest impact (by a factor of 89 times) being on the \u2018\u2018comparative\u2019\u2019 questions. Also, the 5\u00d7 better performance of CIPITR over NSM over All category evinces the better generalizability of the abstract high-level program decomposition approach of the former.\nOn the other hand, training the KVMnet model on the balanced data helps showcase the real performance of the model, where CIPITR outperforms KVMnet significantly on most of the harder query classes. The only exception is the hardest class (Comp, Count with numerical answers) where the abrupt \u2018\u2018best performance\u2019\u2019 of KVMnet can be attributed to its rote learning\nabilities simply because of its knowledge of the answer vocabulary, which the program induction models are oblivious to, as they never see the actual answer.\nLastly, in our experimental configurations, whereas CIPITR and NSM\u2019s parameter-size is almost comparable, KVMnet\u2019s is approximately 6\u00d7 larger.\nAblation Study: To quantitatively analyze the utility of the features mentioned in Section 5, we experiment with various ablations in Table 4 by turning off each feature, one at a time. We show the effect on the hardest question category (\u2018\u2018comparative\u2019\u2019) on which our proposed model achieved reasonable performance. We see in the table that each of the 6 techniques helped the model significantly. Some of them boosted F1 by 1.5\u20134 times, while others proved to be instrumental to obtained large improvements in F1 score of over 6\u20139 times.\nTo summarize, CIPITR has the following advantages, inducing programs more efficiently\nand pragmatically, as illustrated by the sample outputs in Table 5:\n\u2022 Generating syntactically correct programs: Because of the token-by-token decoding of the program, NSM cannot restrict its search to only syntactically correct programs, but rather only resorts to a post-filtering step during training. However, at test time, it could still generate programs with wrong syntax, as shown in Table 5. For example, for the Logical question, it invokes a gen\u2212set with a wrong argument type None and for the Quantitative count question, it invokes the set\u2212union operator on a non-set argument. On the other hand, CIPITR, by design, can never generate a syntactically incorrect program because at every step it implicitly samples only feasible actions.\n\u2022 Generating semantically correct programs: CIPITR is capable of incorporating different generic programming styles as well as problemspecific constraints, restricting its search space to only semantically correct programs. As shown in Table 5, CIPITR is able to generate at least meaningful programs having the desired answer-type or without repeating lines of code. On the other hand the NSMgenerated programs are often semantically wrong, for instance, both in the Quantitative and Quantitative Count based questions, the\ntype of the answer is itself wrong, rendering the program meaningless. This arises once again, owing to the token-by-token decoding of the program by NSM which makes it hard to incorporate high level rules to guide or constrain the search.\n\u2022 Efficient search-space exploration: Owing to the different strategies used to explore the program space more intelligently, CIPITR scales better to a wide variety of complex queries by using less than half of NSM\u2019s beam size. We experimentally established that for programs of length 7 these various techniques reduced the average program space from 1.33\u00d7 1019 to 2,998 programs."
            },
            {
                "heading": "7 Conclusion",
                "text": "We presented CIPITR, an advanced NPI framework that significantly pushes the frontier of complex program induction in absence of gold programs. CIPITR uses auxiliary rewarding techniques to mitigate the extreme reward sparsity and incorporates generic pragmatic programming styles to constrain the combinatorial program space to only semantically correct programs. As future directions of work, CIPITR can be further improved to handle the hardest question types by making the search more strategic, and can be further generalized to a diverse set of goals when training on all question categories together.\nOther potential directions of research could be toward learning to discover sub-goals to further decompose the most complex classes beyond just the two-level phase transition proposed here. Additionally, further improvements are required to induce complex programs without availability of gold program input variables."
            }
        ],
        "references": [
            {
                "title": "Learning to compose neural networks for question answering",
                "author": [
                    "Jacob Andreas",
                    "Marcus Rohrbach",
                    "Trevor Darrell",
                    "Dan Klein."
                ],
                "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for",
                "citeRegEx": "Andreas et al\\.,? 2016a",
                "shortCiteRegEx": "Andreas et al\\.",
                "year": 2016
            },
            {
                "title": "Learning to compose neural networks for question answering",
                "author": [
                    "Jacob Andreas",
                    "Marcus Rohrbach",
                    "Trevor Darrell",
                    "Dan Klein."
                ],
                "venue": "NAACL-HLT , pages 1545\u20131554.",
                "citeRegEx": "Andreas et al\\.,? 2016b",
                "shortCiteRegEx": "Andreas et al\\.",
                "year": 2016
            },
            {
                "title": "Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization",
                "author": [
                    "B. Bakker",
                    "J. Schmidhuber."
                ],
                "venue": "Proceedings of the 8th Conference on Intelligent Autonomous Systems IAS-8, pages 438\u2013445.",
                "citeRegEx": "Bakker and Schmidhuber.,? 2004",
                "shortCiteRegEx": "Bakker and Schmidhuber.",
                "year": 2004
            },
            {
                "title": "Recent advances in hierarchical reinforcement learning",
                "author": [
                    "Andrew G. Barto",
                    "Sridhar Mahadevan."
                ],
                "venue": "Discrete Event Dynamic Systems, 13(1-2):41\u201377.",
                "citeRegEx": "Barto and Mahadevan.,? 2003",
                "shortCiteRegEx": "Barto and Mahadevan.",
                "year": 2003
            },
            {
                "title": "More accurate question answering on freebase",
                "author": [
                    "Hannah Bast",
                    "Elmar Hau\u00dfmann."
                ],
                "venue": "CIKM, pages 1431\u20131440.",
                "citeRegEx": "Bast and Hau\u00dfmann.,? 2015",
                "shortCiteRegEx": "Bast and Hau\u00dfmann.",
                "year": 2015
            },
            {
                "title": "Semantic parsing on Freebase from question-answer pairs",
                "author": [
                    "J. Berant",
                    "A. Chou",
                    "R. Frostig",
                    "P. Liang."
                ],
                "venue": "EMNLP Conference, pages 1533\u20131544.",
                "citeRegEx": "Berant et al\\.,? 2013",
                "shortCiteRegEx": "Berant et al\\.",
                "year": 2013
            },
            {
                "title": "Translating embeddings for modeling multi-relational data",
                "author": [
                    "Antoine Bordes",
                    "Nicolas Usunier",
                    "Alberto GarciaDuran",
                    "Jason Weston",
                    "Oksana Yakhnenko."
                ],
                "venue": "NIPS Conference, pages 2787\u20132795.",
                "citeRegEx": "Bordes et al\\.,? 2013",
                "shortCiteRegEx": "Bordes et al\\.",
                "year": 2013
            },
            {
                "title": "Programming with a differentiable forth interpreter",
                "author": [
                    "Matko Bosnjak",
                    "Tim Rockt\u00e4schel",
                    "Jason Naradowsky",
                    "Sebastian Riedel."
                ],
                "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017,",
                "citeRegEx": "Bosnjak et al\\.,? 2017",
                "shortCiteRegEx": "Bosnjak et al\\.",
                "year": 2017
            },
            {
                "title": "Leveraging grammar and reinforcement learning for neural program synthesis",
                "author": [
                    "Rudy Bunel",
                    "Matthew J. Hausknecht",
                    "Jacob Devlin",
                    "Rishabh Singh",
                    "Pushmeet Kohli."
                ],
                "venue": "International Conference on Learning Representa-",
                "citeRegEx": "Bunel et al\\.,? 2018",
                "shortCiteRegEx": "Bunel et al\\.",
                "year": 2018
            },
            {
                "title": "Question answering on knowledge bases and text using universal schema and memory networks",
                "author": [
                    "Rajarshi Das",
                    "Manzil Zaheer",
                    "Siva Reddy",
                    "Andrew McCallum."
                ],
                "venue": "ACL (2), pages 358\u2013365.",
                "citeRegEx": "Das et al\\.,? 2017",
                "shortCiteRegEx": "Das et al\\.",
                "year": 2017
            },
            {
                "title": "Feudal reinforcement learning",
                "author": [
                    "Peter Dayan",
                    "Geoffrey E. Hinton."
                ],
                "venue": "Advances in Neural Information Processing Systems 5,",
                "citeRegEx": "Dayan and Hinton.,? 1993",
                "shortCiteRegEx": "Dayan and Hinton.",
                "year": 1993
            },
            {
                "title": "Hierarchical reinforcement learning with the maxq value function decomposition",
                "author": [
                    "Thomas G. Dietterich."
                ],
                "venue": "Journal of Artificial Intelligence Research, 13(1):227\u2013303.",
                "citeRegEx": "Dietterich.,? 2000",
                "shortCiteRegEx": "Dietterich.",
                "year": 2000
            },
            {
                "title": "Language to logical form with neural attention",
                "author": [
                    "Li Dong",
                    "Mirella Lapata."
                ],
                "venue": "ACL, volume 1, pages 33\u201343.",
                "citeRegEx": "Dong and Lapata.,? 2016",
                "shortCiteRegEx": "Dong and Lapata.",
                "year": 2016
            },
            {
                "title": "Traversing knowledge graphs in vector space",
                "author": [
                    "Kelvin Guu",
                    "John Miller",
                    "Percy Liang."
                ],
                "venue": "EMNLP Conference.",
                "citeRegEx": "Guu et al\\.,? 2015",
                "shortCiteRegEx": "Guu et al\\.",
                "year": 2015
            },
            {
                "title": "Search-based neural structured learning for sequential question answering",
                "author": [
                    "Mohit Iyyer",
                    "Wen-tau Yih",
                    "Ming-Wei Chang."
                ],
                "venue": "ACL, volume 1, pages 1821\u20131831.",
                "citeRegEx": "Iyyer et al\\.,? 2017",
                "shortCiteRegEx": "Iyyer et al\\.",
                "year": 2017
            },
            {
                "title": "Quantifying the impact of concept recognition on biomedical information retrieval",
                "author": [
                    "Sarvnaz Karimi",
                    "Justin Zobel",
                    "Falk Scholer."
                ],
                "venue": "Information Processing & Management, 48(1): 94\u2013106.",
                "citeRegEx": "Karimi et al\\.,? 2012",
                "shortCiteRegEx": "Karimi et al\\.",
                "year": 2012
            },
            {
                "title": "The impact of named entity normalization on information retrieval for question answering",
                "author": [
                    "Mahboob Alam Khalid",
                    "Valentin Jijkoun",
                    "Maarten De Rijke."
                ],
                "venue": "Proceedings of the IR Research, 30th European Conference on",
                "citeRegEx": "Khalid et al\\.,? 2008",
                "shortCiteRegEx": "Khalid et al\\.",
                "year": 2008
            },
            {
                "title": "Human-level concept learning through probabilistic program induction",
                "author": [
                    "Brenden M. Lake",
                    "Ruslan Salakhutdinov",
                    "Joshua B. Tenenbaum."
                ],
                "venue": "Science, 350(6266):1332\u20131338.",
                "citeRegEx": "Lake et al\\.,? 2015",
                "shortCiteRegEx": "Lake et al\\.",
                "year": 2015
            },
            {
                "title": "Neural program lattices",
                "author": [
                    "Chengtao Li",
                    "Daniel Tarlow",
                    "Alexander L. Gaunt",
                    "Marc Brockschmidt",
                    "Nate Kushman."
                ],
                "venue": "International Conference on Learning Representations (ICLR).",
                "citeRegEx": "Li et al\\.,? 2016",
                "shortCiteRegEx": "Li et al\\.",
                "year": 2016
            },
            {
                "title": "Learning question classifiers",
                "author": [
                    "X. Li",
                    "D. Roth."
                ],
                "venue": "COLING, pages 556\u2013562.",
                "citeRegEx": "Li and Roth.,? 2002",
                "shortCiteRegEx": "Li and Roth.",
                "year": 2002
            },
            {
                "title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision",
                "author": [
                    "Chen Liang",
                    "Jonathan Berant",
                    "Quoc Le",
                    "Kenneth D. Forbus",
                    "Ni Lao."
                ],
                "venue": "Proceedings of the 55th Annual Meeting of the Association for",
                "citeRegEx": "Liang et al\\.,? 2017",
                "shortCiteRegEx": "Liang et al\\.",
                "year": 2017
            },
            {
                "title": "Chains of reasoning over entities, relations, and text using recurrent neural networks",
                "author": [
                    "Andrew McCallum",
                    "Arvind Neelakantan",
                    "Rajarshi Das",
                    "David Belanger."
                ],
                "venue": "Proceedings of the 15th Conference of the European",
                "citeRegEx": "McCallum et al\\.,? 2017",
                "shortCiteRegEx": "McCallum et al\\.",
                "year": 2017
            },
            {
                "title": "Key-value memory networks for directly reading documents",
                "author": [
                    "Alexander H. Miller",
                    "Adam Fisch",
                    "Jesse Dodge",
                    "Amir-Hossein Karimi",
                    "Antoine Bordes",
                    "Jason Weston."
                ],
                "venue": "EMNLP, pages 1400\u20131409.",
                "citeRegEx": "Miller et al\\.,? 2016",
                "shortCiteRegEx": "Miller et al\\.",
                "year": 2016
            },
            {
                "title": "Inductive logic programming: Theory and methods",
                "author": [
                    "Stephen Muggleton",
                    "Luc De Raedt."
                ],
                "venue": "Journal of Logic Programming, 19/20: 629\u2013679.",
                "citeRegEx": "Muggleton and Raedt.,? 1994",
                "shortCiteRegEx": "Muggleton and Raedt.",
                "year": 1994
            },
            {
                "title": "Learning a natural language interface with neural programmer",
                "author": [
                    "Arvind Neelakantan",
                    "Quoc V. Le",
                    "Martin Abadi",
                    "Andrew McCallum",
                    "Dario Amodei."
                ],
                "venue": "arXiv preprint, arXiv: 1611.08945.",
                "citeRegEx": "Neelakantan et al\\.,? 2016",
                "shortCiteRegEx": "Neelakantan et al\\.",
                "year": 2016
            },
            {
                "title": "Neural programmer: Inducing latent programs with gradient descent",
                "author": [
                    "Arvind Neelakantan",
                    "Quoc V. Le",
                    "Ilya Sutskever."
                ],
                "venue": "CoRR, abs/1511.04834.",
                "citeRegEx": "Neelakantan et al\\.,? 2015",
                "shortCiteRegEx": "Neelakantan et al\\.",
                "year": 2015
            },
            {
                "title": "Reinforcement learning with hierarchies of machines",
                "author": [
                    "Ronald Parr",
                    "Stuart Russell."
                ],
                "venue": "Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems 10, NIPS \u201997, pages 1043\u20131049.",
                "citeRegEx": "Parr and Russell.,? 1998",
                "shortCiteRegEx": "Parr and Russell.",
                "year": 1998
            },
            {
                "title": "Compositional semantic parsing on semi-structured tables",
                "author": [
                    "Panupong Pasupat",
                    "Percy Liang."
                ],
                "venue": "arXiv preprint arXiv:1508.00305.",
                "citeRegEx": "Pasupat and Liang.,? 2015",
                "shortCiteRegEx": "Pasupat and Liang.",
                "year": 2015
            },
            {
                "title": "Sequence level training with recurrent neural networks. CoRR, abs/1511.06732",
                "author": [
                    "Marc\u2019Aurelio Ranzato",
                    "Sumit Chopra",
                    "Michael Auli",
                    "Wojciech Zaremba"
                ],
                "venue": null,
                "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E",
                "shortCiteRegEx": "Ranzato et al\\.",
                "year": 2015
            },
            {
                "title": "Neural programmer-interpreters",
                "author": [
                    "Scott Reed",
                    "Nando de Freitas."
                ],
                "venue": "International Conference on Learning Representations (ICLR).",
                "citeRegEx": "Reed and Freitas.,? 2016",
                "shortCiteRegEx": "Reed and Freitas.",
                "year": 2016
            },
            {
                "title": "Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph",
                "author": [
                    "Amrita Saha",
                    "Vardaan Pahuja",
                    "Mitesh M. Khapra",
                    "Karthik Sankaranarayanan",
                    "Sarath Chandar"
                ],
                "venue": null,
                "citeRegEx": "Saha et al\\.,? \\Q2018\\E",
                "shortCiteRegEx": "Saha et al\\.",
                "year": 2018
            },
            {
                "title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
                "author": [
                    "Richard S. Sutton",
                    "Doina Precup",
                    "Satinder Singh."
                ],
                "venue": "Artificial Intelligence, 112(1-2):181\u2013211.",
                "citeRegEx": "Sutton et al\\.,? 1999",
                "shortCiteRegEx": "Sutton et al\\.",
                "year": 1999
            },
            {
                "title": "Semantic parsing using content and context: A case study from requirements elicitation",
                "author": [
                    "Reut Tsarfaty",
                    "Ilia Pogrebezky",
                    "Guy Weiss",
                    "Yaarit Natan",
                    "Smadar Szekely",
                    "David Harel."
                ],
                "venue": "Proceedings of the 2014 Conference on",
                "citeRegEx": "Tsarfaty et al\\.,? 2014",
                "shortCiteRegEx": "Tsarfaty et al\\.",
                "year": 2014
            },
            {
                "title": "PROW: A step toward automatic program writing",
                "author": [
                    "Richard J. Waldinger",
                    "Richard C.T. Lee."
                ],
                "venue": "Proceedings of the 1st International Joint Conference on Artificial Intelligence, pages 241\u2013252.",
                "citeRegEx": "Waldinger and Lee.,? 1969",
                "shortCiteRegEx": "Waldinger and Lee.",
                "year": 1969
            },
            {
                "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
                "author": [
                    "Ronald J Williams."
                ],
                "venue": "Reinforcement Learning, Springer, pages 5\u201332.",
                "citeRegEx": "Williams.,? 1992",
                "shortCiteRegEx": "Williams.",
                "year": 1992
            },
            {
                "title": "Question answering on Freebase via relation extraction and textual evidence",
                "author": [
                    "Kun Xu",
                    "Siva Reddy",
                    "Yansong Feng",
                    "Songfang Huang",
                    "Dongyan Zhao."
                ],
                "venue": "arXiv preprint, arXiv: 1603.00957.",
                "citeRegEx": "Xu et al\\.,? 2016",
                "shortCiteRegEx": "Xu et al\\.",
                "year": 2016
            },
            {
                "title": "Lean question answering over Freebase from scratch",
                "author": [
                    "Xuchen Yao."
                ],
                "venue": "NAACL Conference, pages 66\u201370.",
                "citeRegEx": "Yao.,? 2015",
                "shortCiteRegEx": "Yao.",
                "year": 2015
            },
            {
                "title": "Semantic parsing via staged query graph generation: Question",
                "author": [
                    "Scott Wen-tau Yih",
                    "Ming-Wei Chang",
                    "Xiaodong He",
                    "Jianfeng Gao"
                ],
                "venue": null,
                "citeRegEx": "Yih et al\\.,? \\Q2015\\E",
                "shortCiteRegEx": "Yih et al\\.",
                "year": 2015
            }
        ],
        "abstractText": "Recent years have seen increasingly complex question-answering on knowledge bases (KBQA) involving logical, quantitative, and comparative reasoning over KB subgraphs. Neural Program Induction (NPI) is a pragmatic approach toward modularizing the reasoning process by translating a complex natural language query into a multi-step executable program. While NPI has been commonly trained with the \u2018\u2018gold\u2019\u2019 program or its sketch, for realistic KBQA applications such gold programs are expensive to obtain. There, practically only natural language queries and the corresponding answers can be provided for training. The resulting combinatorial explosion in program space, along with extremely sparse rewards, makes NPI for KBQA ambitious and challenging. We present Complex Imperative Program Induction from Terminal Rewards (CIPITR), an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, KB schema, and inferred answer type. CIPITR solves complex KBQA considerably more accurately than key-value memory networks and neural symbolic machines (NSM). For moderately complex queries requiring 2to 5-step programs, CIPITR scores at least 3\u00d7 higher F1 than the competing systems. On one of the hardest class of programs (comparative reasoning) with 5\u201310 steps, CIPITR outperforms NSM by a factor of 89 and memory networks by 9 times.1 \u2217Now at Hike Messenger 1The NSM baseline in this work is a re-implemented version, as the original code was not available."
    },
    {
        "title": "Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts",
        "sections": [
            {
                "heading": "1 Introduction",
                "text": "Grounding of textual phrases, i.e., finding bounding boxes in images which relate to textual phrases, is an important problem for human-computer interaction, robotics and mining of knowledge bases, three applications that are of increasing importance when considering autonomous systems, augmented and virtual reality environments. For example, we may want to guide an autonomous system by using phrases such as \u2018the bottle on your left,\u2019 or \u2018the plate in the top shelf.\u2019 While those phrases are easy to interpret for a human, they pose significant challenges for present day textual grounding algorithms, as interpretation of those phrases requires an understanding of objects and their relations.\nExisting approaches for textual grounding, such as [38, 15] take advantage of the cognitive performance improvements obtained from deep net features. More specifically, deep net models are designed to extract features from given bounding boxes and textual data, which are then compared to measure their fitness. To obtain suitable bounding boxes, many of the textual grounding frameworks, such as [38, 15], make use of region proposals. While being easy to obtain, automatic extraction of region proposals is limiting, because the performance of the visual grounding is inherently constrained by the quality of the proposal generation procedure.\nIn this work we describe an interpretable mechanism which additionally alleviates any issues arising due to a limited number of region proposals. Our approach is based on a number of \u2018image concepts\u2019 such as semantic segmentations, detections and priors for any number of objects of interest. Based on those \u2018image concepts\u2019 which are represented as score maps, we formulate textual grounding as a search over all possible bounding boxes. We find the bounding box with highest accumulated score contained in its interior. The search for this box can be solved via an efficient branch and bound\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nA woman in a green shirt is getting ready to throw her bowling ball down the lane... Two women wearing hats covered in flowers are posing. Young man wearing a hooded jacket sitting on snow in front of mountain area.\nsecond bike from right in front painting next to the two on theleft person all the way to the right\nFigure 1: Results on the test set for grounding of textual phrases using our branch and bound based algorithm. Top Row: Flickr 30k Entities Dataset. Bottom Row: ReferItGame Dataset (Groundtruth box in green and predicted box in red).\nscheme akin to the seminal efficient subwindow search of Lampert et al. [25]. The learned weights can additionally be used as word embeddings. We are not aware of any method that solves textual grounding in a manner similar to our approach and hope to inspire future research into the direction of deep nets combined with powerful inference algorithms.\nWe evaluate our proposed approach on the challenging ReferItGame [20] and the Flickr 30k Entities dataset [35], obtaining results like the ones visualized in Fig. 1. At the time of submission, our approach outperformed state-of-the-art techniques on the ReferItGame and Flickr 30k Entities dataset by 7.77% and 3.08% respectively using the IoU metric. We also demonstrate that the trained parameters of our model can be used as a word-embedding which captures spatial-image relationships and provides interpretability."
            },
            {
                "heading": "2 Related Work",
                "text": "Textual grounding: Related to textual grounding is work on image retrieval. Classical approaches learn a ranking function using recurrent neural nets [30, 6], or metric learning [13], correlation analysis [22], and neural net embeddings [9, 21]. Beyond work in image retrieval, a variety of techniques have been considered to explicitly ground natural language in images and video. One of the first models in this area was presented in [31, 24]. The authors describe an approach that jointly learns visual classifiers and semantic parsers.\nGong et al. [10] propose a canonical correlation analysis technique to associate images with descriptive sentences using a latent embedding space. In spirit similar is work by Wang et al. [42], which learns a structure-preserving embedding for image-sentence retrieval. It can be applied to phrase localization using a ranking framework. In [11], text is generated for a set of candidate object regions which is subsequently compared to a query. The reverse operation, i.e., generating visual features from query text which is subsequently matched to image regions is discussed in [1].\nIn [23], 3D cuboids are aligned to a set of 21 nouns relevant to indoor scenes using a Markov random field based technique. A method for grounding of scene graph queries in images is presented in [17]. Grounding of dependency tree relations is discussed in [19] and reformulated using recurrent nets in [18]. Subject-Verb-Object phrases are considered in [39] to develop a visual knowledge extraction system. Their algorithm reasons about the spatial consistency of the configurations of the involved entities. In [15, 29] caption generation techniques are used to score a set of proposal boxes and returning the highest ranking one. To avoid application of a text generation pipeline on bounding box proposals, [38] improve the phrase encoding using a long short-term memory (LSTM) [12] based deep net. Additional modeling of object context relationship were explored in [32, 14]. Video\n9/5/2017 bbest_redraw\n1/1\ndatasets, although not directly related to our work in this paper, were used for spatiotemporal language grounding in [27, 45].\nCommon datasets for visual grounding are the ReferItGame dataset [20] and a newly introduced Flickr 30k Entities dataset [35], which provides bounding box annotations for noun phrases of the original Flickr 30k dataset [44].\nIn contrast to all of the aforementioned methods, which are largely based on region proposals, we suggest usage of efficient subwindow search as a suitable inference engine.\nEfficient subwindow search: Efficient subwindow search was proposed by Lampert et al. [25] for object localization. It is based on an extremely effective branch and bound scheme that can be applied to a large class of energy functions. The approach has been applied to very efficient deformable part models [43], for object class detection [26], for weakly supervised localization [5], indoor scene understanding [40], diverse object proposals [41] and also for spatio-temporal object detection proposals [33]."
            },
            {
                "heading": "3 Exact Inference for Grounding",
                "text": "We outline our approach for textual grounding in Fig. 2. In contrast to the aforementioned techniques for textual grounding, which typically use a small set of bounding box proposals, we formulate our language grounding approach as an energy minimization over a large number of bounding boxes. The search over a large number of bounding boxes allows us to retrieve an accurate bounding-box prediction for a given phrase and an image. Importantly, by leveraging efficient branch-and-bound techniques, we are able to find the global minimizer for a given energy function very effectively.\nOur energy is based on a set of \u2018image concepts\u2019 like semantic segmentations, detections or image priors. All those concepts come in the form of score maps which we combine linearly before searching for the bounding box containing the highest accumulated score over the combined score map. It is trivial to add additional information to our approach by adding additional score maps. Moreover, linear combination of score maps reveals importance of score maps for specific queries as well as similarity between queries such as \u2018skier\u2019 and \u2018snowboarder.\u2019 Hence the framework that we discuss in the following is easy to interpret and extend to other settings.\nGeneral problem formulation: For simplicity we use x to refer to both given input data modalities, i.e., x = (Q, I), with query text, Q, and image, I . We will differentiate them in the narrative. In addition, we define a bounding box y via its top left corner (y1, y2) and its bottom right corner (y3, y4) and subsume the four variables of interest in the tuple y = (y1, . . . , y4) \u2208 Y = \u220f4 i=1{0, . . . , yi,max}. Every integral coordinate yi, i \u2208 {1, . . . , 4} lies within the set {0, . . . , yi,max}, and Y denotes the\nproduct space of all four coordinates. For notational simplicity only, we assume all images to be scaled to identical dimensions, i.e., yi,max is not dependent on the input data x. We obtain a bounding box prediction y\u0302 given our data x, by solving the energy minimization\ny\u0302 = arg min y\u2208Y E(x, y, w), (1)\nto global optimality. Note that w refers to the parameters of our model. Despite the fact that we are \u2018only\u2019 interested in a single bounding box, the product space Y is generally too large for exhaustive minimization of the energy specified in Eq. (1). Therefore, we pursue a branch-and-bound technique in the following.\nTo apply branch and bound, we assume that the energy function E(x, y, w) depends on two sets of parameters w = [wTt , w T r ] T , i.e., the top layer parameters wt of a neural net, and the remaining parameters wr. In light of this decomposition, our approach requires the energy function to be of the following form:\nE(x, y, w) = wTt \u03c6(x, y, wr).\nNote that the features \u03c6(x, y, wr) may still depend non-linearly on all but the top-layer parameters. This assumption does not pose a severe restriction since almost all of the present-day deep net models typically obtain the logits E(x, y, w) using a fully-connected layer or a convolutional layer with kernel size 1\u00d7 1 as the last computation. Energy Function Details: Our energy function E(x, y, w) is based on a set of \u2018image concepts,\u2019 such as semantic segmentation of object categories, detections, or word priors, all of which we subsume in the set C. Importantly, all image concepts c \u2208 C are attached a parametric score map \u03c6\u0302c(x,wr) \u2208 RW\u00d7H following the image width W and height H . Note that those parametric score maps may depend nonlinearly on some parameters wr. Given a bounding box y, we use the scalar \u03c6c(x, y, wr) \u2208 R to refer to the score accumulated within the bounding box y of score map \u03c6\u0302c(x,wr). To define the energy function we also introduce a set of words of interest, i.e., S. Note that this set contains a special symbol denoting all other words not of interest for the considered task. We use the given query Q, which is part of the data x, to construct indicators, \u03b9s = \u03b4(s \u2208 Q) \u2208 {0, 1}, denoting for every token s \u2208 S its existence in the query Q, where \u03b4 denotes the indicator function. Based on this definition, we formulate the energy function as follows:\nE(x, y, w) = \u2211\ns\u2208S:\u03b9s=1 \u2211 c\u2208C ws,c\u03c6c(x, y, wr), (2)\nwhere ws,c is a parameter connecting a word s \u2208 S to an image concept c \u2208 C. In other words, wt = (ws,c : \u2200s \u2208 S, c \u2208 C). This energy function results in a sparse wt, which increases the speed of inference.\nScore maps: The energy is given by a linear combination of accumulated score maps \u03c6c(x, y, wr). In our case, we use |C| = k1 + k2 + k3 of those maps, which capture three kinds of information: (i) k1 word-priors; (ii) k2 geometric information cues; and (iii) k3 image based segmentations and detections. We discuss each of those maps in the following.\nApproach Accuracy (%) SCRC (2016) [15] 27.80 DSPE (2016) [42] 43.89\nGroundeR (2016) [38] 47.81 CCA (2017) [36] 50.89 Ours (Prior + Geo + Seg + Det) 51.63 Ours (Prior + Geo + Seg + bDet) 53.97\nTable 1: Phrase localization performance on Flickr 30k Entities.\nApproach Accuracy (%) SCRC (2016) [15] 17.93\nGroundeR (2016) [38] 23.44 GroundeR (2016) [38] +SPAT 26.93\nOurs (Prior + Geo) 25.56 Ours (Prior + Geo + Seg) 33.36 Ours (Prior + Geo + Seg + Det) 34.70\nTable 2: Phrase localization performance on ReferItGame.\nFor the top k1 words in the training set we construct word prior maps like the ones shown in Fig. 3 (a). To obtain the prior for a particular word, we search a given training set for each occurrence of the word. With the corresponding subset of image-text pairs and respective bounding box annotations at hand, we compute the average number of times a pixel is covered by a bounding box. To facilitate this operation, we scale each image to a predetermined size. Investigating the obtained word priors given in Fig. 3 (a) more carefully, it is immediately apparent that they provide accurate location information for many of the words.\nThe k2 = 2 geometric cues provide the aspect ratio and the area of the hypothesized bounding box y. Note that the word priors and geometry features contain no information about the image specifics.\nTo encode measurements dedicated to the image at hand, we take advantage of semantic segmentation and object detection techniques. The k3 image based features are computed using deep neural nets as proposed by [4, 37, 2]. We obtain probability maps for a set of class categories, i.e., a subset of the nouns of interest. The feature \u03c6 accumulates the scores within the hypothesized bounding box y.\nInference: The algorithm to find the bounding box y\u0302 with lowest energy as specified in Eq. (1) is based on an iterative decomposition of the output space Y [25], summarized in Fig. 3 (b). To this end we search across subsets of the product space Y and we define for every coordinate yi, i \u2208 {1, . . . , 4} a corresponding lower and upper bound, yi,low and yi,high respectively. More specifically, considering the initial set of all possible bounding boxes Y , we divide it into two disjoint subsets Y\u03021 and Y\u03022. For example, by constraining y1 to {0, . . . , y1,max/2} and {y1,max/2 + 1, . . . , y1,max} for Y\u03021 and Y\u03022 respectively, while keeping all the other intervals unchanged. It is easy to see that we can repeat this decomposition by choosing the largest among the four intervals and recursively dividing it into two parts.\nGiven such a repetitive decomposition strategy for the output space, and since the energy E(x, y, w) for a bounding box y is obtained using a linear combination of word priors and accumulated segmentation masks, we can design an efficient branch and bound based search algorithm to exactly solve the inference problem specified in Eq. (1). The algorithm proceeds by iteratively decomposing a product space Y\u0302 into two subspaces Y\u03021 and Y\u03022. For each subspace, the algorithm computes a lower bound E\u0304(x,Yj , w) for the energy of all possible bounding boxes within the respective subspace. Intuitively, we then know, that any bounding box within the subspace Y\u0302j has a larger energy than the lower bound. The algorithm proceeds by choosing the subspace with lowest lower-bound until this subspace consists of a single element, i.e., until |Y\u0302| = 1. We summarize this algorithm in Alg. 1 (Fig. 3 (b)).\nTo this end, it remains to show how to compute a lower bound E\u0304(x,Yj , w) on the energy for an output space, and to illustrate the conditions which guarantee convergence to the global minimum of the energy function.\nFor the latter, we note that two conditions are required to ensure convergence to the optimum: (i) the bound of the considered product space has to lower-bound the true energy for each of its bounding\nbox hypothesis y\u0302 \u2208 Y\u0302 , i.e., \u2200y\u0302 \u2208 Y\u0302 , E\u0304(x, Y\u0302, w) \u2264 E(x, y\u0302, w); (ii) the bound has to be exact for all possible bounding boxes y \u2208 Y , i.e., E\u0304(x, y, w) = E(x, y, w). Given those two conditions, global convergence of the algorithm summarized in Alg. 1 is apparent: upon termination we obtain an \u2018interval\u2019 containing a single bounding box, and its energy is at least as low as the one for any other interval.\nFor the former, we note that bounds on score maps for bounding box intervals can be computed by considering either the largest or the smallest possible bounding box in the bounding box hypothesis, Y\u0302 , depending on whether the corresponding weight in wt is positive or negative and whether the feature maps contain only positive or negative values. Intuitively, if the weight is positive and the feature mask contains only positive values, we obtain the smallest lower bound E\u0304(x, Y\u0302, w) by considering the content within the smallest possible bounding box. Note that the score maps do not necessarily contain only positive or negative numbers. However we can split the given score maps into two separate score maps (i.e., one with only positive values, and another with only negative values) while applying the same weight.\nIt is important to note that computation of the bound E\u0304(x, Y\u0302, w) has to be extremely effective for the algorithm to run at a reasonable speed. However, computing the feature mask content for a bounding box is trivially possible using integral images. This results in a constant time evaluation of the bound, which is a necessity for the success of the branch and bound procedure.\nLearning the Parameters: With the branch and bound based inference procedure at hand, we now describe how to formulate the learning task. Support-vector machine intuition can be applied. Formally, we are given a training set D = {(x, y)} containing pairs of input data x and groundtruth bounding boxes y. We want to find the parameters w of the energy function E(x, y, w) such that the energy of the groundtruth is smaller than the energy of any other configuration. Negating this statement results in the following desiderata when including an additional margin term L(y, y\u0302), also known as task-loss, which measures the loss between the groundtruth y and another configuration y\u0302:\n\u2212E(x, y, w) \u2265 \u2212E(x, y\u0302, w) + L(y\u0302, y) \u2200y\u0302 \u2208 Y. Since we want to enforce this inequality for all configurations y\u0302 \u2208 Y , we can reduce the number of constraints by enforcing it for the highest scoring right hand side. We then design a cost function which penalizes violation of this requirement linearly. We obtain the following structured support vector machine based surrogate loss minimization:\nmin w\nC 2 \u2016w\u201622 + \u2211 (x,y)\u2208D max y\u0302\u2208Y (\u2212E(x, y\u0302, w) + L(y\u0302, y)) + E(x, y, w) (3)\nwhere C is a hyperparameter adjusting the squared norm regularization to the data term. For the task loss L(y\u0302, y) we use intersection over union (IoU).\nBy fixing the parameters wr and only learning the top layer parameters wt, Eq. (3) is equivalent to the problem of training a structured SVM. We found the cutting-plane algorithm [16] to work well in our context. The cutting-plane algorithm involves solving the maximization task. This maximization over the output space Y is commonly referred to as loss-augmented inference. Loss augmented inference is structurally similar to the inference task given in Eq. (1). Since maximization is identical to negated minimization, the computation of the bounds for the energy E(x, y\u0302, w) remains identical. To bound the IoU loss, we note that a quotient can be bounded by bounding nominator and denominator independently. To lower bound the intersection of the groundtruth box with the hypothesis space we use the smallest hypothesized bounding box. To upper bound the union of the groundtruth box with the hypothesis space we use the largest bounding box.\nFurther, even though not employed to obtain the results in this paper, we mention that it is possible to backpropagate through the neural net parameters wr that influence the energy non-linearly. This underlines that our initial assumption is merely a construct to design an effective inference procedure."
            },
            {
                "heading": "4 Experimental Evaluation",
                "text": "In the following we first provide additional details of our implementation before discussing the results of our approach.\nLanguage processing: In order to process free-form textual phrases efficiently, we restricted the vocabulary size to the top 200 most frequent words in the training set for the ReferItGame, and to the top 1000 most frequent training set words for Flickr 30k Entities; both choices cover about 90% of all phrases in the training set. We map all the remaining words into an additional token. We don\u2019t differentiate between uppercase and lower case characters and we also ignore punctuation.\nSegmentation and detection maps: We employ semantic segmentation, object detection, and poseestimation. For segmentation, we use the DeepLab system [4], trained on PASCAL VOC-2012 [8] semantic image segmentation task, to extract the probability maps for 21 categories. For detection, we use the YOLO object detection system [37], to extract 101 categories, 21 trained on PASCAL VOC-2012, and 80 trained on MSCOCO [28]. For pose estimation, we use the system from [2] to extract the body part location, then post-process to get the head, upper body, lower body, and hand regions.\nFor the ReferItGame, we further fine-tuned the last layer of the DeepLab system to include the categories of \u2018sky,\u2019 \u2018ground,\u2019 \u2018building,\u2019 \u2018water,\u2019 \u2018tree,\u2019 and \u2018grass.\u2019 For the Flickr 30k Entities, we also fine-tuned the last layer of the DeepLab system using the eight coarse-grained types and eleven colors from [36].\nPreprocessing and post-processing: For word prior feature maps and the semantic segmentation maps, we take an element-wise logarithm to convert the normalized feature counts into logprobabilities. The summation over a bounding box region then retains the notion of a joint logprobability. We also centered the feature maps to be zero-mean, which corresponds to choosing an initial decision threshold. The feature maps are resized to dimension of 64 \u00d7 64 for efficient computation, and the predicted box is scaled back to the original image dimension during evaluation. We re-center the prediction box by a constant amount determined using the validation set, as resizing truncate box coordinates to an integer.\nEfficient sub-window search implementation: In order for the efficient subwindow search to run at a reasonable speed, the lower bound on E needs to be computed as fast as possible. Observe that, E(x, y, w), is a weighted sum of the feature maps over the region specified by a hypothesized bounding box. To make this computation efficient, we pre-compute integral images. Given an integral\nimage, the computation for each of the bounding box is simply a look-up operation. This trick can similarly be applied for the geometric features. Since we know the range of the ratio and areas of the bounding boxes ahead of time, we cache the results in a look up table as well.\nThe ReferItGame dataset consists of more than 99,000 regions from 20,000 images. Bounding boxes are assigned to natural language expressions. We use the same bounding boxes as [38] and the same training test set split, i.e., 10,000 images for testing, 9,000 images for training and 1,000 images for validation.\nThe Flickr 30k Entities dataset consists of more than 275k bounding boxes from 31k image, where each bounding box is annotated with the corresponding natural language phrase. We us the same training, validation and testing split as in [35].\nQuantitative evaluation: In Tab. 1 and Tab. 2 we quantitatively compare the results of our approach to recent state-of-the-art baselines, where Prior = word priors, Geo = geometric information, Seg = Segmentation maps, Det = Detection maps, bDet = Detection maps + body parts detection. An example is considered as correct, if the predicted box overlaps with the ground-truth box by more than 0.5 IoU. We observe our approach to outperform competing methods by around 3% on the Flickr 30k Entities dataset and by around 7% on the ReferItGame dataset.\nWe also provide an ablation study of the word and image information as shown in Tab. 1 and Tab. 2.\nIn Tab. 3 we analyze the results for each \u201cphrase type\u201d provided by Flicker30k Entities dataset. As can be seen, our system outperforms the state-of-the-art in all phrase types except for clothing.\nWe note that our results have been surpassed by [3, 7, 34], where they fine-tuned the entire network including the feature extractions or trained more feature detectors; CCA, GroundeR and our approach uses a fixed pre-trained network for extracting image features.\nQualitative evaluation: Next we evaluate our approach qualitatively. In Fig. 1 and Fig. 4 we show success cases. We observe that our method successfully captures a variety of objects and scenes. In Fig. 5 we illustrate failure cases. We observe that for a few cases word prior may hurt the prediction (e.g., shoes are typically on the bottom half of the image.) Also our system may fail when the energy is not a linear combination of the feature scores. For example, the score of \u201cdirt bike\u201d should not be the score of \u201cdirt\u201d + the score of \u201cbike.\u201d We provide additional results in the supplementary material.\nLearned parameters + word embedding: Recall, in Eq. (2), our model learns a parameter per phrase word and concept pair, ws,c. We visualize its magnitude in Fig. 6 (a) for a subset of words and concepts. As can be seen, ws,c is large, when the phrase word and the concept are related, (e.g. s = ship and c = boat). This demonstrates that our model successfully learns the relationship between phrase words and image concepts. This also means that the \u201cword vector,\u201d ws = [ws,1, ws,2, ...ws,|C|], can be interpreted as a word embedding. Therefore, in Fig. 6 (b), we visualize the cosine similarity between pairs of word vectors. Expected groups of words form, for example (bicycle, bike), (camera, cellphone), (coffee, cup, drink), (man woman), (snowboarder, skier). The word vectors capture\nimage-spatial relationship of the words, meaning items that can be \u201creplaced\u201d in an image are similar; (e.g., a \u201csnowboarder\u201d can be replaced with a \u201cskier\u201d and the overall image would still be reasonable).\nComputational Efficiency: Overall, our method\u2019s inference speed is comparable to CCA and much faster than GroundeR. The inference speed can be divided into three main parts, (1) extracting image features, (2) extracting language features, and (3) computing scores. For extracting image features, GroundeR requires a forward pass on VGG16 for each image region, where CCA and our approach requires a single forward pass which can be done in 142.85 ms. For extracting language features, our method requires index lookups, which takes negligible amount of time (less than 1e-6 ms). CCA, uses Word2vec for processing the text, which takes 0.070 ms. GroundeR uses a Long-Short-Term Memory net, which takes 0.7457 ms. Computing the scores with our C++ implementation takes 1.05ms on a CPU. CCA needs to compare projections of the text and image features, which takes 13.41ms on a GPU and 609ms on a CPU. GroundeR uses a single fully connected layer, which takes 0.31 ms on a GPU."
            },
            {
                "heading": "5 Conclusion",
                "text": "We demonstrated a mechanism for grounding of textual phrases which provides interpretability, is easy to extend, and permits globally optimal inference. In contrast to existing approaches which are generally based on a small set of bounding box proposals, we efficiently search over all possible bounding boxes. We think interpretability, i.e., linking of word and image concepts, is an important concept, particularly for textual grounding, which deserves more attention.\nAcknowledgments: This material is based upon work supported in part by the National Science Foundation under Grant No. 1718221. This work is supported by NVIDIA Corporation with the donation of a GPU. This work is supported in part by IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM Cognitive Horizons Network."
            }
        ],
        "references": [
            {
                "title": "Multiple queries for large scale specific object retrieval",
                "author": [
                    "R. Arandjelovic",
                    "A. Zisserman"
                ],
                "venue": "In Proc. BMVC,",
                "citeRegEx": "1",
                "shortCiteRegEx": "1",
                "year": 2012
            },
            {
                "title": "Realtime multi-person 2d pose estimation using part affinity fields",
                "author": [
                    "Z. Cao",
                    "T. Simon",
                    "S.-E. Wei",
                    "Y. Sheikh"
                ],
                "venue": "In Proc. CVPR,",
                "citeRegEx": "2",
                "shortCiteRegEx": "2",
                "year": 2017
            },
            {
                "title": "Query-guided regression network with context policy for phrase grounding",
                "author": [
                    "K. Chen",
                    "R. Kovvuri",
                    "R. Nevatia"
                ],
                "venue": "In Proc. ICCV,",
                "citeRegEx": "3",
                "shortCiteRegEx": "3",
                "year": 2017
            },
            {
                "title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs",
                "author": [
                    "L.-C. Chen",
                    "G. Papandreou",
                    "I. Kokkinos",
                    "K. Murphy",
                    "A.L. Yuille"
                ],
                "venue": "In Proc. ICLR,",
                "citeRegEx": "4",
                "shortCiteRegEx": "4",
                "year": 2015
            },
            {
                "title": "Weakly supervised localization and learning with generic knowledge",
                "author": [
                    "T. Deselaers",
                    "B. Alexe",
                    "V. Ferrari"
                ],
                "venue": null,
                "citeRegEx": "5",
                "shortCiteRegEx": "5",
                "year": 2012
            },
            {
                "title": "Long-term recurrent convolutional networks for visual recognition and description",
                "author": [
                    "J. Donahue",
                    "L.A. Hendricks",
                    "S. Guadarrama",
                    "M. Rohrbach",
                    "S. Venugopalan",
                    "K. Saenko",
                    "T. Darrell"
                ],
                "venue": "In Proc. CVPR,",
                "citeRegEx": "6",
                "shortCiteRegEx": "6",
                "year": 2015
            },
            {
                "title": "An attention-based regression model for grounding textual phrases in images",
                "author": [
                    "K. Endo",
                    "M. Aono",
                    "E. Nichols",
                    "K. Funakoshi"
                ],
                "venue": "In Proc. IJCAI,",
                "citeRegEx": "7",
                "shortCiteRegEx": "7",
                "year": 2017
            },
            {
                "title": "The pascal visual object classes (voc) challenge",
                "author": [
                    "M. Everingham",
                    "L. Van Gool",
                    "C.K. Williams",
                    "J. Winn",
                    "A. Zisserman"
                ],
                "venue": null,
                "citeRegEx": "8",
                "shortCiteRegEx": "8",
                "year": 2010
            },
            {
                "title": "Devise: A deep visual-semantic embed- ding model",
                "author": [
                    "A. Frome",
                    "G.S. Corrado",
                    "J. Shlens",
                    "S. Bengio",
                    "J. Dean",
                    "T. Mikolov"
                ],
                "venue": "In Proc. NIPS,",
                "citeRegEx": "9",
                "shortCiteRegEx": "9",
                "year": 2013
            },
            {
                "title": "Improving image-sentence embeddings using large weakly annotated photo collections",
                "author": [
                    "Y. Gong",
                    "L. Wang",
                    "M. Hodosh",
                    "J. Hockenmaier",
                    "S. Lazebnik"
                ],
                "venue": "In Proc. ECCV,",
                "citeRegEx": "10",
                "shortCiteRegEx": "10",
                "year": 2014
            },
            {
                "title": "Open-vocabulary object retrieval",
                "author": [
                    "S. Guadarrama",
                    "E. Rodner",
                    "K. Saenko",
                    "N. Zhang",
                    "R. Farrell",
                    "J. Donahue",
                    "T. Darrell"
                ],
                "venue": "In Proc. RSS,",
                "citeRegEx": "11",
                "shortCiteRegEx": "11",
                "year": 2014
            },
            {
                "title": "Long short-term memory",
                "author": [
                    "S. Hochreiter",
                    "J. Schmidhuber"
                ],
                "venue": "Neural Computation,",
                "citeRegEx": "12",
                "shortCiteRegEx": "12",
                "year": 1997
            },
            {
                "title": "Learning distance metrics with contextual constraints for image retrieval",
                "author": [
                    "S.C. Hoi",
                    "W. Liu",
                    "M.R. Lyu",
                    "W.-Y. Ma"
                ],
                "venue": "In Proc. CVPR,",
                "citeRegEx": "13",
                "shortCiteRegEx": "13",
                "year": 2006
            },
            {
                "title": "Modeling relationships in referential expressions with compositional modular networks",
                "author": [
                    "R. Hu",
                    "M. Rohrbach",
                    "J. Andreas",
                    "T. Darrell",
                    "K. Saenko"
                ],
                "venue": "In Proc. CVPR,",
                "citeRegEx": "14",
                "shortCiteRegEx": "14",
                "year": 2017
            },
            {
                "title": "Natural language object retrieval",
                "author": [
                    "R. Hu",
                    "H. Xu",
                    "M. Rohrbach",
                    "J. Feng",
                    "K. Saenko",
                    "T. Darrell"
                ],
                "venue": "In Proc. CVPR,",
                "citeRegEx": "15",
                "shortCiteRegEx": "15",
                "year": 2016
            },
            {
                "title": "Cutting-plane training of structural svms",
                "author": [
                    "T. Joachims",
                    "T. Finley",
                    "C.-N.J. Yu"
                ],
                "venue": "Machine Learning,",
                "citeRegEx": "16",
                "shortCiteRegEx": "16",
                "year": 2009
            },
            {
                "title": "Image retrieval using scene graphs",
                "author": [
                    "J. Johnson",
                    "R. Krishna",
                    "M. Stark",
                    "L.J. Li",
                    "D. Shamma",
                    "M. Bernstein",
                    "L. Fei-Fei"
                ],
                "venue": "In Proc. CVPR,",
                "citeRegEx": "17",
                "shortCiteRegEx": "17",
                "year": 2015
            },
            {
                "title": "Deep visual-semantic alignments for generating image descriptions",
                "author": [
                    "A. Karpathy",
                    "L. Fei-Fei"
                ],
                "venue": "In Proc. CVPR,",
                "citeRegEx": "18",
                "shortCiteRegEx": "18",
                "year": 2015
            },
            {
                "title": "Deep fragment embeddings for bidirectional image sentence mapping",
                "author": [
                    "A. Karpathy",
                    "A. Joulin",
                    "L. Fei-Fei"
                ],
                "venue": "In Proc. NIPS,",
                "citeRegEx": "19",
                "shortCiteRegEx": "19",
                "year": 2014
            },
            {
                "title": "ReferItGame: Referring to objects in photographs of natural scenes",
                "author": [
                    "S. Kazemzadeh",
                    "V. Ordonez",
                    "M. Matten",
                    "T.L. Berg"
                ],
                "venue": "In Proc. EMNLP,",
                "citeRegEx": "20",
                "shortCiteRegEx": "20",
                "year": 2014
            },
            {
                "title": "Unifying visual-semantic embeddings with multimodal neural language models",
                "author": [
                    "R. Kiros",
                    "R. Salakhutdinov",
                    "R.S. Zemel"
                ],
                "venue": "In TACL,",
                "citeRegEx": "21",
                "shortCiteRegEx": "21",
                "year": 2015
            },
            {
                "title": "Fisher vectors derived from hybrid gaussian-laplacian mixture models for image annotation",
                "author": [
                    "B. Klein",
                    "G. Lev",
                    "G. Sadeh",
                    "L. Wolf"
                ],
                "venue": "In arXiv preprint arXiv:1411.7399,",
                "citeRegEx": "22",
                "shortCiteRegEx": "22",
                "year": 2014
            },
            {
                "title": "What are you talking about? text-to-image coreference",
                "author": [
                    "C. Kong",
                    "D. Lin",
                    "M. Bansal",
                    "R. Urtasun",
                    "S. Fidler"
                ],
                "venue": "In Proc. CVPR,",
                "citeRegEx": "23",
                "shortCiteRegEx": "23",
                "year": 2014
            },
            {
                "title": "Jointly learning to parse and perceive: connecting natural language to the physical world",
                "author": [
                    "J. Krishnamurthy",
                    "T. Kollar"
                ],
                "venue": "In Proc. TACL,",
                "citeRegEx": "24",
                "shortCiteRegEx": "24",
                "year": 2013
            },
            {
                "title": "Efficient Subwindow Search: A Branch and Bound Framework for Object Localization",
                "author": [
                    "C.H. Lampert",
                    "M.B. Blaschko",
                    "T. Hofmann"
                ],
                "venue": null,
                "citeRegEx": "25",
                "shortCiteRegEx": "25",
                "year": 2009
            },
            {
                "title": "Fast PRISM: Branch and Bound Hough Transform for Object Class Detection",
                "author": [
                    "A. Lehmann",
                    "B. Leibe",
                    "L.V. Gool"
                ],
                "venue": null,
                "citeRegEx": "26",
                "shortCiteRegEx": "26",
                "year": 2011
            },
            {
                "title": "Visual semantic search: Retrieving videos via complex textual queries",
                "author": [
                    "D. Lin",
                    "S. Fidler",
                    "C. Kong",
                    "R. Urtasun"
                ],
                "venue": "In Proc. CVPR,",
                "citeRegEx": "27",
                "shortCiteRegEx": "27",
                "year": 2014
            },
            {
                "title": "Microsoft coco: Common objects in context",
                "author": [
                    "T.-Y. Lin",
                    "M. Maire",
                    "S. Belongie",
                    "J. Hays",
                    "P. Perona",
                    "D. Ramanan",
                    "P. Doll\u00e1r",
                    "C.L. Zitnick"
                ],
                "venue": "In Proc. ECCV,",
                "citeRegEx": "28",
                "shortCiteRegEx": "28",
                "year": 2014
            },
            {
                "title": "Generation and comprehension of unambiguous object descriptions",
                "author": [
                    "J. Mao",
                    "J. Huang",
                    "A. Toshev",
                    "O. Camburu",
                    "A. Yuille",
                    "K. Murphy"
                ],
                "venue": "In Proc. CVPR,",
                "citeRegEx": "29",
                "shortCiteRegEx": "29",
                "year": 2016
            },
            {
                "title": "Deep captioning with multimodal recurrent neural networks (m-rnn)",
                "author": [
                    "J. Mao",
                    "W. Xu",
                    "Y. Yang",
                    "J. Wang",
                    "Z. Huang",
                    "A. Yuille"
                ],
                "venue": "In Proc. ICLR,",
                "citeRegEx": "30",
                "shortCiteRegEx": "30",
                "year": 2015
            },
            {
                "title": "A joint model of language and perception for grounded attribute learning",
                "author": [
                    "C. Matuszek",
                    "N. Fitzgerald",
                    "L. Zettlemoyer",
                    "L. Bo",
                    "D. Fox"
                ],
                "venue": "In Proc. ICML,",
                "citeRegEx": "31",
                "shortCiteRegEx": "31",
                "year": 2012
            },
            {
                "title": "Modeling context between objects for referring expression understanding",
                "author": [
                    "V.K. Nagaraja",
                    "V.I. Morariu",
                    "L.S. Davis"
                ],
                "venue": "In Proc. ECCV,",
                "citeRegEx": "32",
                "shortCiteRegEx": "32",
                "year": 2016
            },
            {
                "title": "Spatio-temporal object detection proposals",
                "author": [
                    "D. Oneata",
                    "J. Revaud",
                    "J. Verbeek",
                    "C. Schmid"
                ],
                "venue": "In Proc. ECCV,",
                "citeRegEx": "33",
                "shortCiteRegEx": "33",
                "year": 2014
            },
            {
                "title": "Phrase localization and visual relationship detection with comprehensive image-language cues",
                "author": [
                    "B.A. Plummer",
                    "A. Mallya",
                    "C.M. Cervantes",
                    "J. Hockenmaier",
                    "S. Lazebnik"
                ],
                "venue": "In Proc. ICCV,",
                "citeRegEx": "34",
                "shortCiteRegEx": "34",
                "year": 2017
            },
            {
                "title": "Collecting region-to-phrase correspondences for richer image-to- sentence models",
                "author": [
                    "B.A. Plummer",
                    "L. Wang",
                    "C.M. Cervantes",
                    "J.C. Caicedo",
                    "J. Hockenmaier",
                    "S. Lazebnik"
                ],
                "venue": "In Proc. ICCV,",
                "citeRegEx": "35",
                "shortCiteRegEx": "35",
                "year": 2015
            },
            {
                "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
                "author": [
                    "B.A. Plummer",
                    "L. Wang",
                    "C.M. Cervantes",
                    "J.C. Caicedo",
                    "J. Hockenmaier",
                    "S. Lazebnik"
                ],
                "venue": null,
                "citeRegEx": "36",
                "shortCiteRegEx": "36",
                "year": 2017
            },
            {
                "title": "Yolo9000: Better, faster, stronger",
                "author": [
                    "J. Redmon",
                    "A. Farhadi"
                ],
                "venue": "In CVPR,",
                "citeRegEx": "37",
                "shortCiteRegEx": "37",
                "year": 2017
            },
            {
                "title": "Grounding of Textual Phrases in Images by Reconstruction",
                "author": [
                    "A. Rohrbach",
                    "M. Rohrbach",
                    "R. Hu",
                    "T. Darrell",
                    "B. Schiele"
                ],
                "venue": "In Proc. ECCV,",
                "citeRegEx": "38",
                "shortCiteRegEx": "38",
                "year": 2016
            },
            {
                "title": "Viske: Visual knowledge extraction and question answering by visual verification of relation phrases",
                "author": [
                    "F. Sadeghi",
                    "S.K. Divvala",
                    "A. Farhadi"
                ],
                "venue": "In Proc. CVPR,",
                "citeRegEx": "39",
                "shortCiteRegEx": "39",
                "year": 2015
            },
            {
                "title": "Efficient Exact Inference for 3D Indoor Scene Understanding",
                "author": [
                    "A.G. Schwing",
                    "R. Urtasun"
                ],
                "venue": "In Proc. ECCV,",
                "citeRegEx": "40",
                "shortCiteRegEx": "40",
                "year": 2012
            },
            {
                "title": "Submodboxes: Near-optimal search for a set of diverse object proposals",
                "author": [
                    "Q. Sun",
                    "D. Batra"
                ],
                "venue": "In Proc. NIPS,",
                "citeRegEx": "41",
                "shortCiteRegEx": "41",
                "year": 2015
            },
            {
                "title": "Learning deep structure-preserving image-text em- beddings",
                "author": [
                    "L. Wang",
                    "Y. Li",
                    "S. Lazebnik"
                ],
                "venue": "In Proc. CVPR,",
                "citeRegEx": "42",
                "shortCiteRegEx": "42",
                "year": 2016
            },
            {
                "title": "The Fastest Deformable Part Model for Object Detection",
                "author": [
                    "J. Yan",
                    "Z. Lei",
                    "L. Wen",
                    "S.Z. Li"
                ],
                "venue": "In Proc. CVPR,",
                "citeRegEx": "43",
                "shortCiteRegEx": "43",
                "year": 2014
            },
            {
                "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
                "author": [
                    "P. Young",
                    "A. Lai",
                    "M. Hodosh",
                    "J. Hockenmaier"
                ],
                "venue": "In Proc. TACL,",
                "citeRegEx": "44",
                "shortCiteRegEx": "44",
                "year": 2014
            },
            {
                "title": "Grounded language learning from video described with sen- tences",
                "author": [
                    "H. Yu",
                    "J.M. Siskind"
                ],
                "venue": "In Proc. ACL,",
                "citeRegEx": "45",
                "shortCiteRegEx": "45",
                "year": 2013
            }
        ],
        "abstractText": "Textual grounding is an important but challenging task for human-computer interaction, robotics and knowledge mining. Existing algorithms generally formulate the task as selection from a set of bounding box proposals obtained from deep net based systems. In this work, we demonstrate that we can cast the problem of textual grounding into a unified framework that permits efficient search over all possible bounding boxes. Hence, the method is able to consider significantly more proposals and doesn\u2019t rely on a successful first stage hypothesizing bounding box proposals. Beyond, we demonstrate that the trained parameters of our model can be used as word-embeddings which capture spatial-image relationships and provide interpretability. Lastly, at the time of submission, our approach outperformed the current state-of-the-art methods on the Flickr 30k Entities and the ReferItGame dataset by 3.08% and 7.77% respectively."
    },
    {
        "title": null,
        "sections": [
            {
                "heading": null,
                "text": "ar X\niv :1\n51 2.\n03 95\n8v 1\n[c s.\nC V\n] 12\nD ec\nRecurrent Neural Networks (RNNs) have had considerable success in classifying and predicting sequences. We demonstrate that RNNs can be effectively used in order to encode sequences and provide effective representations. The methodology we use is based on Fisher Vectors, where the RNNs are the generative probabilistic models and the partial derivatives are computed using backpropagation. State of the art results are obtained in two central but distant tasks, which both rely on sequences: video action recognition and image annotation. We also show a surprising transfer learning result from the task of image annotation to the task of video action recognition."
            },
            {
                "heading": "1. Introduction",
                "text": "Fisher Vectors have been shown to provide a significant performance gain on many different applications in the domain of computer vision [39, 33, 2, 35]. In the domain of video action recognition, Fisher Vectors and Stacked Fisher Vectors [33] have recently outperformed state-of-theart methods on multiple datasets [33, 53]. Fisher Vectors (FV) have also recently been applied to word embedding (e.g. word2vec [30]) and have been shown to provide state of the art results on a variety of NLP tasks [24], as well as on image annotation and image search tasks [18].\nIn all of these contributions, the FV of a set of local descriptors is obtained as a sum of gradients of the loglikelihood of the descriptors in the set with respect to the parameters of a probabilistic mixture model that was fitted on a training set in an unsupervised manner. In spite of being richer than the mean vector pooling method, Fisher Vectors based on a probabilistic mixture model are invariant to order. This makes them less appealing for annotating, for example, video, in which the sequence of events determines much of the meaning.\nThis work presents a novel approach for FV representation of sequences using a Recurrent Neural Network\n(RNN). The RNN is trained to predict the next element of a sequence given the previous elements. Conveniently, the gradients needed for the computation of the FV are extracted using the available backpropagation infrastructue.\nThe new representation is sensitive to ordering and therefore mitigates the disadvantage of using the standard Fisher Vector representation. It is applied to two different and challenging tasks: video action recognition and image annotation by sentences.\nSeveral recent works have proposed to use an RNN for sentence representation [44, 1, 31, 17]. The Recurrent Neural Network Fisher Vector (RNN-FV) method differs from these works in that a sequence is represented by using derived gradient from the RNN as features, instead of using a hidden or an output layer of the RNN.\nThe paper explores two different approaches for training the RNN for the image annotation and image search tasks. In the classification approach, the RNN is trained to predict the following word in the sentence. The regression approach tries to predict the embedding of the following word (i.e. treating it as a regression task). The large vocabulary size makes the regression approach more scalable and achieves better results than the classification approach. In the video action recognition task, the regression approach is the only variant being used, since the notion of a discrete word does not exist. The VGG [41] Convolutional Neural Network (CNN) is used to extract features from the frames of the video and the RNN is trained to predict the embedding of the next frame given the previous ones. Similarly, C3D [46] features of sequential video sub-volumes are used with the same training technique.\nAlthough the image annotation and video action recognition tasks are quite different, a surprising boost in performance in the video action recognition task was achieved by using a transfer learning approach from the image annotation task. Specifically, the VGG image embedding of a frame is projected using a linear transformation which was learned on matching images and sentences by the Canonical Correlation Analysis (CCA) algorithm [10].\n1\nThe proposed RNN-FV method achieves state-of-theart results in action recognition on the HMDB51 [20] and UCF101 [43] datasets. In image annotation and image search tasks, the RNN-FV method is used for the representation of sentences and achieves state-of-the-art resul s on the Flickr8K dataset [8] and competitive results on other benchmarks."
            },
            {
                "heading": "2. Previous Work",
                "text": "Action Recognition As in other object recognition problems, the standard pipeline in action recognition is comprised of three main steps: feature extraction, pooling and classification. Many works [23, 49, 19] have focused on the first step of extracting local descriptors. Laptev et al. [22] extend the notion of spatial interest points into the spatiotemporal domain and show how the resulting features can be used for a compact representation of video data. Wang et al. [51, 50] used low-level hand-crafted features such as histogram of oriented gradients (HOG), histogram of optical flow (HOF) and motion boundary histogram (MBH).\nRecent works have attempted to replace these handcrafted features by deep-learned features for video action recognition due to its wide success in the image domain. Early attempts [45, 12, 15] achieved lower results in comparison to hand-crafted features, proving that it is challenging to apply deep-learning techniques on videos due to the relatively small number of available datasets and complex motion patterns. More recent attempts managed to overcome these challenges and achieve state of the art results with deep-learned features. Simonyan et al. [40] designed two-stream ConvNets for learning both the appearance of the video frame and the motion as reflected by the estimated optical flow. Du Tran et al. [46] designed an effective approach for spatiotemporal feature learning using 3-dimensional ConvNets.\nIn the second step of the pipeline, the pooling, Wang et al. [54] compared different pooling techniques for the application of action recognition and showed empirically that the Fisher Vector encoding has the best performance. Recently, more complex pooling methods were demonstrated by Peng et al. [33] who proposed Stacked Fisher Vectors (SFV), a multi-layer nested Fisher Vector encoding and Wang et al. [53] who proposed a trajectory-pooled deepconvolutional descriptor (TDD). TDD uses both a motion CNN, trained on UCF101, and an appearance CNN, originally trained on ImageNet [3], and fine-tuned on UCF101.\nImage Annotation and Image Search In the past few years, the state-of-the-art results in image annotation and image search have been provided by deep learning approaches [42, 29, 18, 14, 27, 16, 4, 13, 48, 26]. A typical system is composed of three important components: (i) Image Representation, (ii) Sentence Representation, and (iii)\nMatching Images and Sentences. The image is usually represented by applying a pre-trained CNN on the image and taking the activations from the last hidden layer.\nThere are several different approaches for the sentence representation; Socher et al. [42] used a dependency tree Recursive Neural Network. Yan et al. [29] used a TFIDF histogram over the vocabulary. Klein et al. [18] used word2vec [30] as the word embedding and then applied Fisher Vector based on a Hybrid Gaussian-Laplacian Mixture Model (HGLMM) in order to pool the word2vec embeddings of the words in a given sentence into a single representation. Ma et al. [26] proposed a matching CNN (mCNN) that composes words to different semantic fragments and learns the inter-modal relations between image and the composed fragments at different levels.\nSince a sentence can be seen as a sequence of words, many works have used a Recurrent Neural Network (RNN) in order to represent sentences [14, 48, 27, 16, 17]. To address the need for capturing long term semantics in the sentence, these works mainly use Long Short-Term Memory (LSTM) [7] or Gated Recurrent Unit (GRU) [5] cells. Generally, the RNN treats a sentence as an ordered sequence of words, and incrementally encodes a semantic vector of the sentence, word-by-word. At each time step, a new word is encoded into the semantic vector, until the end of the sentence is reached. All of the words and their dependencies will then have been embedded into the semantic vector, which can be used as a feature vector representation of the entire sentence. Our work also uses an RNN in order to represent sentences but takes the derived gradient from the RNN as features, instead of using a hidden or an output layer of the RNN.\nA number of techniques have been proposed for the task of matching images and sentences. Klein et al. [18] used CCA [10] and Yan et al. [29] introduced a Deep CCA in order to project the images and sentences into a common space and then performed a nearest neighbor search between the images and the sentences in the common space. Kiros et al. [16], Karpathy et al.[14], Socher et al. [42] and Ma et al. [26] used a contrastive loss function trained on matching and unmatching pairs of (image,sentence) in order to learn a score function for a given pair. Mao et al. [27] and Vinyals et al. [48] learned a probabilistic model for inferring a sentence given an image and, therefore, are able to compute the probability that a given sentence will be created by a given image and used it as the score."
            },
            {
                "heading": "3. Baseline pooling methods",
                "text": "In this section we describe two baseline pooling methods that can represent a multiset of vectors as a single vector. The notation of a multiset is used to clarify that the order of the words in a sentence does not affect the representation, and that a vector can appear more than once. Both methods\ncan be applied to sequences, however, the resulting representation will be insensitive to ordering. To address this, we propose in Sec.4 a novel pooling method: RNN-FV."
            },
            {
                "heading": "3.1. Mean Vector",
                "text": "This pooling technique takes a multiset of vectors, X = {x1, x2, . . . , xN} \u2208 R\nD, and computes its mean: v = 1N \u2211N i=1 xi. Clearly, the vectorv that results from the pooling is inRD. The disadvantage of this method is the blurring of the multiset\u2019s content. Consider, for example, the text encoding task, where each word is represented by its word2vec embedding. By adding multiple vectors together, the location obtained \u2013 in the semantic embedding space \u2013 is somewhere in the convex hull of the words that belong to the multiset."
            },
            {
                "heading": "3.2. Fisher Vector of a GMM",
                "text": "Given a multiset of vectors,X = {x1, x2, . . . , xN} \u2208 RD, the standard FV [34] is defined as the gradient of the log-likelihood of X with respect to the parameters of a pre-trained Diagonal-Covariance Gaussian Mixture Model (GMM). It is a common practice to limit the FV representation to the partial derivatives with respect to the means,\u00b5, and the standard deviations,\u03c3, and ignore the partial derivatives with respect to the mixture weights.\nIt is worth noting the linear structure of the GMM FV pooling. Since the likelihood of the multiset is the multiplication of the likelihoods of the individual elements, the log-likelihood is additive. This convenient property would not be preserved in the RNN model, where the probability of an element in the sequence depends on all the previous elements.\nTo all types of FV, we apply the two improvements that were introduced by Perronnin et al. [35]. The first improvement is to apply an element-wise power normalization function, f(z) = sign(z)|z|\u03b1 where0 \u2264 \u03b1 \u2264 1 is a parameter of the normalization. The second improvement is to apply an L2 normalization on the FV after applying the power normalization function."
            },
            {
                "heading": "4. RNN-Based Fisher Vector",
                "text": "The pooling methods described above share a common disadvantage: insensitivity to the order of the elements in the sequence. A way to tackle this, while keeping the power of gradient-based representation, would be to replace the Gaussian model by a generative sequence model that takes into account the order of elements in the sequence. A desirable property of the sequence model would be the ability to calculate the gradient (with respect to the model\u2019s parameters) of the likelihood estimate by this model to an input sequence.\nIn this section, we show that such a model can be obtained by training an RNN to predict the next element in a\nsequence, given the previous elements. Having this, we propose, for the first time, the RNN-FV: A Fisher Vector that is based on such an RNN sequence model.\nWe propose two types of RNN-FVs. One type is based on training a regression problem, and the other on training a classification problem. In practice, only the first type is directly useful for video analysis. For image annotation, the first type outperforms the second.\nGiven a sequence of vectorsS with N vector elementsx1, ..., xN , we convert it to the input sequenceX = (x0, x1, ..., xN\u22121), wherex0 = xstart. This special element is used to denote the beginning of the input sequence, and we usexstart = 0 throughout this paper. The RNN is trained to predict, at each time stepi, the next element xi+1 of the sequence, given the previous elementsx0, ..., xi. Therefore, given the input sequence, the target sequence would be:Y = (x1, x2, ...xN ).\nThe training data and the training process are application dependent, as is described in Sec.5 for action recognition and in Sec.6 for image annotation."
            },
            {
                "heading": "4.1. RNN Trained for Regression",
                "text": "Given a sequence of input vectorsX , the regression RNN is trained to predict the next vector in the sequence S, i.e., the sequenceY . The output layer of the network is a fully-connected layer, the size of which would beD, i.e., the dimension of the input vector space.\nThere are several regression loss functions that can be used. Here, we consider the following loss function:\nLoss(y, v) = 1\n2 \u2016y \u2212 v\u20162 (1)\nwherey is the target vector andv is the predicted vector. After the RNN training is done, and given a new sequenceS, the derived sequenceX is fed to the RNN. Denote the output of the RNN at time stepi (i = 0, ..., N \u2212 1) by RNN(x0, ..., xi) = vi \u2208 RD. The target at time stepi is xi+1 (the next element in the sequence), and the loss is:\nLoss(xi+1, vi) = 1\n2 \u2016xi+1 \u2212 vi\u2016\n2 (2)\nThe RNN can be seen as a generative model, and the likelihood of any vectorx being the next element of the sequence, givenx0, ..., xi, can be defined as:\np (x|x0, ..., xi) = (2\u03c0) \u2212D/2 exp\n(\n\u2212 1\n2 \u2016x\u2212 vi\u2016\n2\n)\n(3)\nWe are generally interested in the likelihood of the correct prediction, i.e., in the likelihood of the vectorxi+1 givenx0, ..., xi: p (xi+1|x0, ..., xi).\nThe RNN-based likelihood of the entire sequence X is:\np(X) =\nN\u22121 \u220f\ni=0\np (xi+1|x0, ..., xi) (4)\nThe negative log likelihood ofX is:\nL(X) = \u2212 log (p(X)) = \u2212\nN\u22121 \u2211\ni=0\nlog (p (xi+1|x0, ..., xi))\n= ND\n2 log(2\u03c0) +\n1\n2\nN\u22121 \u2211\ni=0\n\u2016xi+1 \u2212 vi\u2016 2\n(5)\nIn order to representX using the Fisher Vector scheme, we have to compute the gradient ofL(X) with respect to our model\u2019s parameters. With RNN being our model, the parameters are the weightsW of the network. By (2) and (5), we get thatL(X) equals the loss that would be obtained whenX is fed as input to the RNN, up to an additive constant. Therefore, the desired gradient can be computed by backpropagation: we feedX to the network and perform forward and backward passes. The obtained gradient \u2207WL(X) would be the (unnormalized) RNN-FV representation ofX . Notice that this gradient isnot used to update the network\u2019s weights as done in training - here we perform backpropagationat inference time.\nOther loss functions may be used instead of the one presented in this analysis. Given a sequence, the gradient of the RNN loss may serve as the sequence representation, even if the loss is not interpretable as a likelihood."
            },
            {
                "heading": "4.2. RNN Trained for Classification",
                "text": "The classification application is applicable for predicting a sequence of symbolsw1,w2,...,wN that have matching vector representationsR(w1) = x1, R(w2) = x2, ..., R(wN ) = xN . The RNN predicts the sequenceU = (w1, w2, . . . , wN ) from the sequenceX = (x0, x1, . . . , xN\u22121).\nDenote byM the size of our symbol alphabet, i.e., the number of unique symbols in the input sequences. The output layer of the network is a softmax layer withM units, where thej\u2019th element in the output is the probability of the j\u2019th symbol to be the next output element. The loss function for the training of the RNN is the cross-entropy loss.\nAfter the RNN is trained, it is ready to be used as a feature vector extractor for new sequences. Denote the new sequence byU and its vector representation byX as above. Consider feeding the sequenceX to the RNN. At time stepi (i = 0, ..., N \u2212 1), the output of the RNN is RNN(x0, ..., xi) = (pi1, ..., p i M ), where \u2211M j=1 p i j = 1. Here,pij is the probability which the RNN gives to thej\u2019th symbol at time stepi.\nThe cross-entropy loss at time stepi is derived from the probability given to the correct next symbol:\nlossi = \u2212 log ( piwi+1 ) = \u2212 log (Pr (wi+1|w0, ..., wi))\n(6)\nThe RNN can be seen as a generative model which gives likelihood to the sequenceU :\nPr(U) =\nN\u22121 \u220f\ni=0\nPr (wi+1|w0, ..., wi) =\nN\u22121 \u220f\ni=0\npiwi+1 (7)\nThe negative log likelihood ofU is:\nL(U) = \u2212 log (Pr(U)) = \u2212\nN\u22121 \u2211\ni=0\nlog (\npiwi+1\n)\n(8)\nBy (6) and (8), we get thatL(U) equals the loss that would be obtained whenX is fed as input, andU as output to the RNN. Therefore, the desired gradient can be computed by backpropagation, i.e. feedingX to the network and performing forward and backward passes. The obtained gradient\u2207WL(U) would be the (unnormalized) RNN-FV representation ofU ."
            },
            {
                "heading": "4.3. Normalization of the RNN-FV",
                "text": "It was suggested by [34] that normalizing the FVs by the Fisher Information Matrix is beneficial. We approximated the diagonal of the Fisher Information Matrix (FIM), which is usually used for FV normalization. Note, however, that we did not observe any empirical improvement due to this normalization, and our experiments are reported without it.\nLet \u03c9 \u2208 W be a single weight of the RNN. The term in the diagonal of the FIM which corresponds to\u2202L(X|W )\u2202\u03c9 is: F\u03c9 = \u222b\nX p (X |W )\n[\n\u2202L(X|W ) \u2202\u03c9\n]2\ndX .\nSince the probabilistic model which determines p (X |W ) is the RNN, it is impossible to derive a closedform expression for this term. Therefore, we approximated it directly from the gradients of the training sequences, by computing the mean of [\n\u2202L(X|W ) \u2202\u03c9\n]2\nfor each\u03c9 \u2208 W .\nThe normalized partial derivatives of the FV are then: F \u22121/2 w \u2202L(X|W ) \u2202\u03c9 ."
            },
            {
                "heading": "5. Action recognition pipeline",
                "text": "The action recognition pipeline contains the underlying appearance features used to encode the video, the sequence encoding using the RNN-FV, and an SVM classifier on top."
            },
            {
                "heading": "5.1. Visual features",
                "text": "The RNN-FV is capable of encoding the sequence properties, and as underlying features, we rely on video encodings that are based on single frames or on fixed length blocks of frames. VGG Using the pre-trained VGG convolutional network [41], we extract a 4096-dimensional representation of each video frame. The VGG pipeline is used, namely, the original image is cropped in ten different ways into 224 by\n224 pixel images: the four corners, the center, and their xaxis mirror image. The mean intensity is then subtracted in each color channel and the resulting images are encoded by the network. The average of the 10 feature vectors obtained is then used as the single image representation. In order to speed up the method, the input video was sub-sampled, and one in every 10 frames was encoded. Empirically, we noticed that recognition performance was comparable to that of using all video frames. To further reduce run-time, the data dimensionality was reduced via PCA to 500D. In addition, L2 normalization was applied to each vector. All PCAs in this work were trained for each dataset and each training/test split separately, using only the training data. CCA Using the same VGG representation of video frames as mentioned above and the code of [18]1, we represented each frame by a vector as follows: we considered the common image-sentence vector space obtained by the CCA algorithm, using the best model (GMM+HGLMM) of [18] trained on the COCO dataset [25]. We mapped each frame to that vector space, getting a 4096-dimensional image representation. As the final frame representation, we used the first (i.e. the principal) 500 dimensions out of the 4096. For our application, the projected VGG representations were L2 normalized. The CCA was trained for an unrelated task of image to sentence matching, and its success, therefore, suggests a new application of transfer learning: from image annotation to action recognition. C3D While the representations above encode single frames, the C3D method [46] splits the video into sub-volumes that are encoded one by one. Following the recommended settings, we applied the Du Tran et al. pre-trained 3D convolutional neural network in order to extract 4096D representation to 16-frame blocks. The blocks are sampled with an 8 frame stride. Following feature extraction, PCA dimensionality reduction (500D) and L2 normalization were applied."
            },
            {
                "heading": "5.2. Network structure",
                "text": "Our RNN model consists of three layers: a 200D fullyconnected layer units with Leaky-Relu activation (\u03b1 = 0.1), a 200-units Long Short-Term Memory (LSTM) [7] layer, and a 500D linear fully-connected layer. Our network is trained for regression with the mean square error (MSE) loss function. Weight decay and dropouts were also applied. An improvement in recognition performance was noticed when the dropout rate was enlarged, up to a rate of 0.95, due to its ability to ensure the discriminative characterisics of each weight and hence also of each gradient."
            },
            {
                "heading": "5.3. Training and classification",
                "text": "We train the RNN to predict the next element in our video representation sequence, given the previous elements, as described in Sec.4.1. In our experiments, we use only\n1Available atwww.cs.tau.ac.il/~wolf/code/hglmm\nthe part of gradient corresponding to the weights of the last fully-connected layer. Empirically, we saw no improvement when using the partial derivatives with respect to weights of other layers. In order to obtain a fixed size representation, we average the gradients over all time steps. The gradient representation dimension is 500x201=100500, which is the number of weights in the last fully-connected layer. We then apply PCA to reduce the representation size to 1000D, followed by power and L2 normalization.\nVideo classification is performed using a linear SVM with a parameterC = 1. Empirically, we noticed that the the best recognition performance is obtained very quickly and hence early stopping is necessary. In order to choose an early stopping point we use a validation set. Some of the videos in the dataset are actually segments of the same original video, and are included in the dataset as different samples. Care was taken to ensure that no such similar videos are both in the training and validation sets, in order to guarantee that high validation accuracy will ensure good generalization and not merely over-fitting.\nAfter each RNN epoch, we extract the RNN-FV representation as described above, train a linear SVM classifier on the training set and evaluate the performance on the validation set. The early stopping point is chosen at the epoch with highest recognition accuracy on the validation set. After choosing our model this way, we train an SVM classifier on all training samples (training + validation samples) and report our performance on the test set."
            },
            {
                "heading": "6. Image-sentence retrieval",
                "text": "In the image-sentence retrieval tasks, vector representations are extracted separately for the sentences and the images. These representations are then mapped into a common vector space, where the two are being matched. [18] have presented a similar pipeline for GMM-FV. We replace this representation with RNN-FV.\nA sentence, being an ordered sequence of words, can be represented as a vector using the RNN-FV scheme. Given a sentence withN wordsw1, ..., wN , (wherewN is considered to be the period, namely awend special token), we treat the sentence as an ordered sequenceS = (w0, w1, ..., wN\u22121), wherew0 = wstart. An RNN is trained to predict, at each time stepi, the next wordwi+1 of the sentence, given the previous wordsw0, ..., wi. Therefore, given the input sequenceS, the target sequence would be: (w1, w2, ...wN ).\nThe training data may be any large set of sentences. These sentences may be extracted from the dataset of a specific benchmark, or, in order to obtain a generic representation, any external corpus, e.g., Wikipedia, may be used.\nThe two network alternatives are explored: classification and regression. As observed in the action recognition case, we did not benefit from extracting partial derivatives with\nrespect to the weights of the hidden layers, and hence we only use those of the output layer as our representation.\nWhen the RNN is trained for classification, each word in the dictionary is considered as a class. The input to the network is the word\u2019s embedding, a 300D vector in our case. The hidden layer is LSTM with 512 units, which is followed by a softmax output layer. This design creates two challenges. The first is dimensionality: the size of the softmax layer is the size of the dictionary,M , which is typically large. As a result,\u2207WL(X) has a high dimensionality. The second issue is with generalization capability: since the softmax layer is fixed, a network cannot handle a sentence containing a word that does not appear in its training data.\nWhen training the RNN for regression, the same 300D input is used, followed by an LSTM layer of size 100. The output layer, in this case, is fully-connected, where the (300 dimensional) word embedding of next word is predicted. We use no activation function at the output layer. Notice that the two issues pointed out regarding the classification RNN are not present in the regression case. First, the size D of the output layer depends only on the dimension of the word embedding. Second, the network can naturally handle unseen words, since it predicts vectors in the word vector space rather than an index of a specific word.\nFor matching images and text, each image is represented as a 4096-dimensional vector extracted using the 19-layer VGG, as described in Sec.5 1. The regularized CCA algorithm [47], where the regularization parameter is selected based on the validation set, is used to match the the VGG representation with the sentence RNN-FV representation. In the shared CCA space, the cosine similarity is used.\nWe explored several configurations for training the RNN. RNN training data We employed either the training data of each split in the respective benchmark, or the 2010-EnglishWikipedia-1M dataset made available by the Leipzig Corpora Collection [38]. This dataset contains 1 million sentences randomly sampled from English Wikipedia.Word embeddingA word was represented either by word2vec, or by the GMM+HGLMM representation of [18], projected to a 300D sentence to VGG-encoded-image CCA space. We made sure to match the training split according to the benchmark tested. Sentence sequence directionWe explored both the conventional left-to-right sequence of words and the reverse direction."
            },
            {
                "heading": "7. Experiments",
                "text": "We evaluated the effectiveness of the various pooling methods on two important yet distinct application domains: action recognition and image textual annotation and search.\nAs mentioned, applying the FIM normalization (Sec.4.3) did not seem to improve results. Another form of normalization we have tried, is to normalize each dimension of the gradient by subtracting its mean and\ndividing by its standard deviation. This also did not lead to an improved performance. Two normalizations that were found to be useful are the Power Normalization and the L2 Normalization, which were introduced in [36] (see Section2). Both are employed, using a constant\u03b1 = 1/2."
            },
            {
                "heading": "7.1. Action recognition",
                "text": "Our experiments were conducted on two large action recognition benchmarks. The UCF101 [43] dataset consists of 13,320 realistic action videos, collected from YouTube, and divided into 101 action categories. We use the three splits provided with this dataset in order to evaluate our results and report the mean average accuracy over these splits.\nThe HMDB51 dataset [20] consists of 6766 action videos, collected from various sources, and divided into 51 action categories. Three splits are provided as an official benchmark and are used here. The mean average accuracy over these splits is reported.\nTable1 compares our RNN-FV pooling method to Mean and GMM-FV pooling. Three sets of features, as described in Sec.5.1 are used: VGG coupled with PCA, VGG projected by the image to sentence matching CCA, and C3D.\nThe parameters were set on the validation split that we created for the provided training set. For GMM-FV, the only parameter isk, which is the number of components in the mixture. The validated values ofk were in the set {1, 2, 4, 8, 16, 32}. The parameter for RNN-FV was the stopping point of the RNN training, as described in Sec.5.3. Classification is conducted in all experiments using a multiclass (one-vs-all) linear SVM with C=1.\nAs can be seen in table1, the RNN-FV pooling outperformed the other pooling methods by a sizable margin. Another interesting observation is that with VGG frame representation, CCA outperformed PCA consistently in all pooling methods. Not shown is the performance obtained when using the activations of the RNN as a feature vector. These results are considerably worse than all pooling methods. Notice that the representation dimension of Mean pooling is 500 (like the features we used), the GMM-FV dimension is 2 \u00d7 k \u00d7 500, where k is the number of clusters and the RNN-FV dimension is 1000.\nTable2 compares our proposed RNN-FV method, combining multiple features together, with recently published methods on both datasets. The combinations were performed using early fusion, i.e, we concatenated the normalized low-dimensional gradients of the models and train multi-class linear SVM on the combined representation. We also tested the combination of our two best models with idt [52] and got state of the art results on both benchmarks. Interestingly, when training the RNNs on UCF101 and applying to encode HMDB51 videos, a comparable results of 66.99 (54.47 without idt) is obtained, which is also above current state of the art.\nDataset HMDB51 UCF101 Method MP GMM-FV RNN-FV MP GMMFV RNN-FV\nVGG PCA 42.16 36.8 45.62 75.51 76.53 79.29 VGG CCA 43.05 39.61 46.14 77.49 76.84 79.49 C3D 51.2 45.82 52.88 81.05 80.04 82.33\nTable 1. Comparing pooling techniques (mean pooling, GMM-FV and RNN-FV) on HMDB51 and UCF101. Three types of features are used: VGG-PCA, VGGCCA, and C3D. The table reports recognition average accuracy (higher is better).\nMethod HMDB51 UCF101\nidt [52] 57.2 85.9 idt + high-D encodings [32] 61.1 87.9 Two-stream CNN (2 nets) [40] 59.4 88 Multi-skip Feature Stacking [21] 65.4 89.1 C3D (1 net) [46] \u2013 82.3 C3D (3 nets) [46] \u2013 85.2 C3D (3 nets) + idt [46] \u2013 90.4 TDD (2 nets) [53] 63.2 90.3 TDD (2 nets) + idt [53] 65.9 91.5 stacked FV [33] 56.21 \u2013 stacked FV + idt [33] 66.78 \u2013 RNN-FV(C3D + VGG-CCA) 54.33 88.01 RNN-FV(C3D + VGG-CCA) + idt 67.71 94.08\nTable 2. comparison to the state of the art on UCF101 and HMDB51. In order to obtain the best performance, we combine, similar to all other contributions, multiple features. We also present a result where idt [52] is combined, similar to all other top results (Multi-skip extends idt). This adds motion based information to our method."
            },
            {
                "heading": "7.2. Image-sentence retrieval",
                "text": "The effectiveness of RNN-FV as sentence representation is evaluated on the bidirectional image and sentence retrieval task. We perform our experiments on three benchmarks: Flickr8K [8], Flickr30K [9], and COCO [25]. The datasets contain8, 000, 30, 000, and123, 000 images respectively. Each image is accompanied with 5 sentences describing the image content, collected via crowdsourcing.\nThe Flickr8k dataset is provided with training, validation, and test splits. For Flickr30K and COCO, no training splits are given, and we use the same splits used by [18].\nThere are three tasks in this benchmark: image annotation, in which the goal is to retrieve, given a query image, the five ground truth sentences; image search, in which, given a query sentence, the goal is to retrieve the ground truth image; and sentence similarity, in which the goal is, given a sentence, to retrieve the other four sentences describing the same image. Evaluation is performed using Recall@K, namely the fraction of times the correct result was ranked within the top K items. The median and mean rank of the first ground truth result are also reported. For the sentence similarity task, only mean rank is reported.\nAs mentioned in Sec.6, we explored RNN-FV based\non several RNNs. The first RNN is a generic one: it was trained with the Wikipedia sentences as training data and word2vec as word embedding. In addition, for each of the three datasets, we trained three RNNs with the dataset\u2019s training sentences as training data: one with word2vec as word embedding; one with the \"CCA word embedding\" derived from the semantic vector space of [18], as explained in Sec.6; and one with the CCA word embedding, and with feeding the sentences in reverse order. These RNNs were all trained for regression. For Flickr8K, we also trained an RNN for classification (with Flickr8K training sentences, and word2vec embedding). In this network, the softmax layer was of size 8,148, corresponding to the number of unique words in the Flickr8k dataset. Since the resulting number of weights of the output layer is around 4 million, we reduced the dimension of the gradient feature vector by random sampling of 72,000 coordinates. Training a classification model on the larger datasets is virtually impractical, since the number of unique words in these datasets is much higher, resulting in a very large softmax layer and a huge number of weights.\nIn the regression RNNs, we used an LSTM layer of size 100. We did not observe a benefit in using more LSTM units. We used the part of the gradient corresponding to all 30,300 weights of the output layer (including one bias per word2Vec dimension). In the case of the larger COCO dataset, due to the computational burden of the CCA calculation, we used PCA to reduce the gradient dimension from 30,300 to 20,000. PCA was calculated on a random subset of 300,000 sentences (around 50%) of the training set. We also tried PCA dimension reduction to a lower dimension of 4,096, for all three datasets. We observed no change in performance (Flickr8K) or slightly worse results (Flickr30K and COCO).\nThe number of RNN training epochs was 400, 100, 20, and 15, for the Flickr8k, Flickr30k, COCO and Wikipedia datasets respectively.\nTables3, 4 and5 show the results of the different RNNFV variants compared to the current state of the art methods. We also report results of combinations of models. Combining was done by averaging the image-sentence (or sentencesentence) cosine similarities obtained by each model.\nFirst, we see that regression-based RNN-FV should be preferred over the classification-based one. In addition to its lower dimension and natural handling of unseen words, the results obtained by regression RNN-FV are better. Sec-\nond, we notice the competitive performance of the model trained on Wikipedia sentences, which demonstrates the generalization power of the RNN-FV, being able to perform well on data different than the one which the RNN was trained on. Training using the dataset\u2019s sentences only slightly improves result, and not always. Improved results are obtained when using the CCA word embedding instead of word2vec. It is interesting to see the result of the \u201creverse\u201d model, which is on a par with the other models. It is somewhat complementary to the \u201cleft-to-right\u201d model, as the combination of the two yields somewhat improved results. Finally, the combination of RNN-FV with the best model (GMM+HGLMM) of [18] outperforms the current state of the art on Flickr8k, and is competitive on the other datasets."
            },
            {
                "heading": "8. Conclusions",
                "text": "This paper introduces a novel FV representation for sequences that is derived from RNNs. The proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive \u201cbag\u201d model typically used for conventional FVs.\nThe RNN-FV representation surpasses the state-of-theart results for video action recognition on two challenging datasets. When used for representing sentences, the RNNFV representation achieves state-of-the-art or competitiv results on image annotation and image search tasks. Since the length of the sentences in these tasks is usually short and, therefore, the ordering is less crucial, we believe that using the RNN-FV representation for tasks that use longer text will provide an even larger gap between the conventional FV and the RNN-FV.\nA transfer learning result from the image annotation task to the video action recognition task was shown. The con-\nceptual distance between these two tasks makes this result both interesting and surprising. It supports a human development-like way of training, in which visual labeling is learned through natural language, as opposed to, e.g., associating bounding boxes with nouns. While such training was used in computer vision to learn related image to text tasks, and while recently zero-shot action recognition was shown [11, 55], NLP to video action recognition transfer was never shown to be as general as presented here."
            },
            {
                "heading": "Acknowledgments",
                "text": "This research is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."
            }
        ],
        "references": [
            {
                "title": "Neural machine translation by jointly learning to align and translate",
                "author": [
                    "D. Bahdanau",
                    "K. Cho",
                    "Y. Bengio"
                ],
                "venue": "arXiv preprint arXiv:1409.0473 ",
                "citeRegEx": "1",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "The devil is in the details: an evaluation of recent feature encoding methods",
                "author": [
                    "K. Chatfield",
                    "V. Lempitsky",
                    "A. Vedaldi",
                    "A. Zisserman"
                ],
                "venue": "InBritish Machine Vision Conference ",
                "citeRegEx": "2",
                "shortCiteRegEx": null,
                "year": 2011
            },
            {
                "title": "and A",
                "author": [
                    "K. Chatfield",
                    "K. Simonyan",
                    "A. Vedaldi"
                ],
                "venue": "Zisserman. Return of the devil in the details: Delving deep into convolutional nets.arXiv preprint arXiv:1405.3531 ",
                "citeRegEx": "3",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Learning a recurrent visual rep resentation for image caption generation",
                "author": [
                    "X. Chen",
                    "C.L. Zitnick"
                ],
                "venue": "arXiv preprint arXiv:1411.5654",
                "citeRegEx": "4",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "and Y",
                "author": [
                    "J. Chung",
                    "C. Gulcehre",
                    "K. Cho"
                ],
                "venue": "Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling.arXiv preprint arXiv:1412.3555 ",
                "citeRegEx": "5",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "and T",
                "author": [
                    "J. Donahue",
                    "L.A. Hendricks",
                    "S. Guadarrama",
                    "M. Rohrbach",
                    "S. Venugopalan",
                    "K. Saenko"
                ],
                "venue": "Darrell. Long-term recurrent convolutional networks for visual recognition and description.arXiv preprint arXiv:1411.4389 ",
                "citeRegEx": "6",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Long short-term memor y",
                "author": [
                    "S. Hochreiter",
                    "J. Schmidhuber"
                ],
                "venue": "Neural computation , 9(8):1735\u20131780",
                "citeRegEx": "7",
                "shortCiteRegEx": null,
                "year": 1997
            },
            {
                "title": "Framing image description as a ranking task: Data",
                "author": [
                    "M. Hodosh",
                    "P. Young",
                    "J. Hockenmaier"
                ],
                "venue": "models and evaluation metrics.J. Artif. Intell. Res.(JAIR) , 47:853\u2013899",
                "citeRegEx": "8",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "Relations between two sets of variates",
                "author": [
                    "H. Hotelling"
                ],
                "venue": "Biometrika, pages 321\u2013377",
                "citeRegEx": "10",
                "shortCiteRegEx": null,
                "year": 1936
            },
            {
                "title": "Objects2action: Classifying and localizing actions witho ut any video example",
                "author": [
                    "M. Jain",
                    "J.C. van Gemert",
                    "T. Mensink",
                    "C.G.M. Snoek"
                ],
                "venue": "In Proceedings of the IEEE International Conference on Computer Vision",
                "citeRegEx": "11",
                "shortCiteRegEx": "11",
                "year": 2015
            },
            {
                "title": "3d convolutional neural networks for human action recognition",
                "author": [
                    "S. Ji",
                    "W. Xu",
                    "M. Yang",
                    "K. Yu"
                ],
                "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on , 35(1):221\u2013 231",
                "citeRegEx": "12",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "Deep visual-semantic align ments for generating image descriptions",
                "author": [
                    "A. Karpathy",
                    "L. Fei-Fei"
                ],
                "venue": "Technical report , Computer Science Department, Stanford University",
                "citeRegEx": "13",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Deep fragment em beddings for bidirectional image sentence mapping",
                "author": [
                    "A. Karpathy",
                    "A. Joulin",
                    "L. Fei-Fei"
                ],
                "venue": "arXiv preprint arXiv:1406.5679 ",
                "citeRegEx": "14",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "R",
                "author": [
                    "A. Karpathy",
                    "G. Toderici",
                    "S. Shetty",
                    "T. Leung"
                ],
                "venue": "Suktha nk r, and L. Fei-Fei. Large-scale video classification with convolutional neural networks. InComputer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on , pages 1725\u20131732. IEEE",
                "citeRegEx": "15",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "and R",
                "author": [
                    "R. Kiros",
                    "R. Salakhutdinov"
                ],
                "venue": "S. Zemel. Unifying visual-semantic embeddings with multimodal neural language models.arXiv preprint arXiv:1411.2539 ",
                "citeRegEx": "16",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Skip-thought vectors",
                "author": [
                    "R. Kiros",
                    "Y. Zhu",
                    "R. Salakhutdinov",
                    "R.S. Zemel",
                    "A. Torralba",
                    "R. Urtasun",
                    "S. Fidler"
                ],
                "venue": "a Xiv preprint arXiv:1506.06726 ",
                "citeRegEx": "17",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Associating neural word embeddings with deep image representations using fisher vectors",
                "author": [
                    "B. Klein",
                    "G. Lev",
                    "G. Sadeh",
                    "L. Wolf"
                ],
                "venue": "InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4437\u2013 4446",
                "citeRegEx": "18",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "M otion interchange patterns for action recognition in unconstrained videos",
                "author": [
                    "O. Kliper-Gross",
                    "Y. Gurovich",
                    "T. Hassner",
                    "L. Wolf"
                ],
                "venue": "InComputer Vision\u2013ECCV 2012 , pages 256\u2013269. Springer",
                "citeRegEx": "19",
                "shortCiteRegEx": null,
                "year": 2012
            },
            {
                "title": "and T",
                "author": [
                    "H. Kuehne",
                    "H. Jhuang",
                    "E. Garrote",
                    "T. Poggio"
                ],
                "venue": "Serre . HMDB: a large video database for human motion recognition. In Proc. IEEE Int. Conf. Comput. Vision ",
                "citeRegEx": "20",
                "shortCiteRegEx": null,
                "year": 2011
            },
            {
                "title": "and B",
                "author": [
                    "Z. Lan",
                    "M. Lin",
                    "X. Li",
                    "A.G. Hauptmann"
                ],
                "venue": "Raj. Beyond gaussian pyramid: Multi-skip feature stacking for action recognition.arXiv preprint arXiv:1411.6660 ",
                "citeRegEx": "21",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "On space-time interest points",
                "author": [
                    "I. Laptev"
                ],
                "venue": "Int. J. Comput. Vision, 64(2):107\u2013123",
                "citeRegEx": "22",
                "shortCiteRegEx": null,
                "year": 2005
            },
            {
                "title": "Learning realistic human actions from movies",
                "author": [
                    "I. Laptev",
                    "M. Marszalek",
                    "C. Schmid",
                    "B. Rozenfeld"
                ],
                "venue": "Proc. IEEE Conf. Comput. Vision Pattern Recognition , pages 1\u20138",
                "citeRegEx": "23",
                "shortCiteRegEx": null,
                "year": 2008
            },
            {
                "title": "In defense of word embedding for generic text representation",
                "author": [
                    "G. Lev",
                    "B. Klein",
                    "L. Wolf"
                ],
                "venue": "Natural Language Processing and Information Systems , pages 35\u201350. Springer International Publishing",
                "citeRegEx": "24",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "D",
                "author": [
                    "T.-Y. Lin",
                    "M. Maire",
                    "S. Belongie",
                    "J. Hays",
                    "P. Perona"
                ],
                "venue": "R amanan, P. Doll\u00e1r, and C. Zitnick. Microsoft coco: Common objects in context. In D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, editors, Computer Vision \u2013 ECCV 2014 , volume 8693 ofLecture Notes in Computer Science , pages 740\u2013755. Springer International Publishing",
                "citeRegEx": "25",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Multimodal convolution al neural networks for matching image and sentence",
                "author": [
                    "L. Ma",
                    "Z. Lu",
                    "L. Shang",
                    "H. Li"
                ],
                "venue": "arXiv preprint arXiv:1504.06063 ",
                "citeRegEx": "26",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Deep captioning with multimodal recurrent neural networks (m-rnn)",
                "author": [
                    "J. Mao",
                    "W. Xu",
                    "Y. Yang",
                    "J. Wang",
                    "A. Yuille"
                ],
                "venue": "arXiv preprint arXiv:1412.6632 ",
                "citeRegEx": "27",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Explain images with multimodal recurrent neural networks",
                "author": [
                    "J. Mao",
                    "W. Xu",
                    "Y. Yang",
                    "J. Wang",
                    "A.L. Yuille"
                ],
                "venue": "arXiv preprint arXiv:1410.1090 ",
                "citeRegEx": "28",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Deep correlation for matching",
                "author": [
                    "F.Y.K. Mikolajczyk"
                ],
                "venue": "ima ges and text. 2015.2,",
                "citeRegEx": "29",
                "shortCiteRegEx": "29",
                "year": 2015
            },
            {
                "title": "Distributed representations of words and phrases and their compositionality",
                "author": [
                    "T. Mikolov",
                    "I. Sutskever",
                    "K. Chen",
                    "G.S. Corrado",
                    "J. Dean"
                ],
                "venue": "InAdvances in Neural Information Processing Systems , pages 3111\u20133119",
                "citeRegEx": "30",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "Deep sentence embedding using the long short term memory network: Analysis and application to information retrieval",
                "author": [
                    "H. Palangi",
                    "L. Deng",
                    "Y. Shen",
                    "J. Gao",
                    "X. He",
                    "J. Chen",
                    "X. Song",
                    "R. Ward"
                ],
                "venue": "arXiv preprint arXiv:1502.06922 ",
                "citeRegEx": "31",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice",
                "author": [
                    "X. Peng",
                    "L. Wang",
                    "X. Wang",
                    "Y. Qiao"
                ],
                "venue": "arXiv preprint arXiv:1405.4506",
                "citeRegEx": "32",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Action recognition with stacked fisher vectors",
                "author": [
                    "X. Peng",
                    "C. Zou",
                    "Y. Qiao",
                    "Q. Peng"
                ],
                "venue": "Computer Vision\u2013ECCV 2014 , pages 581\u2013595. Springer",
                "citeRegEx": "33",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Fisher kernels on visual voca bularies for image categorization",
                "author": [
                    "F. Perronnin",
                    "C. Dance"
                ],
                "venue": "Computer Vision and Pattern Recognition, 2007. CVPR\u201907. IEEE Conference on , pages 1\u20138. IEEE",
                "citeRegEx": "34",
                "shortCiteRegEx": null,
                "year": 2007
            },
            {
                "title": "Largescale image retrieval with compressed fisher vectors",
                "author": [
                    "F. Perronnin",
                    "Y. Liu",
                    "J. S\u00e1nchez",
                    "H. Poirier"
                ],
                "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 3384\u20133391. IEEE",
                "citeRegEx": "35",
                "shortCiteRegEx": null,
                "year": 2010
            },
            {
                "title": "Improving the fisher kernel for large-scale image classification",
                "author": [
                    "F. Perronnin",
                    "J. S\u00e1nchez",
                    "T. Mensink"
                ],
                "venue": "Computer Vision\u2013ECCV 2010 , pages 143\u2013156. Springer",
                "citeRegEx": "36",
                "shortCiteRegEx": null,
                "year": 2010
            },
            {
                "title": "and S",
                "author": [
                    "B. Plummer",
                    "L. Wang",
                    "C. Cervantes",
                    "J. Caicedo",
                    "J. Hockenmaier"
                ],
                "venue": "Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to sentence models.arXiv preprint arXiv:1505.04870 ",
                "citeRegEx": "37",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Corpus portal for search in monolingual corpora",
                "author": [
                    "U. Quasthoff",
                    "M. Richter",
                    "C. Biemann"
                ],
                "venue": "Proceedings of the fifth international conference on language resources and evalua tion, volume 17991802",
                "citeRegEx": "38",
                "shortCiteRegEx": null,
                "year": 2006
            },
            {
                "title": "Fisher vector faces in the wild",
                "author": [
                    "K. Simonyan",
                    "O.M. Parkhi",
                    "A. Vedaldi",
                    "A. Zisserman"
                ],
                "venue": "Proc. BMVC, volume 1, page 7",
                "citeRegEx": "39",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "Two-stream convolutiona l networks for action recognition in videos",
                "author": [
                    "K. Simonyan",
                    "A. Zisserman"
                ],
                "venue": "Advances in Neural Information Processing Systems , pages 568\u2013576",
                "citeRegEx": "40",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Very deep convolutional networks for large-scale image recognition",
                "author": [
                    "K. Simonyan",
                    "A. Zisserman"
                ],
                "venue": "CoRR, abs/1409.1556",
                "citeRegEx": "41",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Grounded compositional semantics for finding and describing images with sentences",
                "author": [
                    "R. Socher",
                    "Q. Le",
                    "C. Manning",
                    "A. Ng"
                ],
                "venue": "InNIPS Deep Learning Workshop ",
                "citeRegEx": "42",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "UCF101: A dataset of 101 human action classes from videos in the wild",
                "author": [
                    "K. Soomro",
                    "A.R. Zamir",
                    "M. Shah"
                ],
                "venue": "CRCV- TR-12-01,",
                "citeRegEx": "43",
                "shortCiteRegEx": "43",
                "year": 2012
            },
            {
                "title": "Sequence to seque nce learning with neural networks",
                "author": [
                    "I. Sutskever",
                    "O. Vinyals",
                    "Q.V. Le"
                ],
                "venue": "Advances in neural information processing systems , pages 3104\u20133112",
                "citeRegEx": "44",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Convolutional learning of spatio-temporal features",
                "author": [
                    "G.W. Taylor",
                    "R. Fergus",
                    "Y. LeCun",
                    "C. Bregler"
                ],
                "venue": "Computer Vision\u2013ECCV 2010 , pages 140\u2013153. Springer",
                "citeRegEx": "45",
                "shortCiteRegEx": null,
                "year": 2010
            },
            {
                "title": "and M",
                "author": [
                    "D. Tran",
                    "L. Bourdev",
                    "R. Fergus",
                    "L. Torresani"
                ],
                "venue": "Palu ri. Learning spatiotemporal features with 3d convolutional ne tworks. arXiv preprint arXiv:1412.0767 ",
                "citeRegEx": "46",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Canonical ridge and econometrics of joint pro duction",
                "author": [
                    "H. Vinod"
                ],
                "venue": "Journal of Econometrics , 4(2):147 \u2013 166",
                "citeRegEx": "47",
                "shortCiteRegEx": null,
                "year": 1976
            },
            {
                "title": "Show and tell: A neural image caption generator",
                "author": [
                    "O. Vinyals",
                    "A. Toshev",
                    "S. Bengio",
                    "D. Erhan"
                ],
                "venue": "arXiv preprint arXiv:1411.4555",
                "citeRegEx": "48",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Action recognition by dense trajectories",
                "author": [
                    "H. Wang",
                    "A. Klaser",
                    "C. Schmid",
                    "C. Liu"
                ],
                "venue": "Proc. IEEE Conf. Comput. Vision Pattern Recognition , pages 3169\u20133176",
                "citeRegEx": "49",
                "shortCiteRegEx": null,
                "year": 2011
            },
            {
                "title": "Dense trajectories and motion boundary descriptors for action recog nition",
                "author": [
                    "H. Wang",
                    "A. Kl\u00e4ser",
                    "C. Schmid",
                    "C.-L. Liu"
                ],
                "venue": "Int. J. Comput. Vision , 103(1):60\u201379",
                "citeRegEx": "50",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "Action Recognition with Improved Trajectories",
                "author": [
                    "H. Wang",
                    "C. Schmid"
                ],
                "venue": "InInternational Conference on Computer Vision,",
                "citeRegEx": "51",
                "shortCiteRegEx": "51",
                "year": 2013
            },
            {
                "title": "Action recognition with improved trajectories",
                "author": [
                    "H. Wang",
                    "C. Schmid"
                ],
                "venue": "InComputer Vision (ICCV), 2013 IEEE International Conference on , pages 3551\u20133558. IEEE",
                "citeRegEx": "52",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "Action recognition with trajectory-pooled deep-convolutional descriptors",
                "author": [
                    "L. Wang",
                    "Y. Qiao",
                    "X. Tang"
                ],
                "venue": "arXiv preprint arXiv:1505.04868 ",
                "citeRegEx": "53",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "A comparative study of encoding",
                "author": [
                    "X. Wang",
                    "L. Wang",
                    "Y. Qiao"
                ],
                "venue": "pooling and normalization methods for action recog nition. In Computer Vision\u2013ACCV 2012 , pages 572\u2013585. Springer",
                "citeRegEx": "54",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "Semantic embedding space for zero-shot action recognition",
                "author": [
                    "X. Xu",
                    "T.M. Hospedales",
                    "S. Gong"
                ],
                "venue": "CoRR, abs/1502.01540",
                "citeRegEx": "55",
                "shortCiteRegEx": null,
                "year": 2015
            }
        ],
        "abstractText": "Recurrent Neural Networks (RNNs) have had considerable success in classifying and predicting sequences. We demonstrate that RNNs can be effectively used in order to encode sequences and provide effective representations . The methodology we use is based on Fisher Vectors, where the RNNs are the generative probabilistic models and the partial derivatives are computed using backpropagation. State of the art results are obtained in two central but dista nt tasks, which both rely on sequences: video action recognition and image annotation. We also show a surprising transfer learning result from the task of image annotation to the task of video action recognition."
    },
    {
        "title": "Classifying Emotions in Customer Support Dialogues in Social Media",
        "sections": [
            {
                "heading": null,
                "text": "Proceedings of the SIGDIAL 2016 Conference, pages 64\u201373, Los Angeles, USA, 13-15 September 2016. c\u00a92016 Association for Computational Linguistics\nProviding customer support through social media channels is gaining increasing popularity. In such a context, automatic detection and analysis of the emotions expressed by customers is important, as is identification of the emotional techniques (e.g., apology, empathy, etc.) in the responses of customer service agents. Result of such an analysis can help assess the quality of such a service, help and inform agents about desirable responses, and help develop automated service agents for social media interactions. In this paper, we show that, in addition to text based turn features, dialogue features can significantly improve detection of emotions in social media customer service dialogues and help predict emotional techniques used by customer service agents."
            },
            {
                "heading": "1 Introduction",
                "text": "An interesting use case for social media is customer support that can now take place over public social media channels. Using this medium has its advantages as described, for example, in (DeMers, 2014): Customers appreciate the simplicity and immediacy of social media conversations, the ability to reach real human beings, the transparency, and the feeling that someone listens to them. Businesses also benefit from the publicity of giving good services almost in real-time, online, building an online community of customers and encouraging more brand mentions in social media. A recent study shows that one in five (23%) customers in the U.S. say they have used social media for customer service in2014, up from17% in 20121. Obviously, companies hope that such\n1http://about.americanexpress.com/ news/docs/2014x/2014-Global-Customer-\nuses are associated with a positive experience. Yet there are limited tools for assessing this. In this paper, we analyze customer support dialogues using the Twitter platform and show the utility of such analyses.\nThe particular aspect of such dialogues that we concentrate on isemotions. Emotions are a cardinal aspect of inter-personal communication: they are an implicit or explicit part of essentially any communication, and of particular importance in the setting of customer service, as they relate directly to customer satisfaction and experience (Oliver, 2014). Typical emotions expressed by customers in the context of social media service dialogues include anger and frustration, as well as gratitude and more (Gelbrich, 2010). On the other hand, customer service agents also express emotions in service conversations, for example apology or empathy. However, it is important to note that emotions expressed by service agents are typically governed by company policies that specify which emotions should be expressed in which situation (Rafaeli and Sutton, 1987). This is why we talk in this paper about agent emotionaltechniques rather than agent emotions.\nConsider, for example, the real (anonymized) Twitter dialogue depicted in Figure 1. In this dialogue, customer disappointment is expressed in the first turn (\u2019Bummer. =/\u2019), followed by customer support empathy (\u2019Uh oh!\u2019). Then in the last two turns both customer and support express gratitude.\nThe analysis of emotions being expressed in customer support conversations can take two applications: (1) to discern and compute quality of service indicators and (2) to provide real-time clues to customer service agents regarding the cus-\nService-Barometer-US.pdf\n64\nGot excited to pick up the latest bundle since it was on sale today, but now I can\u2019t download it at all. Bummer. =/\nYeah, no problems there. The error is coming when I actually try to download the games. Error code: 412344\nUh oh! To check, were you able to purchase that title? Let\u2019s confirm by signing in at http://t.co/53fsdfd real quick.\nAppreciate that! Let\u2019s power cycle and unplug modem/router for 2 mins then try again.\nSeems to be working now. Weird. I tried that 3 different times earlier. Thanks.\nOdd, but glad to hear that\u2019s sorted! Happy gaming, and we\u2019ll be here to help if any other questions or concerns arise.\nFigure 1: Example of customer service dialogue that was initiated by a customer (left side), and the agent responses (right side).\ntomer emotion expressed in a conversation. A possible application here is recommending to customer service agents what should be their emotional response (for example, in each situation, should they apologize, should they thank the customer, etc.)\nAnother interesting trend in customer service, in addition to the use of social media described above, is the automation of various functions of customer interaction. Several companies are developing text-based chat agents, typically accessible through corporate web sites, and partially automatized: In these platforms, a computer program handles simple conversations with customers, and more complicated dialogues are transferred to a human agent. Such partially automated systems are also in use for social media dialogues. The automation in such systems helps save human resources and, with further development based on Artificial Intelligence, more automation in customer service chats is likely to appear. Given the importance of emotions in service dialogues, such systems will benefit from the ability to detect (customer) emotions and will need to guide employees (and machines) regarding the right emotional technique in various situations (e.g., apologizing at the right point).\nThus, our goal, in this paper, is to show that the\nfunctionality of guiding employees regarding appropriate responses can be developed based on the analysis of textual dialogue data. We show first that it is possible to automatically detect emotions being expressed and, second that it is possible to predict the emotional technique that is likely to be used by a human agent in a given situation. This analysis reflects our ultimate goal: To enable a computer system to discern the emotions expressed by human customers, and to develop computerized tools that mimic the emotional technique used by a human customer service agent in a particular situation.\nWe see the main contributions of this paper as follows: (1) To our knowledge, this is the first research focusing on automatic analysis of emotions expressed in customer service provided through social media. (2) This is the first research us-\ning unique dialogue features (e.g., emotions ex-\npressed in previous dialogue turns by the agent\nand customer, time between dialogue turns) to im-\nprove emotion detection. (3) This is the first research studying the prediction of the agent emotional techniques to be used in the response to customer turns.\nThe rest of this paper is organized as follows. We start by reviewing the related work and a description of the data that we collected. Then we formally define the methodology for detection and prediction of emotion expression in dialogues. Finally, we describe our experiments, evaluate the various models, conclude and suggest future directions."
            },
            {
                "heading": "2 Related Work",
                "text": ""
            },
            {
                "heading": "2.1 Emotion Detection",
                "text": "Approaches to categorical emotion classification often employ machine learning classifiers, and SVM has typically outperformed other classifiers. In (Mohammad, 2012; Roberts et al., 2012; Qadir and Riloff, 2014) a series of binary SVM classifiers (one for each emotion) were trained over datasets from different domains (news headlines, social media). These works utilize unigrams and bigrams among other lexical based features (e.g., utilizing the NRC emotion lexicon (Mohammad and Turney, 2013)) and punctuation based features. In our work, we also used an SVM classifier, however, while these works aim at classifying single posts (i.e., sentence, tweet, etc.) without context, our work utilizes the context while con-\nsidering dialogues. The work in (Hasegawa et al., 2013) showed how to predict and elicit emotions in online dialogues. Their approach for emotion classification is different from ours, for example they only considered the last turn as informative (we consider the full context of the dialogue), and focused on eliciting emotions, while we focus on predicting the agent emotional technique."
            },
            {
                "heading": "2.2 Emotion Expression Prediction",
                "text": "The works in (Skowron, 2010) and (D\u2019Mello et al., 2009) presented dialogue systems that sense the user emotions, such that the system further optimizes its affect response. Both systems use rulebased approaches to generate responses, however, the authors do not discuss how they developed the rules.\nIt is worth mentioning the works in (Ritter et al., 2011; Sordoni et al., 2015) that are focused on data-driven response generation in the context of dialogues in social media. These works generated general responses, while we focused on predicting the appropriate emotional response."
            },
            {
                "heading": "2.3 Emotions in Written Customer Service Interactions",
                "text": "In the domain of customer support, several papers studied emotions as part of written interactions. The work in (Gupta et al., 2013), analyzed emotions in textual email communications and the authors focused on prioritizing customer support emails based on detected emotions. In the setting of online customer service (chats), in (Zhang et al., 2011) the authors studied the impact of emotional text on the customer\u2019s perception of the service agent. To extract the emotions, the authors used relatively basic features such as emoticons, exclamation marks, all caps, and some internet acronyms (such as \u2019lol\u2019 or \u2019imho\u2019).\nEmotion detection is also applied to the domain of call centers (Vidrascu and Devillers, 2005; Morrison et al., 2007) and this differs from our focus since call center data are voice, and, thus, emotion detection is mainly based on paralinguistic aspects rather than on the text. In addition, if the textual part is considered, then the texts are transcripts of calls that are very different from written text (Wallace Chafe, 1987), and even more different from the social media setting where the dialogue is fully public."
            },
            {
                "heading": "3 Data",
                "text": "In this section we describe the data collection process and provide some statistics about the Twitter dialogue dataset we have collected."
            },
            {
                "heading": "3.1 Data Collection",
                "text": "Companies that utilize the Twitter platform as a channel for customer service use a dedicated Twitter account which provides real-time support by monitoring tweets that customers address to it. At the same time corporate support agents reply to these tweets also through the Twitter platform. A customer and an agent, can use the Twitter reply mechanism to discuss until the issue is solved ( .g., a solution is provided, or the customer is directed to another channel), or until the customer is no longer active.\nIn the present work, we define a dialogue to be a sequence of turns between a specific customer and an agent, where the customer initiates the first turn. Consecutive posts of the same party (customer or agent) uninterrupted by the other party, are considered as a single turn (even if there are several tweets). Given the nature of customer support services, we assume the last turn in the dialogue is an agent turn (e.g., \u201cYou\u2019re very welcome. :) Hit us back any time you need support\u201d). Thus, we expect an even number of turns in the dialogue. We filtered out dialogues in which more than one customer or one agent are involved. Formally, we define a dialogue to be an ordered list of turns[t1, t2, \u00b7 \u00b7 \u00b7 , tn] where odd turns are customer turns, and even turns are agent turns, andn is even.\nEach turnti is a tuple consisting of{turn number, timestamp, content} whereturn numberrepresents the sequential position of the turn in the dialogue,timestampcaptures the time the message was published on Twitter, andcontentis the textual message."
            },
            {
                "heading": "3.2 Data Statistics",
                "text": "We gathered data for two North America based customer support services Twitter accounts that provide support for customers from North America (so tweets are in English). One service is for general customer care (denoted asGen), and the other is for technical customer support (denoted asTech). We extracted this data from December 2014 until June2015. Specifically, for each customer that posted a tweet to the customer support accounts, we searched for the previous, if any, turn\n# Dialogues Mean # turns AVG word count Gen 4243 4.83 16.69 Tech 4016 6.81 14.28\nTable 1: Descriptive statistics of customer service dialogues extracted from Twitter.\nto which it replied. Given this method we traced back previous turns and reconstructed entire dialogues.\nTable 1 summarizes some statistics about the collected data, and Figure 2 depicts the frequencies of dialogue lengths which follow a power-law relationship. Table 1 shows differences between the two services; the dialogues inTechtend to be longer (i.e., typically include more turns), with an average of6.81 turns vs. average of4.83 turns for Gen.\nAs most of the dialogues include at most8 turns (88% and76% for GenandTech, respectively), we removed dialogues longer than8 turns. In addition, we removed dialogues that contained only2 turns as these are too short to be meaningful as the customer never replied or provided more details about the issue. After applying these preprocessing steps, we had1189 dialogues ofGensupport, and1224 dialogues ofTechsupport."
            },
            {
                "heading": "4 Methodology",
                "text": "The first objective of our work is to detect emotions expressed in customer turns and the second is to predict the emotional technique in agent turns. We treated these two objectives as two classification tasks. We generated a classifier for each task, where the classification output of one classifier can be part of the input to the other classifier. While both classifiers work at the level of turns, i.e., classify the current turn to emotions ex-\npressed in it, they are inherently different. When detecting emotions in a customer turn, the turn\u2019s content is available at classification time (as well as the history of the dialogue) - meaning, the customer has already provided her input and the system must now understand what is the emotion being expressed. Whereas, when predicting the emotional technique for an agent turn, the turn\u2019s content is not available during classification time, but only the agent action and the history of the dialogue since the agent did not respond yet. This difference stems from the fact that in order to train an automated service agent to respond based on customer input, the agent\u2019s emotional technique needs to be computed before the agent generates its response sentence.\nWe defined a different set of relevant emotion\nclasses for each party in the dialogue (customer or\nagent), based on our above survey of research on customer service (e.g., (Gelbrich, 2010)). Relevant customer emotions to be detected are:Confusion, Frustration, Anger, Sadness, Happiness, Hopefulness, Disappointment, Gratitude,andPoliteness. Relevant agent emotional techniques to be predicted are:Empathy, Gratitude, Apology, andCheerfulness.\nWe utilized the context of the dialogue to extract informative features that we refer to asdialogue features. Using these features for emotion classification in written dialogues is novel, and as our experimental results show, it improves performance compared to a model based only on features extracted from the turn\u2019s text."
            },
            {
                "heading": "4.1 Features",
                "text": "We used the following features in our models."
            },
            {
                "heading": "4.1.1 Dialogue Features",
                "text": "Comprises three contextual feature families:integral, emotional, and temporal. A feature can be global, namely its value is constant across an entire dialogue or it can be alocal, meaning that its value may change at each turn. In addition, a feature can behistorical (as will be discussed below).\nThe integral family of features includes three sets of features:\n1. Dialogue topic: a set ofglobal binary features representing the intent of the customer who initiated the support inquiry. Multiple intents can be assigned to a dialogue from a taxonomy of popular topics, which are adapted to the specific service. Examples of topics includeac-\ncount issues, payments, technical problemand more2. This feature set captures the notion that customer emotions are influenced by the event that led the customer to contact the customer service (Steunebrink et al., 2009).\n2. Agent essence: a set of local binary features that represent the action used by the agent to address the last customer turn, independently\nof any emotional technique expressed. We refer\nto these actions as theessenceof the agent turn.\nMultiple essences can be assigned to an agent\nturn from a predefined taxonomy. For instance,\n\u201casking for more information\u201dand\u201coffering a solution\u201d are possible essences3. This feature\nset captures the notion that customer emotions\nare influenced by actions of agents (Little et al.,\n2013).\n3. Turn number: a local categorical feature representing the number of the turn.\nTheemotionalfamily of features includesAgent emotionandCustomer emotion: these two sets of\nlocal binary features represent emotions predicted for previous turns. Our model generates predictions of emotions for each customer and agent turn, and uses these predictions as features to classify a later customer or agent turn with emotion expression.\nThe temporal family of features includes the following features extracted from the timeline of the dialogue:\n1. Customer/agent response time: two local features that indicate the time elapsed between the timestamp of the last customer/agent turn and the timestamp of the subsequent turn. This is a categorical feature with valueslow, medium or high (using categorical values yielded better results than using a continuous value).\n2. Median customer/agent response time: two local categorical features defined as the median of thecustomer/agent response timespreceding the current turn. The categories are the same as the previous temporal features. 2Currently this feature is not supported in social media. In other channels, for example, customer support on the phone, the customer is requested to provide a topic before she is connected to a support agent (usually using an IVR system). As this feature is inherent in other customer support channels, we assume that in the future it will also be supported in social media.\n3We assume that if the agent is human, then this input is known to her e.g., based on company policies. For the automated service agent case, we assume that the dialogue system will manage and provide this input.\nt i\u22123\nagent\nt i\u22122\ncustomer\nt i\ncustomer\nt i\u22121\nagent\n{Agent essence} {Agent Emotion}\n{Customer Emotion}\n{Agent Emotion}\n{Customer Emotion}\n{Agent essence}\nFigure 3: Example forHistorical features propagation for customer turn,ti, with history = 3. Whenhistory = 1, thehistorical features are the agent essenceof turn ti\u22121 and theagent emotion predicted for turnti\u22121 (purple solid line). When history = 2, we also add thecustomer emotion detected in turnti\u22122 (red dashed line). Finally, if we sethistory = 3, then we also add theagent essenceof turn ti\u22123 and theagent emotionpredicted for turnti\u22123 (blue dotted line), so in total we have5 historical features. Notice that the customer emotionandagent essencef atures have different values based on their turn number.\n3. Day of week: a local categorical feature indicating the day of the week when the turn was published [Monday - Sunday]. This feature captures the effects of weekend versus weekday influences on emotions (Ryan et al., 2010).\nWhen representing a turn,ti as a feature vector, we added some features originating in previous turnsj < i to ti. These features, that arehistorical, include theemotionalfeatures family andlocal integralfeatures (namelyagent emotions, customer emotionsandagent essence). We do not include theturn numberof previous turns, as this is dependent on the turn number ofti. We denote these features ashistorical features. The value of history, that is a parameter of our models, defines the number of sequential turns that precede ti which propagatehistorical features toti.\nFigure 3 shows an example of theistorical features in relation to the classification of customer turn ti, for historysize between1 and3."
            },
            {
                "heading": "4.1.2 Textual Features",
                "text": "These features are extracted from the text of a customer turn, without considering the context of the dialogue. We use various state-of-the-art text based features that have been shown to be effective for the social media domain (Mohammad, 2012;\nRoberts et al., 2012). These features include various n-grams, punctuation and social media features. Namely,unigrams, bigrams, NRC lexicon features(number of terms in a post associated with each affect label in NRC lexicon), and presence of exclamation marks, question marks, usernames, links, happy emoticons, andsad emoticons. We note that these are the features we used in our baseline model detailed below, in the description of our experiments."
            },
            {
                "heading": "4.2 Turn Classification System",
                "text": "For both of the agent and customer turn classification tasks, we implemented two different models which incorporate all of the feature sets we have detailed above. We considered these tasks as multi-label classification tasks. This captures the notion that a party can express multiple emotions (e.g., confusion and anger) in a turn. We chose to use a problem transformation approach which maps the multi-label classification task into several binary classification tasks, one for each emotion class which participates in the multi-label problem (Tsoumakas and Katakis, 2006). For each emotione, a binary classifier is created using the one-vs.-all approach which classifies a turn as expressinge or not. A test sample is fully classified by aggregating the classification results from all independent binary classifiers. We next define our two modeling approaches."
            },
            {
                "heading": "4.2.1 SVM Dialogue Model",
                "text": "In our first approach we trained an SVM classifier for each emotion class as explained above. The feature vector we used to represent a turn incorporatesdialogueandtextual features. Thehistory size is also a parameter of this model. Feature extraction for a training/testing feature vector representing a turnti, works as follows. Textual featuresare extracted forti if it is a customer turn, or for ti\u22121 if it is an agent turn (recall that the system does not have the content of agent turn ti at classification time). Thetemporal features are also extracted using time lapse values between previous turns as explained above. As discussed above,agent essenceis assumed to be an input to our module, whileagent emotionandcustomer emotionfeatures are propagated from classification results of previous turns during testing (or from ground truth labels during training), where the number of previous turns is determined according to the value ofhistory. Thesehistorical\nfeatures are also appended to the feature vector of ti, similarly to (Kim et al., 2010) where this method was used for classifying dialogue acts."
            },
            {
                "heading": "4.2.2 SVM-HMM Dialogue Model",
                "text": "Our second approach to classifying dialogue turns is to use a sequence classification method (SVMHMM), which classifies a sample sequence into its most probable tag sequence. For instance (Kim et al., 2010; Tavafi et al., 2013) used SVM-HMM and Conditional Random Fields for dialogue act classification. Since emotions expressed in customer and agent turns are different, we treated them as different classification tasks (like in our previous approach) and trained a separate classifier for each emotion. We made the following changes when using SVM-HMM:\n(1) We treated the emotion classification problem of turnti as a sequence classification problem of the sequencet1, t3, ..., ti (i.e., only customer turns) if ti is a customer turn andt2, t4, ..., ti (i.e., only agent turns) if it is an agent turn. (2) The SVM-HMM classifier generates models that are isomorphic to akth-order hidden Markov model. Under this model, dependency in past classification results is captured internally by modeling transition probabilities between emotion states. Thus, we removed historicalcustomer emotion (resp.agent emotion) feature sets when representing a feature vector for a customer (resp. agent) turn. (3) We note that in our setting we provide classifications in real-time during the progress of the dialogue, so at classification time we have access only to previous turns and global information, and we cannot change classification decisions for past turns. Thus, we tagged a test turn,ti, by classifying the sequence which ends inti. Then, ti was tagged with its sequence classification result."
            },
            {
                "heading": "5 Experiments",
                "text": ""
            },
            {
                "heading": "5.1 Experimental Setup",
                "text": "A first step in building a classification model is to obtain ground truth data. For this, we sampled dialogues from our dataset, as detailed in Table 2, based on each data source\u2019s dialogue length distribution. This sample included1056 customer turns and1056 agent turns in total. The sampled dialogues were tagged using Amazon Mechanical Turk4. Each dialogue was tagged by five different Mechanical Turk\u2019s master level judges. Each\n4https://www.mturk.com/\njudge performed the following tagging tasks given the full dialogue:\n1. Emotion tagging: indicate the intensity of emotion expressed in each turn (customer or agent) for each emotion, on a scale of ([0...5]), such that0 defines no emotion,1 a low emotion intensity and5 a high emotion intensity. The intraclass correlation (ICC) among the judges was0.53 which indicates a moderate agreement which is common in this setting (LeBreton and Senter, 2007).\n2. Dialogue topic tagging: select one or several topic(s), to represent the customer\u2019s intent. The topics are based on a taxonomy of popular customer support topics (Zeithaml et al., 2006): Account issues, Pricing, Payments, Customer service, Customer experience, Technical problem, Technical question, Order and delivery issues, Behavior of a staff member, Company policy issuesandGeneral statement.\n3. Agent essence tagging: select one or several of the following for each agent\u2019s turn, to describe the agent\u2019s action in the specific turn:Recognizing the issue raised, Asking for more information, Providing an explanation, Offering a solution, General statementandAssurance of efforts. The taxonomy is based on (Zomerdijk and Voss, 2010).\nWe generated true binary labels from the emotion tagging. For turnti, we considered it to express emotione if tag(e, ti) \u2265 2 wheretag(e, t) is the average judges\u2019 tag value ofe in t. This process generated the class sizes detailed in Table 3. Dialogue topic tagging was converted to binary features representing the top-2 selected topics. Agent essencefeature set representation for each turn was defined analogously. The temporal response time values were translated to low/medium/high categorical values according to their relation to the33-th and66-th percentiles.\nWe evaluated our methods by using leave-onedialogue-out cross-validation (as in (Kim et al., 2010)), over the whole dataset (for the two cus-\ntomer service data sources together). Each test dialogue was classified by its order of turns, where each turn type (customer or agent) is classified by its corresponding classifier.\nOur baseline in all experiments is an SVM classifier that uses only thetextual featuresdescribed above, which do not utilize the dialogue context. This was used as a state-of-the-art single sentence emotion detection approach in many cases, e.g., (Mohammad, 2012; Roberts et al., 2012; Qadir and Riloff, 2014) and more. As described above, agent turn emotion prediction is performed before its content is known. Thus, the baseline representation of an agent turn consisted oftextual featuresextracted from its preceding customer turn. We evaluated each emotion\u2019s classification performance by using precision (P ), recall (R) and F1score (F ). We evaluated the total performance for all emotion classes usingmicro and macro averages. We used Liblinear5 as an SVM implementation and SVM-HMM6 for sequence classification. Additionally, we used ClearNLP7 for textual features extraction."
            },
            {
                "heading": "5.2 History Size Impact",
                "text": "Sincehistory size is a parameter of our models, we first tested the classification results for all possible history sizes (given that that maximum dialogue size in our dataset is8). For each task and for each possiblehistorysize, we generatedSVM Dialogue and SVM-HMM Dialoguemodels and evaluated them as detailed above. We compared themacroandmicroaverageF1-scoreof our classifiers against the baseline classifier performance. As depicted in Figure 4 both theSVM Dialogue and SVM-HMM Dialoguemodels were superior\n5http://liblinear.bwaldvogel.de/ 6https://www.cs.cornell.edu/people/tj/\nsvm_light/svm_hmm.html 7https://github.com/clir/clearnlp\n0.25\n0.3\n0.35\n0.4\n0.45\n1 2 3 4 5 6 7\nF 1\n-s co\nre\nHistory size\n(a) Customer\n(b) Agent\n0.42\n0.47\n0.52\n0.57\n0.62\n0 1 2 3 4 5 6\nF 1\n-s co\nre\nHistory size\nBaseline Micro SVM Micro SVM-HMM Micro Baseline Macro SVM Macro SVM-HMM Macro\nfor all history ranges and for both tasks. Examining the customer turns emotion detection performance, we can see in Figure 4(a) that it increases until history = 3, and then remains relatively stable for largerhistory sizes. This means that information about the behavior of the customer and agent in past turns is beneficial for detecting customer emotions in a current turn. For assessing the performance of our predictions of agent turns emotion techniques, we first note that we tested with history > 0 range, since we assume that the minimal information needed for agent turn classification is the information extracted from the last customer turn. Figure 4(b) shows that overall, performance is highest whenhistory = 1, and does not decline much for higherhistory values. This indicates that for agent emotion technique prediction the last customer turn is the most informative one.\nIn all of our experiments, we used theWilcoxon signed-rank testo validate the statistical significance of our models\u2019micro and macro average F1-scorecomparing to baseline performance. Additionally, we usedMcNemar\u2019s teston the contingency tables aggregated over all emotions. These tests showed that both of our models were significantly different from the baseline model, under a value of0.001, for both classification tasks and all historysizes."
            },
            {
                "heading": "5.3 Detailed Classification Results",
                "text": "Table 4 depicts the detailed classification results for optimal history values that obtained maximal macro F1-score, namely for customer emotion detectionhistory = 4 and for agent emotion technique predictionhistory = 1. The table presents performance for each emotion, formacro andmicro average results over all dialogues, and for each data source (Genor Tech) separately. For both classification tasks, both of our models outperformed baseline results for almost all emotions, where averagemacroandmicro results are statistically significant compared to the baseline, as described above.\nFor customer turn emotion detection, theSVMHMM Dialoguemodel performed better than the SVM Dialoguemodel, and reached amacro and\nmicro averageF1-scoreimprovements over all di-\nalogues of17.8% and11.7%, respectively. Fur-\nthermore, themacroandmicro averageF1-score\nresults of theSVM-HMM Dialoguemodel (0.519 and 0.6, respectively) are satisfying given the moderate ICC score between the judges (0.53). For predicting the agent emotional technique, the SVM Dialoguemodel obtained slightly better results thanSVM-HMM Dialoguemodel, and reached amacro and micro averageF1-score improvements over all dialogues of53.9% and 43.5%, respectively. These results emphasize the differences between theSVM Dialogueand SVM-HMM Dialoguemodels. Specifically, when history size is large, as in customer emotion prediction, SVM-HMM Dialoguemodel, which internally captures dependencies in past classifications, outperforms the simplisticSVM Dialogue model. We note that an improvement is also obtained when calculatingmacroandmicro average performance for each data source separately. This highlights our models\u2019 superiority as well as their general applicability and robustness for different data sources."
            },
            {
                "heading": "5.4 Feature Set Contribution Analysis",
                "text": "We examined the contribution of different feature sets in an incremental fashion, using the optimal history value detailed above. Based on the families of feature sets that we defined in the Methodology section, we tested the performance of different feature set combinations in our models, added in the following order:baseline(textual features), emotional, temporalandintegral. Figure 5 depicts\nthe results for both classification tasks. Thex-axis represents specific combination of features sets, and they-axis represents themacroor microaverageF1-scorevalue obtained. Figure 5 shows that adding each feature set improved performance for all models, for both tasks, which indicates the informative value of each feature set. Additionally, the figure suggests that the most informative dialogue feature sets are theintegralandemotional."
            },
            {
                "heading": "6 Conclusions",
                "text": "In this work we studied emotions being expressed in customer service dialogues in the social me-\ndia. Specifically, we described two classification\ntasks, one for detecting customer emotions and\nthe other for predicting the emotional technique\nused by support service agent. We have pro-\nposed two different models (SVM Dialogueand SVM-HMM Dialoguemodels) for these tasks. We studied the impact ofdialogue featuresand dialogue history on the quality of the classification and showed improvement in performance for both models and both classification tasks. We also showed the robustness of our models across different data sources. As for future work we plan to work on several aspects: (1) In this work, we showed that it is possible to predict the emotional\n0.40\n0.45\n0.50\n0.55\n0.60\nMacro SVM Micro SVM Macro SVM-HMM Micro SVM-HMM\nF 1\n-s co\nre\nBL BL+emotional BL+emotional+temporal all feautres\n0.24\n0.29\n0.34\n0.39\n0.44\nMacro SVM Micro SVM Macro SVM-HMM Micro SVM-HMM\nF 1\n-s co\nre\n(a) Customer\n(b) Agent\ntechnique. In the future, we plan to run experiments in which the predicted emotional technique is actually applied in the context of new dialogues to measure the effect of such predictions on real support dialogues. (2) Distinguish between dialogues that have positive outcomes (e.g., high customer satisfaction) and others."
            }
        ],
        "references": [
            {
                "title": "7 reasons you need to be using social media as your customer service portal",
                "author": [
                    "Jayson DeMers."
                ],
                "venue": "Forbes.",
                "citeRegEx": "DeMers.,? 2014",
                "shortCiteRegEx": "DeMers.",
                "year": 2014
            },
            {
                "title": "Responding to learners\u2019 cognitiveaffective states with supportive and shakeup dialogues",
                "author": [
                    "Sidney D\u2019Mello",
                    "Scotty Craig",
                    "Karl Fike",
                    "Arthur Graesser"
                ],
                "venue": "InProceedings of HCI",
                "citeRegEx": "D.Mello et al\\.,? \\Q2009\\E",
                "shortCiteRegEx": "D.Mello et al\\.",
                "year": 2009
            },
            {
                "title": "Anger, frustration, and helplessness after service failure: coping strategies and effective informational support",
                "author": [
                    "Katja Gelbrich"
                ],
                "venue": "Journal of the Academy of Marketing Science",
                "citeRegEx": "Gelbrich.,? \\Q2010\\E",
                "shortCiteRegEx": "Gelbrich.",
                "year": 2010
            },
            {
                "title": "Emotion detection in email customer care.Computational Intelligence",
                "author": [
                    "Narendra K. Gupta",
                    "Mazin Gilbert",
                    "Giuseppe Di Fabbrizio"
                ],
                "venue": null,
                "citeRegEx": "Gupta et al\\.,? \\Q2013\\E",
                "shortCiteRegEx": "Gupta et al\\.",
                "year": 2013
            },
            {
                "title": "Predicting and eliciting addressee\u2019s emotion in online dialogue",
                "author": [
                    "Takayuki Hasegawa",
                    "Naoki Yoshinaga Kaji",
                    "Nobuhiro",
                    "Masashi Toyoda."
                ],
                "venue": "InACL (1), pages 964\u2013972.",
                "citeRegEx": "Hasegawa et al\\.,? 2013",
                "shortCiteRegEx": "Hasegawa et al\\.",
                "year": 2013
            },
            {
                "title": "Classifying dialogue acts in one-on-one live chats",
                "author": [
                    "Su Nam Kim",
                    "Lawrence Cavedon",
                    "Timothy Baldwin."
                ],
                "venue": "InProceedings of EMNLP",
                "citeRegEx": "Kim et al\\.,? 2010",
                "shortCiteRegEx": "Kim et al\\.",
                "year": 2010
            },
            {
                "title": "Answers to 20 questions about interrater reliability and interrater agreement",
                "author": [
                    "James M LeBreton",
                    "Jenell L Senter"
                ],
                "venue": null,
                "citeRegEx": "LeBreton and Senter.,? \\Q2007\\E",
                "shortCiteRegEx": "LeBreton and Senter.",
                "year": 2007
            },
            {
                "title": "More than happy to help? customer-focused emotion management strategies",
                "author": [
                    "Laura M Little",
                    "Don Kluemper",
                    "Debra L Nelson",
                    "Andrew Ward."
                ],
                "venue": "Personnel Psychology",
                "citeRegEx": "Little et al\\.,? 2013",
                "shortCiteRegEx": "Little et al\\.",
                "year": 2013
            },
            {
                "title": "Crowdsourcing a word\u2013emotion association lexicon",
                "author": [
                    "Saif M Mohammad",
                    "Peter D Turney"
                ],
                "venue": null,
                "citeRegEx": "Mohammad and Turney.,? \\Q2013\\E",
                "shortCiteRegEx": "Mohammad and Turney.",
                "year": 2013
            },
            {
                "title": "Portable features for classifying emotional text",
                "author": [
                    "Saif Mohammad"
                ],
                "venue": "InProceedings of NAACL HLT",
                "citeRegEx": "Mohammad.,? \\Q2012\\E",
                "shortCiteRegEx": "Mohammad.",
                "year": 2012
            },
            {
                "title": "Ensemble methods for spoken emotion recognition in call-centres",
                "author": [
                    "\u201dDonn Morrison",
                    "Ruili Wang",
                    "Liyanage C. De Silva"
                ],
                "venue": null,
                "citeRegEx": "Morrison et al\\.,? \\Q2007\\E",
                "shortCiteRegEx": "Morrison et al\\.",
                "year": 2007
            },
            {
                "title": "Satisfaction: A behavioral perspective on the consumer",
                "author": [
                    "Richard L Oliver"
                ],
                "venue": null,
                "citeRegEx": "Oliver.,? \\Q2014\\E",
                "shortCiteRegEx": "Oliver.",
                "year": 2014
            },
            {
                "title": "Learning emotion indicators from tweets: Hashtags, hashtag patterns, and phrases",
                "author": [
                    "Routledge. Ashequl Qadir",
                    "Ellen Riloff"
                ],
                "venue": null,
                "citeRegEx": "Qadir and Riloff.,? \\Q2014\\E",
                "shortCiteRegEx": "Qadir and Riloff.",
                "year": 2014
            },
            {
                "title": "Expression of emotion as part of the work role.Academy of management review",
                "author": [
                    "Anat Rafaeli",
                    "Robert I Sutton"
                ],
                "venue": null,
                "citeRegEx": "Rafaeli and Sutton.,? \\Q1987\\E",
                "shortCiteRegEx": "Rafaeli and Sutton.",
                "year": 1987
            },
            {
                "title": "Data-driven response generation in social media",
                "author": [
                    "Alan Ritter",
                    "Colin Cherry",
                    "William B. Dolan."
                ],
                "venue": "Proceedings of EMNLP",
                "citeRegEx": "Ritter et al\\.,? 2011",
                "shortCiteRegEx": "Ritter et al\\.",
                "year": 2011
            },
            {
                "title": "Empatweet: Annotating and detecting emotions on twitter",
                "author": [
                    "Kirk Roberts",
                    "Michael A Roach",
                    "Joseph Johnson",
                    "Josh Guthrie",
                    "Sanda M Harabagiu"
                ],
                "venue": null,
                "citeRegEx": "Roberts et al\\.,? \\Q2012\\E",
                "shortCiteRegEx": "Roberts et al\\.",
                "year": 2012
            },
            {
                "title": "Weekends, work, and wellbeing: Psychological need satisfactions and day of the week effects on mood, vitality, and physical symptoms",
                "author": [
                    "Richard M Ryan",
                    "Jessey H Bernstein",
                    "Kirk Warren Brown."
                ],
                "venue": "Journal of social and clinical psychol-",
                "citeRegEx": "Ryan et al\\.,? 2010",
                "shortCiteRegEx": "Ryan et al\\.",
                "year": 2010
            },
            {
                "title": "Affect listeners: Acquisition of affective states by means of conversational systems",
                "author": [
                    "Marcin Skowron."
                ],
                "venue": "InDevelopment of Multimodal Interfaces: Active Listening and Synchrony",
                "citeRegEx": "Skowron.,? 2010",
                "shortCiteRegEx": "Skowron.",
                "year": 2010
            },
            {
                "title": "A neural network approach to context-sensitive generation of conversational responses",
                "author": [
                    "Alessandro Sordoni",
                    "Michel Galley",
                    "Michael Auli",
                    "Chris Brockett",
                    "Yangfeng Ji",
                    "Meg Mitchell",
                    "Jian-Yun Nie",
                    "Jianfeng Gao",
                    "Bill Dolan."
                ],
                "venue": "In",
                "citeRegEx": "Sordoni et al\\.,? 2015",
                "shortCiteRegEx": "Sordoni et al\\.",
                "year": 2015
            },
            {
                "title": "The occ model revisited",
                "author": [
                    "In NAACL-HLT.B.R. Steunebrink",
                    "M.M. Dastani",
                    "J.-J.Ch. Meyer."
                ],
                "venue": "In",
                "citeRegEx": "Steunebrink et al\\.,? 2009",
                "shortCiteRegEx": "Steunebrink et al\\.",
                "year": 2009
            },
            {
                "title": "Dialogue act recognition in synchronous and asynchronous",
                "author": [
                    "Maryam Tavafi",
                    "Yashar Mehdad",
                    "Shafiq Joty",
                    "Giuseppe Carenini",
                    "Raymond Ng"
                ],
                "venue": null,
                "citeRegEx": "Tavafi et al\\.,? \\Q2013\\E",
                "shortCiteRegEx": "Tavafi et al\\.",
                "year": 2013
            },
            {
                "title": "Multi-label classification: An overview.Dept. of Informatics, Aristotle University of Thessaloniki, Greece",
                "author": [
                    "Grigorios Tsoumakas",
                    "Ioannis Katakis"
                ],
                "venue": null,
                "citeRegEx": "Tsoumakas and Katakis.,? \\Q2006\\E",
                "shortCiteRegEx": "Tsoumakas and Katakis.",
                "year": 2006
            },
            {
                "title": "Detection of real-life emotions in call centers",
                "author": [
                    "Laurence Vidrascu",
                    "Laurence Devillers."
                ],
                "venue": "In",
                "citeRegEx": "Vidrascu and Devillers.,? 2005",
                "shortCiteRegEx": "Vidrascu and Devillers.",
                "year": 2005
            },
            {
                "title": "The relation between written and spoken language",
                "author": [
                    "Deborah Tannen Wallace Chafe"
                ],
                "venue": null,
                "citeRegEx": "Chafe.,? \\Q1987\\E",
                "shortCiteRegEx": "Chafe.",
                "year": 1987
            },
            {
                "title": "Services marketing: Integrating customer focus across the firm",
                "author": [
                    "Valarie A Zeithaml",
                    "Mary Jo Bitner",
                    "Dwayne D Gremler"
                ],
                "venue": null,
                "citeRegEx": "Zeithaml et al\\.,? \\Q2006\\E",
                "shortCiteRegEx": "Zeithaml et al\\.",
                "year": 2006
            },
            {
                "title": "Effects of emotional text on online customer service chat",
                "author": [
                    "L. Zhang",
                    "L.B. Erickson",
                    "H.C. Webb."
                ],
                "venue": "InGraduate Student Research Conference in Hospitality and Tourism.",
                "citeRegEx": "Zhang et al\\.,? 2011",
                "shortCiteRegEx": "Zhang et al\\.",
                "year": 2011
            },
            {
                "title": "Service design for experience-centric services.Journal of Service Research",
                "author": [
                    "Leonieke G Zomerdijk",
                    "Christopher A Voss"
                ],
                "venue": null,
                "citeRegEx": "Zomerdijk and Voss.,? \\Q2010\\E",
                "shortCiteRegEx": "Zomerdijk and Voss.",
                "year": 2010
            }
        ],
        "abstractText": "Providing customer support through social media channels is gaining increasing popularity. In such a context, automatic detection and analysis of the emotions expressed by customers is important, as is identification of the emotional techniques (e.g., apology, empathy, etc.) in the responses of customer service agents. Result of such an analysis can help assess the quality of such a service, help and inform agents about desirable responses, and help develop automated service agents for social media interactions. In this paper, we show that, in addition to text based turn features, dialogue features can significantly improve detection of emotions in social media customer service dialogues and help predict emotional techniques used by customer service agents."
    },
    {
        "title": "Learning Implicit Generative Models by Matching Perceptual Features",
        "sections": [
            {
                "heading": "1. Introduction",
                "text": "The use of features from deep convolutional neural networks (DCNNs) pretrained on ImageNet [35] has led to important advances in computer vision. DCNN features, usually called perceptual features (PFs), have been used in tasks such as transfer learning [40, 16], style transfer [9] and super-resolution [17]. While there have been previous works on the use of PFs in the context of image generation and transformation [7, 17], exploration of PFs as key source of information for learning generative models is not well studied. Particularly, the efficacy of PFs for implicit generative models trained through moment matching is an open question.\nMoment matching approaches for generative modeling are based on the assumption that one can learn the data distribution by matching the moments of the model distribution to the empirical data distribution. Two representative meth-\n\u2217 Equal contribution.\nods of this family are based on maximum mean discrepancy (MMD) [11, 12, 22] and the method of moments (MoM) [33]. While MoM based methods embed a probability distribution into a finite-dimensional vector (i.e., matching of a finite number of moments), MMD based methods embed a distribution into an infinite-dimensional vector [33]. A challenge for MMD methods is to define a kernel function that is statistically efficient and can be used with small minibatch sizes [21]. A solution comes by using adversarial learning for the online training of kernel functions [21, 3]. However, this solution inherits the problematic min/max game of adversarial learning. The main challenges of using MoM for training deep generative networks consist in defining millions of sufficiently distinct moments and specifying an objective function to learn the desirable moments. Ravuri et al. [33] addressed these two issues by defining the moments as features and derivatives from a moment network that is trained online (together with the generator) by using a specially designed objective function.\nIn this work we demonstrate that, by using PFs to perform moment matching, one can overcome some of the difficulties found in current moment matching approaches. More specifically, we propose a simple but effective moment matching method that: (1) breaks away from the problematic min/max game completely; (2) does not use online learning of kernel functions; and (3) is very efficient with regard to both number of used moments and required minibatch size. Our proposed approach, named Generative Feature Matching Networks (GFMN), learns implicit generative models by performing mean and covariance matching of features extracted from all convolutional layers of pretrained deep ConvNets. Some interesting properties of GFMNs include: (a) the loss function is directly correlated to the generated image quality; (b) mode collapsing is not an issue; and (c) the same pretrained feature extractor can be used across different datasets.\nWe perform an extensive number of experiments with different challenging datasets: CIFAR10, STL10, CelebA and LSUN. We demonstrate that our approach can achieve state-of-the-art results for challenging benchmarks such as CIFAR10 and STL10. Moreover, we show that the same\n1\nar X\niv :1\n90 4.\n02 76\n2v 1\n[ cs\n.C V\n] 4\nA pr\n2 01\n9\nfeature extractor is effective across different datasets. The main contributions of this work can be summarized as follows: (1) We propose a new effective moment matchingbased approach to train implicit generative models that does not use adversarial or online learning of kernel functions, provides stable training, and achieves state-of-the-art results; (2) We show theoretical results that demonstrate GFMN convergence under the assumption of the universality of perceptual features; (3) We propose an ADAM-based moving average method that allows effective training with small minibatches; (4) Our extensive quantitative and qualitative experimental results demonstrate that pretrained autoencoders and DCNN classifiers can be effectively used as (cross-domain) feature extractors for GFMN training."
            },
            {
                "heading": "2. Generative Feature Matching Networks",
                "text": ""
            },
            {
                "heading": "2.1. The method",
                "text": "Let G be the generator implemented as a neural network with parameters \u03b8, and let E be a pretrained neural network with L hidden layers. Our proposed approach consists in training G by minimizing the following loss function:\nmin \u03b8\nM\u2211\nj=1\n||\u00b5jpdata \u2212 \u00b5jpG(\u03b8)||2 + ||\u03c3jpdata \u2212 \u03c3jpG(\u03b8)||2 (1)\nwhere:\n\u00b5jpdata = Ex\u223cpdataEj(x) \u2208 Rdj\n\u00b5jpG(\u03b8) = Ez\u223cN (0,Inz )Ej(G(z; \u03b8)) \u2208 R dj \u03c3jpdata,` = Ex\u223cpdataEj,`(x) 2 \u2212 [\u00b5j,`pdata ]2, ` = 1 . . . dj\n\u03c3jpG,`(\u03b8) = Ez\u223cN (0,Inz )Ej,`(G(z; \u03b8)) 2\u2212[\u00b5j,`pG ]2, `=1 . . . dj\nand ||.||2 is the L2 loss; x is a real data point sampled from the data generating distribution pdata; z \u2208 Rnz is a noise vector sampled from the normal distribution N (0, Inz ); Ej(x), denotes the output vector/feature map of the hidden layer j from E; M \u2264 L is the number of hidden layers used to perform feature matching. Note that \u03c32pdata and \u03c3 2 pG denote the variances of the features from real data and generated data, respectively. We use diagonal covariance matrices as computing full covariance matrices is impractical for large numbers of features.\nIn practice, we train G by first precomputing estimates of \u00b5jpdata and \u03c3 j pdata\non the training data, then running multiple training iterations where we sample a minibatch of generated (fake) data and optimize the parameters \u03b8 using stochastic gradient descent (SGD) with backpropagation. The network E is used for the purpose of feature extraction only and is kept fixed during the training of G. Fig. 1 presents GFMN training pipeline. Autoencoder Features: A natural choice of unsupervised method to train a feature extractor is the autoencoder (AE) framework. The decoder part of an AE consists exactly of an image generator that uses features extracted by the encoder. Therefore, by design, the encoder network should be a good feature extractor for the purpose of generation. Classifier Features: We experiment with different DCNN architectures pretrained on ImageNet to play the role of the feature extractor E. Our hypothesis is that ImageNet-based PFs are informative enough to allow the training of (crossdomain) generators by feature matching."
            },
            {
                "heading": "2.2. Matching Feat. with ADAM Moving Average",
                "text": "From feature matching loss to moving averages. In order to train with a mean and covariance feature matching loss, one needs large minibatches to obtain good mean and covariance estimates. With images larger than 32\u00d732, DCNNs produce millions of features, resulting easily in memory issues. We propose to alleviate this problem by using moving averages of the difference of means (covariances) of real and generated data. Instead of computing the (memory) expensive feature matching loss in Eq. 1, we keep moving averages vj of the difference of feature means (covariances) at layer j between real and generated data. We detail our moving average strategy for the mean features only, but the same approach applies for the covariances. The mean features from the first term of Eq. 1, ||\u00b5jpdata\u2212Ez\u223cN (0,Inz )Ej(G(z; \u03b8))||2 can be approximated by:\nv>j ( \u00b5jpdata \u2212 1\nN\nN\u2211\nk=1\nEj(G(zk; \u03b8))\n) ,\nwhere N is the minibatch size and vj is a moving average on \u2206j , the difference of the means of the features extracted\nby the j-th layer of E:\n\u2206j = \u00b5 j pdata \u2212 1 N\nN\u2211\nk=1\nEj(G(zk; \u03b8)). (2)\nUsing these moving averages we replace the first term of the loss given in Eq. 1 by\nmin \u03b8\nM\u2211\nj=1\nv>j ( \u00b5jpdata\u2212 1\nN\nN\u2211\nk=1\nEj(G(zk; \u03b8))\n) . (3)\nThe moving average formulation of features matching above has a major advantage on the naive formulation of Eq. 1 since we can now rely on vj to get better estimates of the population feature means of real and generated data while using a small minibatch of sizeN . For a similar result using the feature matching loss given in Eq. 1, one would need a minibatch with large size N , which is problematic for large number of features.\nADAM moving average: from SGD to ADAM updates. Note that for a rate \u03b1, the moving average vj has the following update:\nvj,new = (1\u2212 \u03b1) \u2217 vj,old + \u03b1 \u2217\u2206j ,\u2200j = 1 . . .M\nIt is easy to see that the moving average is a gradient descent update on the following loss:\nmin vj\n1 2 ||vj \u2212\u2206j ||2. (4)\nHence, writing the gradient update with learning rate \u03b1 we have equivalently:\nvj,new = vj,old\u2212\u03b1\u2217(vj,old\u2212\u2206j) = (1\u2212\u03b1)\u2217vj,old+\u03b1\u2217\u2206j .\nWith this interpretation of the moving average, we propose to get a better moving average estimate by using the ADAM optimizer [18] on the loss of the moving average given in Eq. 4, such that vj,new = vj,old \u2212 \u03b1ADAM(vj,old \u2212\u2206j). ADAM(x) function is computed as follows:\nmt = \u03b21 \u2217mt\u22121 + (1\u2212 \u03b21) \u2217 x m\u0302t = mt/(1\u2212 \u03b2t1) ut = \u03b22 \u2217 ut\u22121 + (1\u2212 \u03b22) \u2217 x2 u\u0302t = ut/(1\u2212 \u03b2t2) ADAM(x) = m\u0302t/( \u221a u\u0302t + ),\nwhere x is the gradient for the loss function in Eq. 4, t is the iteration number,mt and ut are the first and second moment vectors at iteration t, \u03b21 = .9, \u03b22 = .999 and = 10\u22128 are constants. m0 and u0 are initialized as proposed by [18]. We refer to [18] for a detailed ADAM optimizer description.\nThis moving average formulation, which we call ADAM Moving Average (AMA) promotes stable training when using small minibatches. Although we detail AMA using\nmean feature matching only, we use this approach for both mean and covariance matching. The main advantage of AMA over simple moving average (MA) is in its adaptive first and second order moments that ensure stable estimation of the moving averages vj . In fact, this is a non-stationary estimation since the mean of the generated data changes in the training, and it is well known that ADAM works well for such online non-stationary losses [18].\nIn Section 5.3 we provide experimental results supporting: (1) The memory advantage that the AMA formulation of feature matching offers over the naive implementation; (2) The stability advantage and improved generation results that AMA allows compared to the naive implementation. We discuss in Appendix 2 the advantage of AMA on MA from a regret bounds point of view [34]."
            },
            {
                "heading": "3. Universality of PFs and GFMN Convergence",
                "text": "Our proposed approach is related to the recent body of work on MMD or MM based generative models [22, 8, 21, 3, 33]. We highlight the main differences between MMD-GANs and GFMN in terms of requirements on the kernel for MMD-GAN and on the feature map (Extractor) for GFMN, that ensure convergence of the generator to the data distribution. See Tab. 1 for a summary.\nGMMN, MMD-GAN Convergence: MMD Matching with Universal Kernels. We start by reviewing known results on MMD. Let Hk be a Reproducing Kernel Hilbert Space (RKHS) defined with a continuous kernel k. Informally, k is universal if any bounded continuous function can be approximated to an arbitrary precision in Hk (formal definition in Appendix ). Theorem 1 [12] shows that the MMD is a well defined metric for universal kernels.\nTheorem 1 ([12]). Given a kernel k, let p, q be two distributions, their MMD is: MMD2(k, p, q) = ||\u00b5p \u2212 \u00b5q||2Hk , where \u00b5p = Ex\u223cpkx is the mean embedding. If k is universal then MMD2(k, p, q) = 0 if and only if p = q.\nGiven a Universal kernel such as a Gaussian Kernel as outlined in GMMN [22, 8], one can learn implicit Generative models G\u03b8 that defines a family of distribution {q\u03b8} by minimizing the MMD distance:\ninf \u03b8 MMD(k, pdata, q\u03b8) (5) Assuming pdata is in the family {q\u03b8} (\u2203\u03b8\u2217, q\u03b8\u2217 = pdata), the infimum of MMD minimization for a universal kernel is achieved for q\u03b8 = pdata (immediate consequence of Theorem 1). This elegant setup for MMD matching with universal kernels, while avoiding the difficult min/max game in GAN, does not translate into good results in image generation. To remedy that, other discrepancies introduced in [21, 3, 33] compose universal kernels k with a feature map \u03c6 \u2208 \u03a8 as follows:\nDMMD(p, q) = sup\u03c6\u2208\u03a8 MMD(k \u25e6 \u03c6, p, q). For learning implicit generative models [21] replaces MMD in Eq. (5) by DMMD. Under conditions on the kernel and the learned feature map this discrepancy is continuous in the weak topology (Prop. 2 in [1, 21]). Nevertheless, learning generative models remains challenging with it as it boils down to a min/max game as in original GAN [10].\nGFMN Convergence: MMD Matching with Universal Features. While universality is usually thought on the kernel level, it is not straightforward to define universality for kernels defined by feature maps. Micchelli et al. [26] define universality of feature maps and how it connects to their corresponding kernels. Specifically for a fixed feature set on a space X (space of images) S = {\u03c6j , j \u2208 I, \u03c6j : X \u2192 R}, where I is a countable set of indices, define the kernel K\u03c6(x, y) = \u2211 j\u2208I \u03c6j(x)\u03c6j(y). Micchelli et al. [26] in Thm. 7 show that this Kernel is universal if the set S is universal. Informally speaking, a feature set S is universal if linear functions in this feature space ( \u2211 j\u2208I uj\u03c6j(x)), are dense in the set of continuous bounded functions (formal definition in Appendix 1).\nThis is of interest since GFMN corresponds to MMD matching with a kernel K\u03a6 defined on a fixed feature map \u03a6(x)={\u03c6j(x)}j\u2208I , where I is finite. We have K\u03a6(x, y)= \u3008\u03a6(x),\u03a6(y)\u3009=\u2211j\u2208I \u03c6j(x)\u03c6j(y) and\nMMD2(K\u03a6, p, q) = ||Ex\u223cp\u03a6(x)\u2212 Ex\u223cq\u03a6(x)||2 .\nFor MMD2(K\u03a6, p, q) to be a metric it is enough to have the set features S be universal (by Thm.1 and Thm. 7 in [26]). Prop. 1 gives conditions for GFMN convergence:\nProposition 1. Assume pdata belongs to the family defined by the generator {q\u03b8}\u03b8. GFMN converges to the real distribution by matching in a feature space S = {\u03c6j , j \u2208 I}, where I is a countable set, if the features set S is universal (informally means that any continuous functions can be written as linear combination in the span of S) .\nProof. S is universal =\u21d2 k\u03a6 is universal [26]. Hence MMD(k\u03a6, pdata, q\u03b8) = 0 iff q\u03b8 = pdata. GFMN solves\ninf\u03b8 MMD2(K\u03a6, pdata, q\u03b8), and the infimum is achieved for \u03b8 such that q\u03b8 = pdata ( pdata \u2208 {q\u03b8}\u03b8 ).\nRemark 1. The analysis covers here mean matching but the same applies to covariance matching considering S = {\u03c6j , \u03c6j\u03c6k, j, k \u2208 I}.\nUniversality of Perceptual Features in Computer Vision. From Prop. 1 we see that for GFMN to be convergent with pretrained feature extractors Ej that are perceptual features (such as features from VGG or ResNet pretrained on ImageNet), we need to assume universality of those features in the image domain. We know from transfer learning that features from ImageNet pretrained VGG/ResNet can express any functions for a downstream task by finding a linear weight in their span. Note that this is the definition of universal feature as given in [26]: continuous functions can be approximated in the linear span of those features. Hence, assuming universality of PFs defined by ImageNet pretrained VGG or ResNet, GFMN is guaranteed to converge to the data distribution by Prop. 1. Our results complement the common wisdom on \u201cuniversality\u201d of PFs in transfer learning and style transfer by showing that they are sufficient for learning implicit generative models."
            },
            {
                "heading": "4. Related work",
                "text": "GFMN is related to the recent body of work on MMD and moment matching based generative models [22, 8, 21, 3, 33]. The closest to our method is the Generative Moment Matching Network + Autoencoder (GMMN+AE) proposed in [22]. In GMMN+AE, the objective is to train a generator G that maps from a prior uniform distribution to the latent code learned by a pretrained AE, and then uses the frozen pretrained decoder to map back to image space. As discussed in Section 3 one key difference in our approach is that, while GMMN+AE uses a Gaussian kernel to perform moment matching using the AE low dimensional latent code, GFMN performs mean and covariance matching in a PF space induced by a non-linear kernel function (a DCNN) that is orders of magnitude larger than the AE latent code, and that we argued is universal in the image domain.\nLi et al. [21] demonstrate that GMMN+AE is not competitive with GANs for challenging datasets such as CIFAR10. MMD-GANs, discussed in Section 3, demonstrated competitive results with the use of adversarial learning by learning a feature map in conjuction with a Gaussian kernel [21, 3]. Finally, Ravuri et al. [33] recently proposed a method to perform online learning of the moments while training the generator. Our proposed method differs by using fixed pretrained PF extractors for moment matching.\nBojanowski et al. [4] proposed the Generative Latent Optimization (GLO) model that jointly optimizes model parameters and noise input vectors z, while avoiding adversarial training. Our work relates also to plug and play generative models of [30] where a pretrained classifier is used to sample new images, using MCMC sampling methods.\nOur work is also related to AE-based generative models variational AE (VAE) [19], adversarial AE (AAE) [25] and Wasserstein AE (WAE) [38]. However, GFMN is quite distinct from these methods because it uses pretrained AEs to play the role of feature extractors only, while these methods aim to impose a prior distrib. on the latent space of AEs. Another recent line of work that involves the use of AEs in generative models consists in applying AEs to improve GANs stability [42, 39]. Finally, our objective function is related to the McGan loss function [29], where authors match first and second order moments."
            },
            {
                "heading": "5. Experiments",
                "text": ""
            },
            {
                "heading": "5.1. Experimental Setup",
                "text": "Datasets: We evaluate our proposed approach on images from CIFAR10 [20] (50k train., 10k test, 10 classes), STL10 [6] (5k train., 8k test, 100k unlabeled, 10 classes), CelebA [24] (200k) and LSUN bedrooms [41] datasets. STL10 images are rescaled to 32\u00d732, while CelebA and LSUN images are rescaled to either 64\u00d764 or 128\u00d7128, depending on the experiment. CelebA images are center-cropped to 160\u00d7160 before rescaling. GFMN Generator: In most of our experiments the generator G uses a DCGAN-like architecture [32]. For CIFAR10, STL10, LSUN and CelebA64\u00d764, we use two extra layers as commonly used in previous works [28, 13]. For CelebA128\u00d7128 and some experiments with CIFAR10 and STL10, we use a ResNet-based generator such as the one in [13]. Architecture details are in the supplementary material. Autoencoder Features: For most AE experiments, we use an encoder network whose architecture is similar to the discriminator in DCGAN (strided convolutions). We use batch normalization and ReLU non-linearity after each convolution. We set the latent code size to 128, 128, and 512 for CIFAR10, STL10 and CelebA, respectively. To perform feature extraction, we get the output of each ReLU in the network. Additionally, we also perform some experiments where the encoder uses a VGG19 architecture. The decoder\nnetwork D uses a network architecture similar to our generator G. More details in the supplementary material. Classifier Features: We perform our experiments on classifier features with VGG19 [37] and Resnet18 networks [14] which we pretrained using the whole ImageNet dataset [35] with 1000 classes. Pretrained ImageNet classifiers details can be found in the supplementary material. GFMN Training: GFMNs are trained with an ADAM optimizer; most hyperparameters are kept fixed across datasets. We use nz = 100 and minibatch of 64. Dataset dependent learning rates are used for updating G (10\u22124 or 5\u00d710\u22125) and AMA (5\u00d710\u22125 or 10\u22125). We use AMA moving average (Sec. 2.2) in all reported experiments."
            },
            {
                "heading": "5.2. Autoencoder Features vs. (Cross-domain) Classifier Features",
                "text": "This section presents a comparative study on the use of pretrained autoencoders and cross-domain classifiers as feature extractors in GFMN. Tab. 2 shows the Inception Score (IS) [36] and Fre\u0301chet Inception Distance (FID) [15] for GFMN trained on CIFAR10 using different feature extractors E. The two first rows in Tab. 2 correspond to GFMN models that use pretrained encoders asE, while the last four rows use pretrained VGG19/Resnet18 ImageNet classifiers. We can see in Tab. 2 that there is a large boost in performance when ImageNet classifiers are used as feature extractors instead of encoders. Despite the classifiers being trained on a different domain (ImageNet vs. CIFAR10), the classifier features are significantly more effective. While the best IS with encoders is 4.95, the lowest IS with ImageNet classifier is 7.88. Additionally, when using simultaneously VGG19 and Resnet18 as feature extractors (two last rows), which increases the number of features to 832K, we get even better performance. Finally, we achieve the best performance in terms of both IS and FID (last row1) when using a generator architecture that contains residual blocks, similar to the one propose in [13].\nRandom samples from GFMNVGG19+Resnet18 trained with CIFAR10 and STL10 are shown in Figs. 2a and 2b respectively. Fig. 2c shows random samples from GFMNVGG19 trained with LSUN bedrooms dataset (resolution 64\u00d764). Fig. 3 presents samples from GFMNVGG19 trained with CelebA dataset with resolution 128\u00d7128, which shows that GFMN can achieve good performance with image resolutions larger than 32\u00d732. These results also demonstrate that: (1) the same classifier (VGG19 trained on ImageNet) can be successfully applied to train GFMN models across different domains; (2) perceptual features from DCNNs encapsulate enough statistics to allow the learning of good generative models through moment matching.\nTab. 3 shows IS and FID for increasing number of layers (i.e. number of features) in our extractor VGG19. We se-\n1Average result of five runs with different random seeds.\nlect up to 16 layers, excluding the output of fully connected layers. Using more layers dramatically improves the performance of the feature extractor, reaching IS and FID peak performance when the maximum number of layers is used. Note that the features are ReLU activation outputs, meaning the encodings may be quite sparse. In Appendix 7 we show qualitative results that corroborate these results.\nTo verify whether the number of features is the main factor for performance, we conducted an experiment where we train an AE with an encoder using a VGG19 architecture. This encoder is pretrained on ImageNet and produces a total of 296K features. The second row in Tab. 2 shows the\nresults for this experiment. Although there is improvement in both IS and FID compared to the DCGAN encoder (first row), the boost is not comparable to the one obtained with a VGG19 classifier. In other words, features from classifiers are significantly more informative than AEs features for the purpose of training generators by feature matching."
            },
            {
                "heading": "5.3. AMA and Training Stability",
                "text": "This section presents experimental results that evidence the advantage of our proposed ADAM moving average (AMA) over the simple moving average (MA). The main benefit of AMA is the promotion of stable training when using small minibatches. The ability to train with small minibatches is essential due to GFMN\u2019s need for large number of features from DCNNs, which becomes a challenge in\nterms of GPU memory usage. Our Pytorch [31] implementation of GFMN can only handle minibatches of size up to 160 when using VGG19 as a feature extractor and image size 64\u00d764 on a Tesla K40 GPU w/ 12GB of memory. A more optimized implementation minimizing memory overhead could, in principle, handle somewhat larger minibatch sizes (as could a more recent Tesla V100 w/ 16 GB). However, increase image size or feature extractor size and the memory footprint increases quickly. We will always run out of memory when using larger minibatches, regardless of implementation or hardware.\nAll experiments in this section use CelebA training set, and a feature extractor using the encoder from an AE following a DCGAN-like architecture. This feature extractor is smaller than VGG19/Resnet18 allowing for minibatches of size up to 512 for image size 64\u00d764. Fig. 4 shows generated images from GFMN trained with either MA or our proposed AMA. For MA, generated images from GFMN trained with 64 and 512 minibatch size are presented in Figs. 4a and 4b respectively. For AMA, Fig. 4c shows results for minibatch size 64. In MA training, the minibatch size has a tremendous impact on the quality of generated images: with minibatches smaller than 512, almost all images generated are quite distorted. On the other hand, when using AMA, GFMN generates much better images with minibatch size 64 (Fig. 4c). For AMA, increasing the minibatch size from 64 to 512 does not improve the quality of generated images for the given dataset and feature extractor. In the supplementary material, we show a comparison between MA and AMA with VGG19 ImageNet classifier as feature extractor for a minibatch size of 64. AMA also displays a very positive effect on the quality of generated images when a stronger feature extractor is used. An alternative for training with larger minibatches would be the use of multi-GPU, multi-node setups. However, performing large scale experiments is beyond the scope of the current work. Moreover,\nmany practitioners do not have access to a GPU cluster, and the development of methods that can also work on a single GPU with small memory footprint is essential.\nAn important advantage of GFMN over adversarial methods is its training stability. Fig. 5 shows the evolution of the generator loss per epoch and generated examples when using AMA. There is a clear correlation between the quality of generated images and the loss. Moreover, mode collapsing was not observed in our experiments with AMA."
            },
            {
                "heading": "5.4. Comparison to the State-of-the-art",
                "text": "In Tab. 4, we compare GFMN results with different adversarial and non-adversarial approaches for CIFAR10 and STL10. In the middle part of the table, we report results for recent unsupervised models that use a DCGANlike architecture in the generator. Despite using a frozen cross-domain feature extractor, GFMN outperforms the unsupervised systems in IS and FID for both datasets. The bottom part of Tab. 4 includes results for supervised approaches. Some of these models use a Resnet architecture in the generator as indicated in parenthesis. Note that GANbased methods that perform conditional generation use direct feedback from the labels in the form of log likelihoods\nfrom the discriminator (e.g. using the k+1 trick from [36]). In contrast, our generator is trained with a loss function that only performs feature matching. Our generator is agnostic to the labels and there is no feedback in the form of a log likelihood from the labeled data. Despite that, GFMN produces results that are at the same level of supervised GAN models that use labels from the target dataset.\nWe performed additional experiments with a WGANGP architecture where: (1) the discriminator is a VGG19 or a Resnet18; (2) the discriminator is pretrained on ImageNet. The goal was to evaluate if WGAN-GP can benefit from DCNN classifiers pretrained on ImageNet. Although we tried different hyperparameter combinations, we were not able to successfully train WGAN-GP with VGG19 or Resnet18 discriminators (details in Appendix 8)."
            },
            {
                "heading": "6. Discussion & Concluding Remarks",
                "text": "We achieve successful non-adversarial training of implicit generative models by introducing different key ingredients: (1) moment matching on perceptual features from all layers of pretrained neural networks; (2) a more robust way to compute the moving average of the mean features by using ADAM optimizer, which allows us to use small minibatches; and (3) the use of perceptual features from multiple neural networks at the same time (VGG19 + Resnet18).\nOur quantitative results in Tab. 4 show that GFMN achieves better or similar results compared to the state-ofthe-art Spectral GAN (SN-GAN) [27] for both CIFAR10 and STL10. This is an impressive result for a nonadversarial feature matching-based approach that uses pretrained cross-domain feature extractors and has stable train-\ning. When compared to MMD approaches [22, 8, 21, 3, 33], GFMN presents important distinctions (some of them already listed in Secs. 3 and 4) which make it an attractive alternative. Compared to GMMN and GMMN+AE [22], we can see in Tab. 4 that GFMN achieves far better results. In the supplementary material, we also show a qualitative comparison between GFMN and GMMN results. Compared to recent adversarial MMD methods (MMD GAN) [21, 3] GFMN also presents significantly better results while avoiding the problematic min/max game. GFMN achieves better results than the Method of Learned Moments (MoLM) [33], while using a much smaller number of features to perform matching. The best performing model from [33], MoLM1536, uses around 42 million moments to train the CIFAR10 generator, while our best GFMN model uses around 850K moments/features only, almost 50x less.\nOne may argue that the best GFMN results are obtained with feature extractors trained with classifiers. However, there are two important points to note: (1) we use a cross domain feature extractor and do not use labels from the target datasets (CIFAR10, STL10, LSUN, CelebA); (2) classifier accuracy does not seem to be the most important factor for generating good features: VGG19 classifier produces features as good as the ones from Resnet18, although the former is less accurate (more details in supplementary material). We are confident that GFMN can achieve state-ofthe-art results with features from classifiers trained with unsupervised methods such as [5].\nIn conclusion, this work presents important theoretical and practical contributions that shed light on the effectiveness of perceptual features for training implicit generative models through moment matching."
            },
            {
                "heading": "7. Appendix",
                "text": ""
            },
            {
                "heading": "7.1. Continuation of Universality of PFs and GFMN",
                "text": "Convergence\nWe summarize here the main definitions and theorems from [26] regarding universality of kernels and feature maps.\nUniversal Kernels. The following defines a universal kernel\nDefinition 1 (Universal Kernel). Given a kernel K defined on X \u00d7 X . Let Z be any compact subset of X . Define the space of kernel sections:\nK(Z) = span{Ky, y \u2208 Z},\nwhere Ky : X \u2192 R, Ky(x) = K(x, y). Let C(Z) be the space of all continuous real valued functions defined on Z . A kernel is said universal if for any choice of Z (compact subset of X ) K(Z) is dense in C(Z).\nIn other words a kernel is universal if C(Z) = K(Z). Meaning if any continuous function can be expressed in the span of Ky .\nUniversal Feature Maps.We turn now for kernels defined by feature maps and how to characterize their universality. Consider a continuous feature map \u03a6 : X \u2192 W , where (W, \u3008, \u3009W) is a Hilbert space; the kernel K has the following form:\nK(x, y) = \u3008\u03a6(x),\u03a6(y)\u3009W . (6)\nLet Y be an orthonormal basis of W define the following continuous function Fy \u2208 C(Z) defined at x \u2208 Z:\nFy(x) = \u3008\u03a6(x), y\u3009W ,\nand let: \u03a6(Y) = span{Fy, y \u2208 Y}\nDefinition 2 (Universal feature Map). A feature map is universal if \u03a6(Y) is dense in C(Z), for all Z compact subsets of X .i.e A feature map is universal if \u03a6(Y) = C(Z).\nThe following Theorem shows the relation between universality of a kernel defined by feature map and the universality of the feature map:\nTheorem 2 ([26], Thm 4, Relation between K(Z) and \u03a6(Y) ). For kernel defined by feature maps in (6) we have K(Z) = \u03a6(Y). A kernel of form (6) is universal if and only if its feature map is universal.\nHence the following Theorem 7 from [26]:\nTheorem 3 ([26]). Let S = {\u03c6j , j \u2208 I}, where I is a countable set and \u03c6j : X \u2192 R continuous function. Define the following kernel\nK(x, y) = \u2211\nj\u2208I \u03c6j(x)\u03c6j(y)."
            },
            {
                "heading": "K is universal if and only if the set of features S is universal.",
                "text": ""
            },
            {
                "heading": "7.2. Discussion of AMA versus MA",
                "text": "As we already discussed the moving average of v of the difference of features means\n\u2206t = 1\nN\nN\u2211\ni=1\nE(xi)\u2212 1\nN\nN\u2211\ni=1\nE(G(zi, \u03b8t))\nbetween real and generated data at each time step t in the gradient descent up to time T , can be seen as a gradient descent in an online setting on the following cost :\nf\u2217 = min v\nT\u2211\nt=1\nft(v) =\nT\u2211\nt=1\n||v \u2212\u2206t||22\nNote that we are in the online setting since \u2206t is only known when \u03b8t of the generator is updated. The sequence vt generated by MA (moving average) and by AMA (ADAM moving average) is the SGD updates and ADAM updates respectively applied to the cost function ft. Hence we can bound the regret of the sequence {vMAt } and {vAMAt } using known results on SGD and ADAM. Let d be the dimension of the encoding E. For MA, using classic regret bounds for gradient descents we obtain:\nRMAT =\nT\u2211\nt=1\n||vMAt \u2212\u2206t||22 \u2212 f\u2217 \u2264 O( \u221a dT ).\nFor AMA, using ADAM regrets bounds from (Reddi et al., 2018). Let us define\nRAMAT =\nT\u2211\nt=1\n||vAMAt \u2212\u2206t||22 \u2212 f\u2217.\nWe have:\nRAMAT \u2264 O( \u221a T d\u2211\ni=1\nu\u0302 T, 12 i ) + \u00b7 \u00b7 \u00b7\nO\n  d\u2211\ni=1\n\u221a\u221a\u221a\u221a T\u2211\nt=1\n(\u2206t,i \u2212 vAMAt,i )2  + C\nwhere u\u0302 are defined in the ADAM updates as moving averages of second order moments of the gradients. The\nregret bound of AMA is better than MA especially if\u2211d i=1 u\u0302 T, 12 i d and\nd\u2211\ni=1\n\u221a\u221a\u221a\u221a T\u2211\nt=1\n(\u2206t,i \u2212 vAMAt,i )2 \u221a Td."
            },
            {
                "heading": "7.3. Mean Matching vs. Mean + Covariance Matching in GFMN",
                "text": "In this Appendix, we present comparative results between GFMN with mean feature matching vs. GFMN with mean + covariance feature matching. Using the first and second moments to perform feature matching gives statistical advantage over using the first moment only. In Table 5, we can see that for different feature extractors, performing mean + covariance feature matching produces significantly better results in terms of both IS and FID. Mroueh et al. [29] have also demonstrated the advantages of using mean + covariance matching in the context of GANs."
            },
            {
                "heading": "7.4. Neural Network Architectures",
                "text": "In Tables 6 and 7, and Figure 6 we detail the neural net architectures used in our experiments. In both DCGAN-like generator and discriminator, an extra layer is added when using images of size 64\u00d764. In VGG19 architecture, after each convolution, we apply batch normalization and ReLU. The Resnet generator is used for CelebA128\u00d7128 experiments and also for some experiments with CIFAR10 and STL10. For these two last datasets, the Resnet generator has 3 ResBlocks only, and the output size of the DENSE layer is 4\u00d7 4\u00d7 512."
            },
            {
                "heading": "7.5. Pretraining of ImageNet Classifiers and Autoencoders",
                "text": "Both VGG19 and Resnet18 networks are trained with SGD with fixed 10\u22121 learning rate, 0.9 momentum term, and weight decay set to 5 \u00d7 10\u22124. We pick models with\nbest top-1 accuracy on the validation set over 100 epochs of training; 29.14% for VGG19 (image size 32\u00d732), and 39.63% for Resnet18 (image size 32\u00d732). When training the classifiers we use random cropping and random horizontal flipping for data augmentation. When using VGG19 and Resnet18 as feature extractors in GFMN, we use features from the output of each ReLU that follows a conv. layer, for a total of 16 layers for VGG and 17 for Resnet18.\nIn our experiments with autoencoders (AE) we pretrained them using either mean squared error (MSE) or the Laplacian pyramid loss [23, 4]. Let E and D be the encoder and the decoder networks with parameters \u03c6 and \u03c8, respectively.\nmin \u03c6,\u03c8\nEpdata ||x\u2212D(E(x;\u03c6);\u03c8)||2\nor the Laplacian pyramid loss [23]\nLap1(x, x \u2032) =\n\u2211\nj\n2\u22122j |Lj(x)\u2212 Lj(x\u2032)|1\nwhere Lj(x) is the j-th level of the Laplacian pyramid representation of x. The Laplacian pyramid loss provides better signal for learning high frequencies of images and overcome some of the blurriness issue known from using a simple MSE loss. [4] recently demonstrated that the Lap1 loss produces better results than L2 loss for both autoencoders and generative models."
            },
            {
                "heading": "7.6. Quantitative Evaluation Metrics",
                "text": "We evaluate our models using two quantitative metrics: Inception Score (IS) [36] and Fre\u0301chet Inception Distance (FID) [15]. We followed the same procedure used in previous work to calculate IS [36, 27, 33]. For each trained generator, we calculate the IS for randomly generated 5000 images and repeat this procedure 10 times (for a total of 50K generated images) and report the average and the standard deviation of the IS.\nWe compute FID using two sample sizes of generated images: 5K and 50K. In order to be consistent with previous works [27, 33] and be able to directly compare our quantitative results with theirs, the FID is computed as follows:\n\u2022 CIFAR10: the statistics for the real data are computed using the 50K training images. This (real data) statistics are used in the FID computation of both 5K and 50K samples of generated images. This is consistent with both Miyato et al. [27] and Ravuri et al. [33] procedure to compute FID for CIFAR10 experiments.\n\u2022 STL10: when using 5K generated images, the statistics for the real data are computed using the set of 5K (labeled) training images. This is consistent with the FID\nFID computation is repeated 3 times and the average is reported. There is very small variance in the FID results."
            },
            {
                "heading": "7.7. Impact of the number of layers used for feature",
                "text": "extraction\nFigure 7 shows generated images from generators that were trained with a different number of layers employed to feature matching. In all the results in Fig.7, the VGG19 network was used to perform feature extraction. We can see a significant improvement in image quality when more layers are used. Better results are achieved when 11 or more layers are used, which corroborates the quantitative results in Sec. 5.2."
            },
            {
                "heading": "7.8. Pretrained Generator/Discriminator in",
                "text": "WGAN-GP\nThe objective of the experiments presented in this section is to evaluate if WGAN-GP can benefit from DCNN classifiers pretrained on ImageNet. In the experiments, we used a WGAN-GP architecture where: (1) the discriminator is a VGG19 or a Resnet18; (2) the discriminator is pretrained on ImageNet; (3) the generator is pretrained on CIFAR10 through autoencoding. Although we tried different hyperparameter combinations, we were not able to successfully train WGAN-GP with VGG19 or Resnet18 discriminators. Indeed, the discriminator, being pretrained on ImageNet, can quickly learn to distinguish between real and fake images. This limits the reliability of the gradient information from the discriminator, which in turn renders the training of a proper generator extremely challenging or even impossible. This is a well-known issue with GAN training [10] where the training of the generator and discriminator must strike a balance. This phenomenon is covered in [2] Section 3 (illustrated in their Figure 2) as one motivation for work like Wassertein GANs. If a discriminator can distinguish perfectly between real and fake early on, the generator cannot learn properly and the min/max game becomes unbalanced, having no good discriminator gradients for the generator to learn from, producing degenerate models. Figure 8 shows some examples of images generated by the unsuccessfully trained models."
            },
            {
                "heading": "7.9. Impact of Adam Moving Average for VGG19",
                "text": "feature extractor.\nIn this appendix, we present a comparison between the simple moving average (MA) and ADAM moving average (AMA) for the case where VGG19 ImageNet classifier is used as a feature extractor. This experiment uses a minibatch size of 64. We can see in Fig. 9 that AMA has a very positive effect in the quality of generated images. GFMN trained with MA produces various images with some sort of crossing line artifacts."
            },
            {
                "heading": "7.10. Visual Comparison between GFMN and GMMN Generated Images.",
                "text": "Figure 10 shows a visual comparison between images generated by GFMN (Figs. 10a and 10b) and Generative Moment Matching Networks (GMMN) (Figs. 10c and 10d).\nGMMN [22] generated images were obtained from Li et al. [21]. In this experiment, both GMMN and GFMN use a DCGAN-like architecture in the generator. Images generated by GFMN have significantly better quality compared to the ones generated by GMMN, which corroborates the quantitative results in Sec. 5.4."
            },
            {
                "heading": "7.11. Autoencoder features vs. VGG19 features for",
                "text": "CelebA.\nIn this appendix, we present a comparison in image quality for autoencoder features vs. VGG19 features for the CelebA dataset. We show results for both simple moving\naverage (MA) and ADAM moving average (AMA), for both cases we use a minibatch size of 64. In Fig. 11, we show generated images from GFMN trained with either VGG19 features (top row) or autoencoder (AE) features (bottom row). We show images generated by GFMN models trained with simple moving average (MA) and Adam moving average (AMA). We can note in the images that, although VGG19 features are from a cross-domain classifier, they lead to much better generation quality than AE features, specially for the MA case."
            }
        ],
        "references": [
            {
                "title": "On gradient regularizers for mmd gans",
                "author": [
                    "M. Arbel",
                    "D.J. Sutherland",
                    "M. Bi\u0144kowski",
                    "A. Gretton"
                ],
                "venue": "NIPS,",
                "citeRegEx": "1",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Wasserstein generative adversarial networks",
                "author": [
                    "M. Arjovsky",
                    "S. Chintala",
                    "L. Bottou"
                ],
                "venue": "Proc. of ICML, pages 214\u2013223,",
                "citeRegEx": "2",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Demystifying MMD GANs",
                "author": [
                    "M. Bikowski",
                    "D.J. Sutherland",
                    "M. Arbel",
                    "A. Gretton"
                ],
                "venue": "International Conference on Learning Representations,",
                "citeRegEx": "3",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Optimizing the latent space of generative networks, 2018",
                "author": [
                    "P. Bojanowski",
                    "A. Joulin",
                    "D. Lopez-Paz",
                    "A. Szlam"
                ],
                "venue": null,
                "citeRegEx": "4",
                "shortCiteRegEx": "4",
                "year": 2018
            },
            {
                "title": "Deep clustering for unsupervised learning of visual features",
                "author": [
                    "M. Caron",
                    "P. Bojanowski",
                    "A. Joulin",
                    "M. Douze"
                ],
                "venue": "European Conference on Computer Vision,",
                "citeRegEx": "5",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "An analysis of single-layer networks in unsupervised feature learning",
                "author": [
                    "A. Coates",
                    "A. Ng",
                    "H. Lee"
                ],
                "venue": "Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215\u2013223,",
                "citeRegEx": "6",
                "shortCiteRegEx": null,
                "year": 2011
            },
            {
                "title": "Generating images with perceptual similarity metrics based on deep networks",
                "author": [
                    "A. Dosovitskiy",
                    "T. Brox"
                ],
                "venue": "D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 658\u2013666. Curran Associates, Inc.,",
                "citeRegEx": "7",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Training generative neural networks via maximum mean discrepancy optimization",
                "author": [
                    "G.K. Dziugaite",
                    "D.M. Roy",
                    "Z. Ghahramani"
                ],
                "venue": "Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, pages 258\u2013 267,",
                "citeRegEx": "8",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Image style transfer using convolutional neural networks",
                "author": [
                    "L.A. Gatys",
                    "A.S. Ecker",
                    "M. Bethge"
                ],
                "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June",
                "citeRegEx": "9",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Generative adversarial nets",
                "author": [
                    "I. Goodfellow",
                    "J. Pouget-Abadie",
                    "M. Mirza",
                    "B. Xu",
                    "D. Warde-Farley",
                    "S. Ozair",
                    "A. Courville",
                    "Y. Bengio"
                ],
                "venue": "Proc. of NIPS, page 2672,",
                "citeRegEx": "10",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "A kernel method for the two-sample-problem",
                "author": [
                    "A. Gretton",
                    "K.M. Borgwardt",
                    "M. Rasch",
                    "B. Sch\u00f6lkopf",
                    "A.J. Smola"
                ],
                "venue": "Proceedings of the 19th International Conference on Neural Information Processing Systems, pages 513\u2013520,",
                "citeRegEx": "11",
                "shortCiteRegEx": null,
                "year": 2006
            },
            {
                "title": "A kernel two-sample test",
                "author": [
                    "A. Gretton",
                    "K.M. Borgwardt",
                    "M.J. Rasch",
                    "B. Sch\u00f6lkopf",
                    "A. Smola"
                ],
                "venue": "Journal of Machine Learning Research, 13:723\u2013773,",
                "citeRegEx": "12",
                "shortCiteRegEx": null,
                "year": 2012
            },
            {
                "title": "Improved training of wasserstein gans",
                "author": [
                    "I. Gulrajani",
                    "F. Ahmed",
                    "M. Arjovsky",
                    "V. Dumoulin",
                    "A.C. Courville"
                ],
                "venue": "CoRR,",
                "citeRegEx": "13",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Deep residual learning for image recognition",
                "author": [
                    "K. He",
                    "X. Zhang",
                    "S. Ren",
                    "J. Sun"
                ],
                "venue": "CVPR,",
                "citeRegEx": "14",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Gans trained by a two time-scale update rule converge to a nash equilibrium",
                "author": [
                    "M. Heusel",
                    "H. Ramsauer",
                    "T. Unterthiner",
                    "B. Nessler",
                    "G. Klambauer",
                    "S. Hochreiter"
                ],
                "venue": "CoRR, abs/1706.08500,",
                "citeRegEx": "15",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "What makes imagenet good for transfer learning",
                "author": [
                    "M. Huh",
                    "P. Agrawal",
                    "A.A. Efros"
                ],
                "venue": "CoRR, abs/1608.08614,",
                "citeRegEx": "16",
                "shortCiteRegEx": "16",
                "year": 2016
            },
            {
                "title": "Perceptual losses for real-time style transfer and super-resolution",
                "author": [
                    "J. Johnson",
                    "A. Alahi",
                    "L. Fei-Fei"
                ],
                "venue": "European Conference on Computer Vision,",
                "citeRegEx": "17",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Adam: A method for stochastic optimization",
                "author": [
                    "D. Kingma",
                    "J. Ba"
                ],
                "venue": "ICLR,",
                "citeRegEx": "18",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Auto-encoding variational bayes",
                "author": [
                    "D.P. Kingma",
                    "M. Welling"
                ],
                "venue": "arXiv preprint arXiv:1312.6114,",
                "citeRegEx": "19",
                "shortCiteRegEx": null,
                "year": 2013
            },
            {
                "title": "Learning multiple layers of features from tiny images",
                "author": [
                    "A. Krizhevsky"
                ],
                "venue": "page 60,",
                "citeRegEx": "20",
                "shortCiteRegEx": null,
                "year": 2009
            },
            {
                "title": "MMD GAN: Towards deeper understanding of moment matching network",
                "author": [
                    "C.-L. Li",
                    "W.-C. Chang",
                    "Y. Cheng",
                    "Y. Yang",
                    "B. Poczos"
                ],
                "venue": "Advances in Neural Information Processing Systems, pages 2203\u20132213.",
                "citeRegEx": "21",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Generative moment matching networks",
                "author": [
                    "Y. Li",
                    "K. Swersky",
                    "R. Zemel"
                ],
                "venue": "Proceedings of the International Conference on International Conference on Machine Learning, pages 1718\u20131727,",
                "citeRegEx": "22",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Diffusion distance for histogram comparison",
                "author": [
                    "H. Ling",
                    "K. Okada"
                ],
                "venue": "Computer Vision and Pattern Recognition,",
                "citeRegEx": "23",
                "shortCiteRegEx": null,
                "year": 2006
            },
            {
                "title": "Deep learning face attributes in the wild",
                "author": [
                    "Z. Liu",
                    "P. Luo",
                    "X. Wang",
                    "X. Tang"
                ],
                "venue": "Proceedings of International Conference on Computer Vision (ICCV),",
                "citeRegEx": "24",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Adversarial autoencoders",
                "author": [
                    "A. Makhzani",
                    "J. Shlens",
                    "N. Jaitly",
                    "I. Goodfellow"
                ],
                "venue": "International Conference on Learning Representations,",
                "citeRegEx": "25",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Universal kernels",
                "author": [
                    "C.A. Micchelli",
                    "Y. Xu",
                    "H. Zhang"
                ],
                "venue": "J. Mach. Learn. Res., 7:2651\u20132667, Dec.",
                "citeRegEx": "26",
                "shortCiteRegEx": null,
                "year": 2006
            },
            {
                "title": "Spectral normalization for generative adversarial networks",
                "author": [
                    "T. Miyato",
                    "T. Kataoka",
                    "M. Koyama",
                    "Y. Yoshida"
                ],
                "venue": "International Conference on Learning Representations,",
                "citeRegEx": "27",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Fisher GAN",
                "author": [
                    "Y. Mroueh",
                    "T. Sercu"
                ],
                "venue": "Proceedings of NIPS,",
                "citeRegEx": "28",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "McGan: Mean and covariance feature matching GAN",
                "author": [
                    "Y. Mroueh",
                    "T. Sercu",
                    "V. Goel"
                ],
                "venue": "Proceedings of the 34th International Conference on Machine Learning, pages 2527\u2013 2535,",
                "citeRegEx": "29",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Plug & play generative networks: Conditional iterative generation of images in latent space",
                "author": [
                    "A. Nguyen",
                    "J. Clune",
                    "Y. Bengio",
                    "A. Dosovitskiy",
                    "J. Yosinski"
                ],
                "venue": "Conference on Computer Vision and Pattern Recognition, pages 3510\u2013 3520,",
                "citeRegEx": "30",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Automatic differentiation in pytorch",
                "author": [
                    "A. Paszke",
                    "S. Gross",
                    "S. Chintala",
                    "G. Chanan",
                    "E. Yang",
                    "Z. De- Vito",
                    "Z. Lin",
                    "A. Desmaison",
                    "L. Antiga",
                    "A. Lerer"
                ],
                "venue": "NIPS-W,",
                "citeRegEx": "31",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
                "author": [
                    "A. Radford",
                    "L. Metz",
                    "S. Chintala"
                ],
                "venue": "ICLR,",
                "citeRegEx": "32",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Learning implicit generative models with the method of learned moments",
                "author": [
                    "S.V. Ravuri",
                    "S. Mohamed",
                    "M. Rosca",
                    "O. Vinyals"
                ],
                "venue": "Proceedings of the 35th International Conference on Machine Learning, pages 4311\u20134320,",
                "citeRegEx": "33",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "On the convergence of adam and beyond",
                "author": [
                    "S.J. Reddi",
                    "S. Kale",
                    "S. Kumar"
                ],
                "venue": "ICLR, page 23,",
                "citeRegEx": "34",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Imagenet large scale visual recognition challenge",
                "author": [
                    "O. Russakovsky",
                    "J. Deng",
                    "H. Su",
                    "J. Krause",
                    "S. Satheesh",
                    "S. Ma",
                    "Z. Huang",
                    "A. Karpathy",
                    "A. Khosla",
                    "M. Bernstein",
                    "A.C. Berg",
                    "L. Fei-Fei"
                ],
                "venue": "Int. J. Comput. Vision, 115(3):211\u2013252, Dec.",
                "citeRegEx": "35",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Improved techniques for training gans",
                "author": [
                    "T. Salimans",
                    "I. Goodfellow",
                    "W. Zaremba",
                    "V. Cheung",
                    "A. Radford",
                    "X. Chen"
                ],
                "venue": "Proc. of NIPS, pages 2226\u20132234,",
                "citeRegEx": "36",
                "shortCiteRegEx": null,
                "year": 2016
            },
            {
                "title": "Very deep convolutional networks for large-scale image recognition",
                "author": [
                    "K. Simonyan",
                    "A. Zisserman"
                ],
                "venue": "CoRR, abs/1409.1556,",
                "citeRegEx": "37",
                "shortCiteRegEx": null,
                "year": 2014
            },
            {
                "title": "Wasserstein auto-encoders",
                "author": [
                    "I. Tolstikhin",
                    "O. Bousquet",
                    "S. Gelly",
                    "B. Schoelkopf"
                ],
                "venue": "International Conference on Learning Representations,",
                "citeRegEx": "38",
                "shortCiteRegEx": null,
                "year": 2018
            },
            {
                "title": "Improving generative adversarial networks with denoising feature matching",
                "author": [
                    "D. Warde-Farley",
                    "Y. Bengio"
                ],
                "venue": "Proceedings of ICLR,",
                "citeRegEx": "39",
                "shortCiteRegEx": null,
                "year": 2017
            },
            {
                "title": "How transferable are features in deep neural networks",
                "author": [
                    "J. Yosinski",
                    "J. Clune",
                    "Y. Bengio",
                    "H. Lipson"
                ],
                "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2,",
                "citeRegEx": "40",
                "shortCiteRegEx": "40",
                "year": 2014
            },
            {
                "title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop",
                "author": [
                    "F. Yu",
                    "Y. Zhang",
                    "S. Song",
                    "A. Seff",
                    "J. Xiao"
                ],
                "venue": "arXiv preprint arXiv:1506.03365,",
                "citeRegEx": "41",
                "shortCiteRegEx": null,
                "year": 2015
            },
            {
                "title": "Energy-based generative adversarial network",
                "author": [
                    "J.J. Zhao",
                    "M. Mathieu",
                    "Y. LeCun"
                ],
                "venue": "Proceedings of ICLR,",
                "citeRegEx": "42",
                "shortCiteRegEx": null,
                "year": 2017
            }
        ],
        "abstractText": "Perceptual features (PFs) have been used with great success in tasks such as transfer learning, style transfer, and super-resolution. However, the efficacy of PFs as key source of information for learning generative models is not well studied. We investigate here the use of PFs in the context of learning implicit generative models through moment matching (MM). More specifically, we propose a new effective MM approach that learns implicit generative models by performing mean and covariance matching of features extracted from pretrained ConvNets. Our proposed approach improves upon existing MM methods by: (1) breaking away from the problematic min/max game of adversarial learning; (2) avoiding online learning of kernel functions; and (3) being efficient with respect to both number of used moments and required minibatch size. Our experimental results demonstrate that, due to the expressiveness of PFs from pretrained deep ConvNets, our method achieves stateof-the-art results for challenging benchmarks."
    }
]