{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "laySumm_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Summaformers/blob/main/laySumm_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZpvIqI-IrM7"
      },
      "source": [
        "import spacy\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IV79z-jJWDm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIXwKX-rIrNS"
      },
      "source": [
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "nlp = English()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuucQJ4yIrNm"
      },
      "source": [
        "import pickle\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import pickle as pkl\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer, GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "filename = open(\"drive/My Drive/Scientific_Doc_Summarization_Raw_Data/Pickled Datasets/laySumm_Test.pkl\", \"rb\")\n",
        "store = pkl.load(filename)\n",
        "print(\"Number of Datapoints:\", len(store))\n",
        "\n",
        "# Experiment with generated extractive summaries\n",
        "# filename = open(\"drive/My Drive/Scientific_Doc_Summarization_Raw_Data/summa_ext_res.pkl\", \"rb\")\n",
        "# store_ext = pkl.load(filename)\n",
        "# print(\"Number of Datapoints:\", len(store_ext))\n",
        "\n",
        "def get_text(units):\n",
        "    text = \"\"\n",
        "\n",
        "    # To consider a set of paragraphs\n",
        "    count_pg = 0\n",
        "    pg_id = 0\n",
        "\n",
        "    for sent in units:\n",
        "        if sent == 'PARAGRAPH':\n",
        "            count_pg += 1\n",
        "    \n",
        "    for sent in units:\n",
        "        if sent == 'PARAGRAPH':\n",
        "            pg_id += 1\n",
        "            continue\n",
        "        \n",
        "        # Tried out sectional input variations\n",
        "        # if pg_id != 1 and pg_id != 2 and pg_id != count_pg:\n",
        "        #   continue\n",
        "        \n",
        "        tokens = nltk.word_tokenize(sent)\n",
        "        \n",
        "        for token in tokens:\n",
        "            text += token.strip().rstrip(\"\\n\") + \" \"\n",
        "            \n",
        "    return text\n",
        "\n",
        "def get_tops(units):\n",
        "    text = \"\"\n",
        "    count = 0\n",
        "    word_count = 0\n",
        "    \n",
        "    for sent in units:\n",
        "        if sent == 'PARAGRAPH':\n",
        "            continue\n",
        "\n",
        "        text += sent + \"\\n\\n\"\n",
        "        count += 1\n",
        "        word_count += len(sent.split(\" \"))\n",
        "        if word_count > 150:\n",
        "        \tbreak\n",
        "    \n",
        "    return text\n",
        "\n",
        "count = 0\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "535PH8AE2o4b"
      },
      "source": [
        "our_dict = {'abstract': {'url': 'ourpaper', 'id': '1', 'title':'Scientific Document Summarization @ LaySumm 20 and LongSumm 20', 'abstract': ['PARAGRAPH',\n",
        "   \"Automatic text summarization has been widely studied as an important task in natural language processing.\",\n",
        " \"Traditionally, various feature engineering and machine learning based systems have been proposed for extractive as well as abstractive text summarization.\",\n",
        " \"Recently, deep learning based, specifically Transformer-based systems have been immensely popular.\",\n",
        "\"Summarization is a cognitively challenging task - extracting summary worthy sentences is laborious, and expressing semantics in brief when doing abstractive summarization is complicated.\",\n",
        "\"In this paper, we specifically look at the problem of summarizing scientific research papers from multiple domains.\",\n",
        "\"We differentiate between two types of summaries, namely, (a) LaySumm: A very short summary that captures the essence of the research paper in layman terms restricting overtly specific technical jargon and (b) LongSumm: A much longer detailed summary aimed at providing specific insights into various ideas touched upon in the paper.\",\n",
        "\"While leveraging latest Transformer-based models, our systems are simple, intuitive and based on how specific paper sections contribute to human summaries of the two types described above.\"]},\n",
        "'fulltext': {'url': 'ourpaper', 'id': '1', 'title':'Scientific Document Summarization @ LaySumm 20 and LongSumm 20', 'introduction': '...', 'conclusion': '...', 'methods': '...'}}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNySAinu4oQO"
      },
      "source": [
        "# our_dict stores the dictionary for our own paper\n",
        "# To generate LaySumm for our paper:\n",
        "# store = [our_dict]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwaiZT-CpLLP"
      },
      "source": [
        "def cleanup(summary, abstract):\n",
        "    abs_tokens = {}\n",
        "    \n",
        "    for x in word_tokenize(abstract):\n",
        "        abs_tokens[x] = 0\n",
        "    \n",
        "    tokens = []\n",
        "    \n",
        "    for x in word_tokenize(summary):\n",
        "      if not x.isalpha() and x not in abs_tokens and x != '.':\n",
        "        pass\n",
        "        # print(x)\n",
        "      else:\n",
        "        tokens.append(x)\n",
        "            \n",
        "    fixed_string = \"\"\n",
        "    for token in tokens:\n",
        "        if token != '.':\n",
        "            fixed_string += (token + \" \")\n",
        "            \n",
        "        else:\n",
        "            fixed_string += \".\\n\\n\"\n",
        "    \n",
        "    return fixed_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3R2M3CbcIrN5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dc2382b3-a6c8-474e-95dc-8516509fd679"
      },
      "source": [
        "# For saved BART models\n",
        "model_statedict_path = \"drive/My Drive/Scientific_Doc_Summarization_Raw_Data/Saved Models/bart_abs_only_best.pt\"\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
        "model.load_state_dict(torch.load(model_statedict_path, map_location=torch.device('cpu')))\n",
        "\n",
        "# For saved T5 models\n",
        "# model_statedict_path = \"./drive/My Drive/Scientific_Doc_Summarization_Raw_Data/t5.pt\"\n",
        "# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "# model = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)\n",
        "# model.load_state_dict(torch.load(model_statedict_path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9WFoYqdRtR_"
      },
      "source": [
        "# A directory with the provided model name will be created which will hold the submission files\n",
        "\n",
        "model_name = 'model'\n",
        "\n",
        "if os.path.isdir(model_name):\n",
        "    os.rmdir(model_name)\n",
        "os.mkdir(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL_l90mCLN1j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b98890ba-5d8d-4663-8c3b-0265ef2f071a"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW5sauIBTtlN"
      },
      "source": [
        "def check(list_str, term):\n",
        "  for stri in list_str:\n",
        "    if term in stri:\n",
        "      return (True, stri)\n",
        "  return (False, \"\")\n",
        "\n",
        "count = 0\n",
        "for paper in store:\n",
        "    saved = check(paper['fulltext'].keys(), 'result')\n",
        "    if saved[0]:\n",
        "      doc_results = get_text(paper['fulltext'][saved[1]]).replace(\"\\n\",\" \")\n",
        "    else:\n",
        "      doc_results = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9ntNvFzwPp6"
      },
      "source": [
        "missing = 0\n",
        "for paper in store:\n",
        "  if 'title' in paper['fulltext']:\n",
        "    head = paper['fulltext']['title']\n",
        "  else:\n",
        "    missing += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDCNbUBXIrOX"
      },
      "source": [
        "all_summaries = {}\n",
        "\n",
        "for paper in tqdm(store):\n",
        "    no_conc = 0\n",
        "    count += 1\n",
        "\n",
        "    abstract = get_text(paper['abstract']['abstract'])\n",
        "\n",
        "    # abstract = store_ext[paper['abstract']['id']].replace(\"\\n\\n\", \"\")\n",
        "    # to use the generated extractive summaries as input\n",
        "    # print(abstract)\n",
        "\n",
        "    if 'title' in paper['fulltext']:\n",
        "      head = paper['fulltext']['title']\n",
        "    else:\n",
        "      head = \"\"\n",
        "    \n",
        "    if 'discussion' in paper['fulltext'].keys():\n",
        "      doc_diss = get_text(paper['fulltext']['discussion'])\n",
        "    else:\n",
        "      doc_diss = \"\"\n",
        "\n",
        "    if 'conclusion' in paper['fulltext'].keys():\n",
        "      doc_conc = get_text(paper['fulltext']['conclusion']).replace(\"\\n\",\" \")\n",
        "    \n",
        "    elif 'conclusions' in paper['fulltext'].keys():\n",
        "      doc_conc = get_text(paper['fulltext']['conclusions']).replace(\"\\n\",\" \")\n",
        "    \n",
        "    else:\n",
        "      no_conc += 1\n",
        "      doc_conc = \"\"\n",
        "\n",
        "    if 'introduction' in paper['fulltext'].keys():\n",
        "      doc_intro = get_text(paper['fulltext']['introduction']).replace(\"\\n\",\" \")\n",
        "\n",
        "    else:\n",
        "      doc_intro = \"\"\n",
        "\n",
        "    saved = check(paper['fulltext'].keys(), 'result')\n",
        "    if saved[0]:\n",
        "      doc_results = get_text(paper['fulltext'][saved[1]]).replace(\"\\n\",\" \")\n",
        "    else:\n",
        "      dec_results = \"\"\n",
        "\n",
        "    for key in paper['fulltext'].keys():\n",
        "    \t# retrieves sections one by one\n",
        "        if key == 'title':\n",
        "            continue\n",
        "        \n",
        "        section = get_text(paper['fulltext'][key])\n",
        "\n",
        "    # use saved models to generate model output here\n",
        "    # section-wise summarizers on selected sections\n",
        "    # use-budget concat code\n",
        "    # use the abstractor model\n",
        "    # separate into sentences with blank line in between, refer: get_top_3()\n",
        "    \n",
        "    # source = tokenizer.batch_encode_plus([abstract + \" \" + doc_conc], max_length= 512, pad_to_max_length=True,return_tensors='pt',truncation=True)\n",
        "    # source = tokenizer.batch_encode_plus([\"summarize: \" + abstract + \" \" + doc_conc], max_length= 512, pad_to_max_length=True,return_tensors='pt',truncation=True)\n",
        "\n",
        "    source = tokenizer.batch_encode_plus([abstract], max_length= 512, pad_to_max_length=True,return_tensors='pt',truncation=True)\n",
        "\n",
        "    source_ids = source['input_ids'].reshape(1,-1).to(device, dtype = torch.long)\n",
        "    source_mask = source['attention_mask'].reshape(1,-1).to(device, dtype = torch.long)\n",
        "    \n",
        "    generated_ids = model.generate(\n",
        "            input_ids = source_ids,\n",
        "            attention_mask = source_mask, \n",
        "            min_len = 120,\n",
        "            max_length = 140,\n",
        "            num_beams=2,\n",
        "            repetition_penalty=1.8, \n",
        "            length_penalty=1.0, \n",
        "            early_stopping=True\n",
        "            )\n",
        "    \n",
        "    model_laySumm = cleanup([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids][0], abstract).replace(\"( \", \"(\").replace(\" )\", \")\").replace(\"( \", \"(\").replace(\" )\", \")\").replace(\"( \", \"(\").replace(\" )\", \")\").replace(\" .\", \".\").replace(\" ,\", \",\")\n",
        "    # model_laySumm = '.\\n\\n'.join(model_laySumm.split('.'))\n",
        "    # model_laySumm = model_laySumm.replace(')', ' )')\n",
        "    \n",
        "    output = \"https://doi.org/\" + paper['abstract']['url']\n",
        "    output += \"\\n\\nLAYSUMM\\n\\nTITLE\\n\\n\" + paper['abstract']['title']\n",
        "    output += \"\\n\\nPARAGRAPH\\n\\n\"\n",
        "    output += model_laySumm\n",
        "    output += \"\\n\"\n",
        "\n",
        "    all_summaries[paper['abstract']['id']] = model_laySumm\n",
        "\n",
        "    file_out = open(\"./\" + model_name + \"/\" + paper['abstract']['id'] + \"_LAYSUMM.TXT\", \"w+\")\n",
        "    file_out.write(output)\n",
        "    file_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxUmUps7IrOk"
      },
      "source": [
        "!zip -r ./model.zip ./model/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvxR8OXNfqIa"
      },
      "source": [
        "# filename = open(\"bart_best.pkl\", \"wb\")\n",
        "# pickle.dump(all_summaries, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NoL0dXNx0BK"
      },
      "source": [
        "# Trying out a paraphraser\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "tokenizer_p = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")  \n",
        "model_p = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\").to(\"cuda\")\n",
        "\n",
        "sentence =  'The presence of a single large portosystemic shunts ( SPSS ) is associated with complications, especially overt hepatic encephalopathy ( oHE ) in patients with liver cirrhosis. In this study, we show that a total cross-sectional SPST area of 83 mm2 or more increases the risk for oHE and mortality in patients who develop SPSs > 1. Therefore, screening for a large S PSS should be considered when stratifying patients with cirrhotic liver disease.'\n",
        "\n",
        "text =  \"paraphrase: \" + sentence\n",
        "\n",
        "encoding = tokenizer_p.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
        "input_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n",
        "outputs = model_p.generate(\n",
        "    input_ids=input_ids, attention_mask=attention_masks,\n",
        "    do_sample=True,\n",
        "    top_k=120,\n",
        "    top_p=0.9,\n",
        "    max_length=150,\n",
        "    min_length=125,\n",
        "    early_stopping=False,\n",
        "    num_return_sequences=1)\n",
        "\n",
        "for output in outputs:\n",
        "    line = tokenizer_p.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    print(line)\n",
        "\n",
        "# Results were not that great"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6KlYx8Wh_WY"
      },
      "source": [
        "# ^_^ Thank You"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}